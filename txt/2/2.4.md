## 2.4 HBase：一个NoSql数据库

&emsp;&emsp;HBase是一个分布式的、面向列的开源数据库。建立在 HDFS 之上。Hbase的名字的来源是Hadoop database，即 Hadoop 数据库，它是一个分布式、可伸缩的大数据存储。为什么HBase能存储海量的数据？因为HBase是在HDFS的基础之上构建的，其存储能力取决于HDFS集群的存储能力。

### 2.4.1 HBase介绍

&emsp;&emsp;HBase是一个数据库，但问题是我们为什么需要HBase，我们不是已经有了HDFS来存储文件吗？其实可以这样理解：HDFS是文件系统，而HBase是数据库，可以把HBase当做是MySQL，把HDFS当做是硬盘。HBase只是一个NoSQL数据库，把数据存在HDFS上。HBase在HDFS之上提供了高并发的随机写和支持实时查询，这是HDFS不具备的。通俗的说，HBase的存储更“灵活”。

&emsp;&emsp;HBase实际上属于NoSql数据库，类似Redis，使用Key-Value对的形式在存储数据，但其在表现形式上和传统的关系型数据库类似，都可以可视化为一张表的结构，但不同的是关系数据库中表是行式存储，而HBase中表是以列式存储的。那么为什么我们既说HBase存储的是键值对，又说其是列式存储的表？这要从HBase的数据模型说起。

**（1）HBase 数据模型**

&emsp;&emsp;HBase 的数据模型是理解其核心原理的关键部分，本节主要介绍HBase数据模型。与传统的关系型数据库不同，HBase并不像传统的关系型数据库MySQL，以“行和列”的二维结构进行固定模式的存储，而是采用一种灵活的、稀疏的、面向列族（Column Family）的列式存储。


&emsp;&emsp;如何理解HBase的列式存储呢？图2-8中有两名学生的成绩，这种熟悉的数据组织方式正是MySQL的存储方式——行式存储。每个学生作为一个实例占用一行，该行的每一列便是该学生的一个属性，那么如何按照“列”将其存储于HBase中呢？

<p align="center">
    <img src="/pic/2/2-17 学生成绩表.png" width="50%">
    <br/>
    <em>图2-8 学生成绩表</em>
</p>

&emsp;&emsp;HBase其实和关系型数据库类似，HBase里边也有表、行和列的概念。HBase中的数据以“表（Table）”的形式组织，每行由唯一的“行键（Row Key）”标识，行内数据再划分为多个“列族（Column Family）”，每个列族包含若干个“列限定符（Qualifier）”。此外，每条数据都有对应的“时间戳（Timestamp）”，用于版本控制。可以理解为HBase本质上是一个结构化的、支持多版本的 Key-Value 存储系统。其中，完整的 Key 结构是：RowKey + Column Family + Qualifier + Timestamp。每一条数据（也叫一个单元格）的定位，都依赖这个四元组，而 Value 就是对应的值。

&emsp;&emsp;上面这段话对于初学者来说理解起来可能有点难度，不妨画个图理解一下：
<p align="center">
    <img src="/pic/2/2-18 列式存储.png" width="50%">
    <br/>
    <em>图2-9 列式存储</em>
</p>

&emsp;&emsp;可能还是难以理解，让我们放点具体的值：

<p align="center">
    <img src="/pic/2/2-19 列式存储-具体值.png" width="50%">
    <br/>
    <em>图2-10 列式存储-具体值</em>
</p>
&emsp;&emsp;这张表我们有两个列族，分别是 StudentInfo 和 ScoreInfo。在 StudentInfo 下有两个列，分别是 StudentInfo:name 和StudentInfo:stu_id，在 ScoreInfo 下有两个列，分别是 ScoreInfo:Chinese 和 ScoreInfo:math。其中，对于主键（RowKey）为 1 的数据：其 StudentInfo:name 的值为：张三。StudentInfo:stu_id 的值为222。ScoreInfo:Chinese的值为90。ScoreInfo:math的值为100。也就是说在这个表中存在多个键值对：

```
            Key                      Value       
[RowKey:1, StudentInfo, name]   ->    张三
[RowKey:2, StudentInfo, name]   ->    李四
[RowKey:1, StudentInfo, stu_id] ->    222
[RowKey:1, Scoreinfo, Chinese]  ->    90
[RowKey:2, Scoreinfo, Chinese]  ->    88
......
```

&emsp;&emsp;在一开始，我们说HBase是NoSql数据库，数据的存储本质是存储的键值对，可现在我们又说，HBase是有表结构，是列式存储，这两种说法矛盾吗？让我们看一个例子，假设有一张学生表，管理着班级上各学生的信息，现在班上突然加入了一个留学生，留学生要存储一些额外的信息，例如国籍和外文名，如果是传统的行式存储（Sql数据库），我们可以通过给表额外创建列来实现，这样表可能的形式如图2-11所示。但如此做，图中另两位同学所对应的列上便存储了空值，虽然我们没存储信息，但存储空间确是实打实的占用了。而HBase不会，其他行没有用到的属性不会占用额外的空间，因为它实质上是键值对存储，其存储的数据大致如图2-12所示。这也就是我们为什么说它灵活的、稀疏的。在其底层采用键值对存储，但在对外的表现形式上是表的结构。

<p align="center">
    <img src="/pic/2/2-20 严格的表结构.png" width="50%">
    <br/>
    <em>图2-11 严格的表结构</em>
</p>

<p align="center">
    <img src="/pic/2/2-21 HBase存储结构.png" width="50%">
    <br/>
    <em>图2-12 HBase存储结构</em>
</p>


### 2.4.2 部署HBase集群

&emsp;&emsp;HBase作为Hadoop生态中的分布式数据库，构建在HDFS和ZooKeeper之上，如同一位精密的钟表师，需要协调多个组件才能完美运转。请确保已经顺利完成了前两节2.3和2.2的HDFS和ZooKeeper集群的安装，让我们开始部署HBase集群吧！部署过程应该是：

1. 安装HBase 
2. 配置HBase集群参数 
3. 同步HBase集群参数 
4. 启动HBase集群
5. 验证高可用

**（1）安装HBase**

_只在一台主机上执行（在这里，选择在`m1`上执行）_

&emsp;&emsp;本书实验采用2.5.11版本。后续的下载、安装目录均在管理员的家目录`/root`。首先从清华镜像源（官网镜像源可能下载速度缓慢）下载。

&emsp;&emsp;`wget https://mirrors.tuna.tsinghua.edu.cn/apache/hbase/2.5.11/hbase-2.5.11-bin.tar.gz`

&emsp;&emsp;接着解压：

&emsp;&emsp;`tar -xzvf hbase-2.5.11-bin.tar.gz`

&emsp;&emsp;为确保各组件文件夹命名一致，将其名称修改为`hbase-2.5.11`。

&emsp;&emsp;`mv hbase-2.5.11-bin hbase-2.5.11`

&emsp;&emsp;配置环境变量。修改`/etc/profile`文件，通过`vim`进入`/etc/profile`文件：

```
vim /etc/profile
```

&emsp;&emsp;在`/etc/profile`文件后追加：

```
export HBASE_HOME=/root/hbase-2.5.11
export PATH=$PATH:$HBASE_HOME/bin
```

&emsp;&emsp;同样的，`source /etc/profile`更新环境变量。输入指令验证HBase安装成功。

```
root@m1:~# hbase version
HBase 2.5.11
...
```

**（2）配置HBase集群参数** 


&emsp;&emsp;接下来需要编辑HBase的一些配置文件。在`hbase-2.5.11/conf`目录下，存在许多各种组件的配置文件：

```
hbase-2.5.11
    |--conf
        |--hbase-site.xml   #HBase的核心配置文件
        |--hbase-env.sh     
        |--regionservers    
        |--......
```

&emsp;&emsp;在这里，我们主要配置以上列出的三个文件。编辑`hbase-site.xml`，修改`hbase-site.xml`文件，注意此处的`hbase.zookeeper.quorum`参数要与 HDFS 集群上的该参数保持一致。

```
<configuration>
    <property>
          <!--指定Zookeeper集群节点-->
          <name>hbase.zookeeper.quorum</name>
          <value>m1,m2,m3</value>
    </property>

    <property>
          <!--指定Zookeeper端口号-->
          <name>hbase.zookeeper.property.clientPort</name>
          <value>2181</value>
    </property>
    <property>
          <!--指定HBase在HDFS上的根目录-->
          <name>hbase.rootdir</name>
          <value>hdfs://hdfs-cluster/hbase</value>
    </property>
    <property>
          <!--指定true为分布式集群部署-->
          <name>hbase.cluster.distributed</name>
          <value>true</value>
     </property>
     <!-- 开启配置防止 hmaster 启动问题 -->
  <property>
    <name>hbase.unsafe.stream.capability.enforce</name>
    <value>false</value>
  </property>

  <!-- 监控页面端口 -->
  <property>
    <name>hbase.master.info.port</name>
    <value>60010</value>
  </property>
      <property>
    <name>hbase.tmp.dir</name>
    <value>./tmp</value>
  </property>
<property>
    <name>hbase.wal.provider</name>
    <value>filesystem</value>
</property>

</configuration>
```

&emsp;&emsp;其次，通过`vim regionservers`进入`regionservers`文件，将其修改，这一步是指定HBase运行在哪些主机上，于此前在HDFS集群中的配置`workers`类似。

```
m1
m2
m3
```

&emsp;&emsp;接着，编辑同目录下的`hbase-env.sh`，通过`vim hbase-env.sh`进入文件，添加JAVA目录，并且设置不使用HBase内置的ZooKeeper集群，因为我们会使用之前已配置好的独立的ZooKeeper集群。

```
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HBASE_MANAGES_ZK=false
```

&emsp;&emsp;最后，将`/root/hadoop-3.3.6/etc/hadoop`中的`hdfs-site.xml`文件和`core-site.xml`文件复制进入`/root/hbase-2.5.11/conf`中。

```
cp /root/hadoop-3.3.6/etc/hadoop/hdfs-site.xml /root/hbase-2.5.11/conf
cp /root/hadoop-3.3.6/etc/hadoop/core-site.xml /root/hbase-2.5.11/conf
```

**（3）同步HBase集群参数** 

&emsp;&emsp;由于第（1）、（2）步中我们只在一台主机上安装了HBase，故需将配置好的HBase以及修改过的`/etc/profile`发送给其余所有主机，并激活配置文件。使用命令：

```
scp -r /root/hbase-2.5.11 m2:/root
scp -r /root/hbase-2.5.11 m3:/root
scp -r /etc/profile m2:/etc
scp -r /etc/profile m3:/etc
```

&emsp;&emsp;之后，在所有的主机上：

```
source hbase-env.sh
source /etc/profile
```

**（4）启动HBase集群**

&emsp;&emsp;启动ZooKeeper集群，在集群所有节点（m1，m2，m3）上分别执行以下命令启动ZooKeeper集群

```
root@m1:~# zookeeper-3.8.4/bin/zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /root/zookeeper-3.8.4/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
...
```

&emsp;&emsp;启动HDFS集群，在一台主机上执行`start-dfs.sh `命令启动HDFS集群。

```
root@m1:~# start-dfs.sh
Starting namenodes on [m1 m2]
Starting datanodes
Starting journal nodes [m1 m2 m3]
Starting ZK Failover Controllers on NN hosts [m1 m2]
```


&emsp;&emsp;启动HBase集群，在一台主机上执行`start-hbase.sh`命令启动HBase集群，执行该命令的主机会成为HBase集群中的主节点，即运行着HMaster节点（在这里，我们在`m3`上启动HBase集群）。

```
root@m3:~# start-hbase.sh
running master, logging to /root/hbase-2.5.11/logs/hbase-root-master-m3.out
m2: running regionserver, logging to /root/hbase-2.5.11/bin/../logs/hbase-root-regionserver-m2.out
m1: running regionserver, logging to /root/hbase-2.5.11/bin/../logs/hbase-root-regionserver-m1.out
m3: running regionserver, logging to /root/hbase-2.5.11/bin/../logs/hbase-root-regionserver-m3.out
```

&emsp;&emsp;之后我们需要另选一台主机，在其上再单独启动一个 HMaster 主进程（在这里选择的为`m2`），这一步目的是配置HA-HBase。

```
root@m2:~# hbase-daemon.sh start master
```

&emsp;&emsp;在各台主机上使用命令`jps`，可查看运行的节点信息。此时，在`m2`和`m3`上应为：

```
root@m3:~# jps
2579 HRegionServer
2355 HMaster
1687 QuorumPeerMain
2024 JournalNode
3098 Jps
1884 DataNode

root@m2:~# jps
2147 JournalNode
2323 DFSZKFailoverController
2503 HRegionServer
3015 Jps
1688 QuorumPeerMain
2809 HMaster
2012 DataNode
1900 NameNode
```

&emsp;&emsp;可看到相比单独运行HA-HDFS，新增了两个进程，分别为HMaster和HRegionServer。在`m3`上，HMaster为集群管理进程（HBase集群主进程），负责元数据操作和故障转移等，而HRegionServer为数据服务进程，真正地处理客户端读写请求，管理表数据（这与HDFS集群的从节点DataNode稍有不同）。当配置了HA-HBase后，在`m2`上单独启动的HMaster便成为了备选集群管理进程，负责在主HMaster崩溃时，通过ZooKeeper选举立即接管成为新主节点，避免服务中断。


**（5）验证高可用**

&emsp;&emsp;HBase可以通过Web可视化的方式访问，Web网页端访问端口配置在`hbase-site.xml`文件中的`hbase.master.info.port`属性上，本文设置为`60010`。


&emsp;&emsp;在浏览器打开HBase主节点网页端，如图2-13所示。我们可以在主界面看到一些基本的内容：

1. Region Servers状态表：Region Servers状态表显示当前集群中所有RegionServer的运行状态，包括ServerName、启动时间、HBase版本等信息；
2. 备选节点状态表：备选节点状态表显示当前集群中备选节点（Backup Masters）的主要信息，包括主机名、端口以及启动时间；

3. Tables概览：Tables汇总了所有表的分类和状态，表可分为用户表（User Tables）和 系统表（System Tables两种。

<p align="center">
    <img src="/pic/2/2-12 HBase主节点网页端.png" width="50%">
    <br/>
    <em>图2-13 HBase主节点网页端</em>
</p>


&emsp;&emsp;在浏览器打开HBase备选节点网页端，如图2-14所示。我们可以在主界面看到目前该节点被标记为`Backup Master`，这相当于HDFS集群中的Standby NameNode，同时也指出了活跃的HMaster节点位于主机`m3`上，这和我们之前启动时说明的情况相同。

<p align="center">
    <img src="/pic/2/2-13 HBase备选节点网页端.png" width="50%">
    <br/>
    <em>图2-14 HBase备选节点网页端</em>
</p>

&emsp;&emsp;想要验证HBase的高可用性，首先需要模拟`m3`上的HMaster进程宕机的情况，然后观察备选节点`m2`上HMaster节点的状态变化。故我们在`m3`上终止HMaster进程。在此时其进程号为13319：`kill 13319`。


&emsp;&emsp;终止`m3`上的HMaster进程后，HMaster界面会无法访问，如图2-15所示。

<p align="center">
    <img src="/pic/2/2-14 主节点m3干掉HMaster进程后的网页端.png" width="50%">
    <br/>
    <em>图2-15 终止`m3`上的HMaster进程后的网页端</em>
</p>

&emsp;&emsp;再访问`m2`的HMaster网页端，会发现备选节点`m2`变成了主节点，可以发现`m2`节点的网页端已经变成了主节点网页端！如图2-16所示，至此我们便验证了HBase集群的高可用性，感兴趣的读者可在`m3`上再单独开启一个HMaster进程，观察其会处于何种状态中。

<p align="center">
    <img src="/pic/2/2-15 备选节点m2变成主节点网页端.png" width="50%">
    <br/>
    <em>图2-16 备选节点m2变成主节点网页端</em>
</p>


### 2.4.3 HBase基础操作


&emsp;&emsp;在Active HMaster节点所在主机上执行`hbase shell`进⼊ hbase shell，接下来关于 HBase的数据操作命令都是在hbase shell中执行。

**（1） 建表**

&emsp;&emsp;创建表 student，设置 2个列簇，分别为 stuinfo 和 course；在创建表时，必须指定表的列族个数和列族名，在后续不可更改。

```shell
create 'student', 'stuinfo', 'course'
```

&emsp;&emsp;可使用`describe 'student'`查看该表的信息，可看见该表有两个列族。

```
hbase:005:0> describe 'student'
Table studentaaa is ENABLED                              
...                                      
COLUMN FAMILIES DESCRIPTION                          
{NAME => 'course', ...}                       
{NAME => 'stuinfo', ...}                     
```

&emsp;&emsp;同时也可以在HBase浏览器端查看表的属性，如图2-17。

<p align="center">
    <img src="/pic/2/2-17 浏览器端查看.png" width="50%">
    <br/>
    <em>图2-17 浏览器端查看</em>
</p>

**（2） 插入数据**

&emsp;&emsp;向表中添加数据，添加四条学生信息，添加完成完后，整个表可视化应如图2-18所示。HBase属于NoSql数据库，故插入时相当于插入一个键值对，在其中，形如图2-4中的每一个存储着值单元格都属于一个“Value”，而怎么定位该单元格便属于“Key”的职责，例如左上角的单元格“laowang”如何定位呢？在HBase中，其首先在表的第一行，其次其属于“stuinfo”列族，最后其属于“stuinfo”列族中的“name”列，这样，就相当于我们使用行号，列族名，列名唯一确定了一个单元格。也就是Key应该为行号，列族名，列名三者的结合体。因此，插入一条数据的格式如下，`put`代表插入命令，`student`指定待插入的表名。`1`指定第一行，`stuinfo:name`同时指定列族和列名，三者共同构成了Key，最后一个元素便是待插入的Value`laowang`。

&emsp;&emsp;`put 'student','1','stuinfo:name','laowang'`

&emsp;&emsp;由于命令过多且格式相同，后续命令不再给出，都按照`put 表名, 行号, 列族: 列名, 数据`的格式插⼊数据。


<p align="center">
    <img src="/pic/2/2-4 学生表.png" width="50%">
    <br/>
    <em>图2-18 学生信息表</em>
</p>

**（3） 查询数据**

&emsp;&emsp;添加完数据后，可以使用`scan`命令查询表中的所有信息：

```shell
scan 'student' 
hbase:010:0> scan 'student'
ROW                                 COLUMN+CELL                                                                                           
 1                                  column=stuinfo:name, timestamp=2025-05-21T12:49:11.711, value=laowang
 ......
```

&emsp;&emsp;，我们可以看到，表中的所有单元格的数据，即`value`按行号依次排列，`value`前便是我们上文所说的Key，但在其中多了一个属性`timestamp`，这是什么？其实很简单，这是我们创造该键值对的时间，HBase中同一单元格有多个版本，当你更改单元格时，实际只是创建了一个该单元格更新的版本，而并未覆盖原始内容。你可以尝试`put 'student','1','stuinfo:name','daming`，之后，我们再查看该表：

```shell
hbase:022:0> scan 'student'
ROW                                 COLUMN+CELL                                                                                           
 1                                  column=stuinfo:name, timestamp=2025-05-21T13:02:24.729, value=daming
...... 
```

&emsp;&emsp;原先的数据消失了，这是因为我们在创建表时默认只保留数据的一个版本，我们可以修改该属性，`alter 'student', {NAME=>'stuinfo', VERSIONS=>'3'}`，让`stuinfo`列族支持保存多个版本。我们再更新一次，然后使用`get`指令查询该单元格的多个版本，（VERSIONS => 3）代表查找最新的三个版本：


```shell
hbase:039:0> get 'student', '1', {COLUMN => 'stuinfo:name', VERSIONS => 3}
COLUMN                              CELL                                                                                                  
 stuinfo:name                       timestamp=2025-05-21T13:11:49.644, value=damw                                                         
 stuinfo:name                       timestamp=2025-05-21T13:02:24.729, value=daming 
```

&emsp;&emsp;`get`命令和`scan`命令都可用于查询，`get`命令更使用于查询某一行的数据，例如：`get 'student', '1'`可返回第一行所有单元格的值。`scan`指令也可使用条件查询，一些例子如下：

```shell
# 查询 stuinfo 列簇中的所有信息 
scan 'student', {COLUMN=>'stuinfo'} 

# 查询 stuinfo:sex 列中的所有信息
scan 'student', {COLUMN=>'stuinfo:sex'} 

# 通过 LIMIT 限定查询到的行数
scan 'student', {LIMIT=>2, COLUMN=>'stuinfo'} 
```

**（4） 删除**
&emsp;&emsp;删除单元格的数据，使用`delete`命令，格式为：

```
//delete '表名', '行名', '列族：列'
delete 'student', '1', 'stuinfo:name'
```

&emsp;&emsp;删除某一行的数据，使用`deleteall`命令，格式为：

```
//deleteall '表名', '行名'
deleteall 'student', '1'
```
&emsp;&emsp;如果要删除表：

```shell
# 先使表失效，才能删除
disable 'student' 

# 删除表p
drop 'student'
```

### 2.4.4 HBase背后的存储原理
 

**（1）HBase 存储机制**



<p align="center">
    <img src="/pic/2/2-23 HBase架构图.png" width="50%">
    <br/>
    <em>图2-19 HBase架构图</em>
</p>

&emsp;&emsp;上一小节我们说完了HBase的数据模型，那么剩下的问题便是，这些键值对是如何在真正的文件系统中组织，如何存放在整个集群中。HBase的架构图如图2-19所示，目前可能大家一头雾水，别着急先接着往下看，数据实际存储的过程实际上并不复杂，这在其中，涉及到三个主要的数据结构：

* HRegion
* HLog
* Store

&emsp;&emsp;首先是HRegion，在HBase的表中，其按照行序排列，所有这些数据会在行的方向上分割成多组（例如每一百行一组），每个组会存入一个HRegion（默认的HRegion大小为10G）中，如图2-20，故一个不太大的表可能只会对应一个HRegion。那HRegion存放在哪呢？存放在2.4.2所看到的HRegionServer节点中。HRegion是HBase中分布式存储和负载均衡的最小单元。最小单元就表示不同的HRegion可以分布在不同的HRegionServer上。但一个 HRegion 是不会拆分到多个节点上的。

<p align="center">
    <img src="/pic/2/2-21 拆分表.png" width="50%">
    <br/>
    <em>图2-20 拆分表</em>
</p>

&emsp;&emsp;现在我们已经将表拆分成了一个个的HRegion，但这并不是结束。在HRegion内部，其会按照列族将存储的行进行进一步的切分，相同列族的数据会被存放到同一个Store中，如图2-21。也就是说，一个 HRegion由多个Store组成，每个Store包含一个列族的所有数据（所有针对HRegion中的行而言）。

<p align="center">
    <img src="/pic/2/2-22 HRegion里分成多个Store.png" width="50%">
    <br/>
    <em>图2-21 HRegion里分成多个Store</em>
</p>

&emsp;&emsp;结束了吗？并没有。在Store内部，其由一个位于内存的Memstore和多个位于硬盘的StoreFile组成。当写数据时，（因为存储的是键值对，故写单元格最终都会落到修改某一个Store）数据首先会写入Memstore。原因是显而易见的，操作内存总是比操作磁盘要快得多，当Memstore容量到达上限时，其会把数据刷写入StoreFile，每次写入形成单独一个StoreFile，当 StoreFile 大小超过一定阈值后，会把当前的 HRegion 分割成两个，并由 HMaster 分配给相应的 HRegion 服务器，实现负载均衡。而当一个HRegion里 StoreFile 的数量过多时，会触发合并，将多个小的StoreFile合并。

&emsp;&emsp;那只剩下最后的一个HLog了，其意为Write ahead log，类似 mysql 中的 binlog，也类似HDFS中的Edits，用来做灾难恢复时用，Hlog记录数据的所有变更，当实际存储的数据丢失（例如Memstore上的数据由于宕机未及时写入StoreFile），就可以从HLog中进行恢复。

&emsp;&emsp;每个RegionServer维护一个HLog，而不是每个HRegion维护一个。这样做会使得不同HRegion（不同HRegion里的数据可能来自不同表，或是同一表的不同行）的日志混在一起，但当写操作时即是操作了不同的HRegion，我们也只需不断追加单个文件（也就是这一个HLog），相对于同时写多个文件（也就是每个HRegion一个HLog）而言可以减少磁盘寻址次数，因此可以提高写性能。

&emsp;&emsp;现在让我们总结一下：一个表会在行方向上拆分成多个HRegion，每个HRegion内部会依据列族拆分成多个Store。在这其中，HRegion是HBase中分布式存储和负载均衡的最小单元，这意味着HRegion不能拆分存储到多个RegionServer上，而Store则是最终存储的最小单元。那么数据总归是要存储到切实文件系统上，在这里会被存储到HDFS中，StoreFile底层是以HFile的格式保存到HDFS集群上，HFile存储的便是最终的一个个键值对。现在，相信大家已经能看懂图2-23中RegionServer和HDFS的内容。

**（2） RegionServer和HMaster**

&emsp;&emsp;通过上文的学习，大家已经知道了RegionServer负责存储HRegion，也就是存储实际的表数据，但其职责并不止于此，其还会维护各个表的元数据，也就是该表有哪些列族、被拆分成哪些HRegion、这些HRegion存放在哪个RegionServer上等信息。

&emsp;&emsp;而HMaster会处理 HRegion 的分配或转移。如果我们HRegion的数据量太大的话，HMaster会对拆分后的HRegion重新分配RegionServer。（如果发现失效的HRegion，也会将失效的HRegion分配到正常的HRegionServer中），同时HMaster也会处理元数据的变更和监控RegionServer的状态。


**（3） 读写过程**

_读_

1. 要访问表数据，首先得拿到该表的元数据，首先Client会去访问ZooKeeper集群，从中获取对应表元数据的位置信息，即找到该表元数据在哪个HRegionServer上保存着。

2. 接着Client通过刚才获取到的HRegionServer的IP来访问，从而读获取到所需元数据。

3. Client通过元数据中存储的信息，通过访问的行找到对应的HRegion，进而访问存储该HRegion的HRegionServer，然后通过列族名和列名找到该HRegion上对应的Store，遍历该Store的Memstore和StoreFile找到对应单元格信息。

4. 最后HRegionServer把查询到的数据响应给Client。

_写_

1. 和读类似，找表的元数据，然后访问应该写的HRegion对应的HRegionServer节点。

2. Client向该HRegionServer发起写入数据请求，然后HRegionServer收到请求并响应。

3. Client先把数据写入到HLog，以防止数据丢失。然后将数据写入到对应HRegion，对应Store上的Memstore。如果HLog和Memstore均写入成功，则这条数据写入成功。

4. 如果Memstore达到阈值，会把Memstore中的数据刷写到一个新的Storefile中。当Storefile越来越多，会触发合并操作，把过多的Storefile合并成一个大的Storefile。当Storefile越来越大，HRegion也会越来越大，达到阈值后，会触发Split操作，将HRegion一分为二。

### 2.4.5 总结

&emsp;&emsp;本节我们完整介绍了HBase的各个方面，包括安装、存储机制等。该节内容在本章中属于最多的，学习完了本节的内容也就意味着该章所有的内容告一段落了，希望大家都能有所收获。