## 2.2 HDFS：仓库管理员
&emsp;&emsp;HDFS是一个标准的主从（Master/Slave）体系结构的分布式系统，被设计成适合在通用硬件上运行；HDFS集群包含一个主机(NameNode)和多个从机(DataNode)，它们各司其职提供服务，用户可以通过多种方式同 NameNode 进行交互以访问文件系统。HDFS采用Java语言开发，因此任何支持Java的机器都可以部署 HDFS。

&emsp;&emsp;在逻辑设计上，其采用传统的层次型文件组织结构。用户或者应用程序可以创建目录，然后将文件保存在这些目录里。文件系统名字空间的层次结构和大多数现有的文件系统类似：用户可以创建、删除、移动或重命名文件。这样的目录结构由 NameNode 进行管理和维护，同时 NameNode 还负责与用户交互，进行文件的读写操作。而具体的数据以数据块（Block）的形式存储在 DataNode 上，DataNode是 HDFS 集群中真正存储数据的节点。举例来说，当用户要向HDFS中写入一个文件时，会与 NameNode 交互，NameNode会将数据划分为固定大小，对应数量的多个Block，这些Block最终会被分散存储到多个DataNode上，这就实现了分布式存储。一般来说，集群中所有的主机上都有DataNode，而只有一台主机上运行着NameNode。

&emsp;&emsp;HDFS 运行在多台主机上，如何决定数据该存储在哪台主机？在这其中， NameNode 和 DataNode 的作用各是什么？让我们首先部署一个 HDFS 集群，再带着问题一起学习。

### 2.2.1 部署 HDFS 集群
&emsp;&emsp;HDFS 是 Hadoop 生态的一部分，而 Hadoop 又是构建在 Java（严格说是Java8） 之上，所以准备好你的三台Linux主机，我们要开始部署 HDFS 集群啦！部署的过程应该是：  


1. 配置三台主机ssh免密登录
2. 安装Java8
3. 安装Hadoop
4. 配置HDFS集群参数
5. 同步HDFS集群参数
6. 启动HDFS集群

&emsp;&emsp;首先，应该确保在实验过程中各个端口能被正常使用，如果使用运营商提供的服务器，建议在防火墙规则上对所有端口放行，如是华为云服务器，则防火墙规则选择`System-FullAccess`。

&emsp;&emsp;为了方便后续实验，我们需要在配置hosts文件，将我们的三台主机的ip地址映射为机器号。在每台主机上编辑`/etc/hosts`。在文件末尾追加：
```
192.168.0.2 m1  #ip地址根据你的主机情况填写
192.168.0.3 m2
192.168.0.4 m3
```
&emsp;&emsp;在本书的环境中，三台主机处于同一个局域网下，故使用私有IP地址，读者需根据自身主机/虚拟机实际ip地址设置映射关系。在下文中`m1,m2,m3`均指代如上的三台主机。

**（1）配置免密登录**

&emsp;&emsp;在 HDFS 集群中，我们可以在一台主机上发送命令来启动整个集群，这意味着一台主机需要拥有操纵其他主机的权限，这就需要我们配置免密登录，使得集群中主机间能够相互访问。

&emsp;&emsp;在主机上执行`ssh-keygen`来生成密钥，此时会要求为密钥输入密码，由于需要是免密登录的密钥，在这连按两次回车跳过即可。执行完后会在`.ssh/`目录下生成两个密钥文件`id_xxxx`和`id_xxxx.pub`，分别是私钥和公钥，将公钥存储在另一台主机中，另一台主机便可免密登录该主机。故接下来执行：
```
ssh-copy-id m1
ssh-copy-id m2
ssh-copy-id m3
 ```

将该主机公钥存储在所有主机上，这里需要输入各台主机的登录密码。在集群中所有主机中都要执行如上所示的`ssh-keygen`和`ssh-copy-id`指令。

**（2）安装Java8**

_Java的安装在三台主机上都要执行_


&emsp;&emsp;首先更新软件包索引，否则可能导致后续安装失败。

&emsp;&emsp;`sudo apt-get update`

&emsp;&emsp;接着，安装JDK，指定版本Java8。

&emsp;&emsp;` apt install openjdk-8-jdk-headless`



&emsp;&emsp;然后为Java配置环境变量。编辑`/etc/profile`文件，在文件末尾追加：

```
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$PATH:$JAVA_HOME/bin
```

&emsp;&emsp;注意`JAVA_HOME`应指向Java安装目录，输入指令`whereis java`可查看。最后`source /etc/profile`更新环境变量。


&emsp;&emsp;输入指令验证是否安装成功。
```shell
root@m1:~# java -version
openjdk version "1.8.0_452"
OpenJDK Runtime Environment (build 1.8.0_452-8u452-ga~us1-0ubuntu1~24.04-b09)
OpenJDK 64-Bit Server VM (build 25.452-b09, mixed mode)
```

**（3）安装 Hadoop**

_只在一台主机上执行_

&emsp;&emsp;本书实验采用3.3.6版本。后续的下载、安装目录均在管理员的家目录`/root`。首先从国内清华镜像源下载，若从官网下载则速度较慢。

&emsp;&emsp;`wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz`

&emsp;&emsp;接着解压。

&emsp;&emsp;`tar -zxvf hadoop-3.3.6.tar.gz `

&emsp;&emsp;配置环境变量。依旧修改`/etc/profile`文件，将（2）中所添加部分修改如下：

```
export HADOOP_HOME=/root/hadoop-3.3.6
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$JAVA_HOME/bin
```
&emsp;&emsp;同样的，`source /etc/profile`更新环境变量。输入指令验证Hadoop安装成功。

```shell
root@m1:~# hadoop version
Hadoop 3.3.6
Source code repository https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c
Compiled by ubuntu on 2023-06-18T08:22Z
Compiled on platform linux-x86_64
Compiled with protoc 3.7.1
From source with checksum 5652179ad55f76cb287d9c633bb53bbd
This command was run using /root/hadoop-3.3.6/share/hadoop/common/hadoop-common-3.3.6.jar
```

**（4）配置 HDFS 集群参数**

_只在一台主机上执行_

&emsp;&emsp;接下来需要编辑HADOOP的一些配置文件。在`hadoop-3.3.6/etc/hadoop/`目录下，存在许多各种组件的配置文件：
```
hadoop-3.3.6
    |--etc
        |--hadoop
            |--core-site.xml   #Hadoop的核心配置文件
            |--hdfs-site.xml   #HDFS的核心配置文件
            |--mapred-site.xml #MapReduce的核心配置文件
            |--......
```

&emsp;&emsp;在这里我们主要配置 HDFS 集群，但顺带会部署一些其他组件以方便后续使用。首先编辑`core-site.xml`。将其修改为：

```xml
<configuration>
    <property>
        <!-- HDFS的主机和端口号，这里设置在m1上 -->
        <name>fs.defaultFS</name>        
        <value>hdfs://m1:9000</value>
    </property>  
    <property>
        <!-- Hadoop产生数据存储目录 -->
        <name>hadoop.tmp.dir</name>
        <value>/root/hadoop/data</value>
    </property>
 </configuration>
```

&emsp;&emsp;其次是`hdfs-site.xml`。

```xml
<configuration>
    <property>
        <!-- namenode数据存储目录，若未指定，则在hadoop.tmp.dir下 -->
        <name>dfs.name.dir</name>
        <value>/root/hdfs/namenode</value>
    </property>
    <property>
        <!-- 同上 -->
        <name>dfs.data.dir</name>
        <value>/root/hdfs/datanode</value>
    </property>
    <property>
        <!-- 数据在hdfs中的备份次数 -->
        <name>dfs.replication</name>
        <value>3</value>
    </property>
</configuration>
```

&emsp;&emsp;接着编辑同目录下的`hadoop-env.sh`，根据你的Java目录，在文末追加：`export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64`

&emsp;&emsp;如果使用的是管理员账号，还需额外添加：

```
export HDFS_NAMENODE_USER="root"       
export HDFS_DATANODE_USER="root"       
```

&emsp;&emsp;最后，编辑同目录下的`workers`，其为一空文件，该文件作用为指定集群中应包含哪些主机。应将其指定为最初在`/etc/hosts`中映射的三台主机，每台主机单独一行，在书中为：
```
m1
m2
m3
```

**（5）同步HDFS集群参数**

&emsp;&emsp;由于第（3）、（4）步中我们只在一台主机上安装了HADOOP，故需将配置好的HADOOP以及修改过的`/etc/profile`发送给其余所有主机，并激活配置文件。使用命令：
```shell
scp -r /root/hadoop-3.3.6 m2:/root
scp -r /root/hadoop-3.3.6 m3:/root
scp -r /etc/profile m2:/etc
scp -r /etc/profile m3:/etc
 ```
&emsp;&emsp;之后在所有主机上：
```shell
source $HADOOP_HOME/etc/hadoop/hadoop-env.sh
source /etc/profile
```

**（6）启动HDFS集群**

&emsp;&emsp;在m1上执行NameNode初始化，这一步类似于常说的格式化，生成一个初始的文件系统：`hadoop namenode -format`，紧接着就可以启动了，在`m1`上，执行`start-dfs.sh`，含义为单独启动 HDFS 集群。
```
root@m1:~# start-dfs.sh 
Starting namenodes on m1
Starting datanodes
```
&emsp;&emsp;在各台主机上使用命令`jps`，可查看运行的节点信息。此时，在`m1`上应为：
```
root@m1:~# jps
2658 NameNode
2810 DataNode
6894 Jps
```
&emsp;&emsp;可看到此时`m1`上运行着一个NameNode节点和一个DataNode节点，节点前的数字为该节点所对应的进程号。在`m2`和`m3`上执行指令结果应如下：
```
root@m2:~# jps
6588 Jps
2703 DataNode
```
&emsp;&emsp;到这里，我们的 HDFS 集群就算成功启动了。让我们实验一下吧！在`m1`即NameNode节点所在主机上输入命令：`hdfs dfs -mkdir /hello_world`创建文件夹，然后再输入`hdfs dfs -ls /`查看根目录下所有文件。
```shell
root@m1:~# hdfs dfs -mkdir /hello_world
root@m1:~# hdfs dfs -ls /
Found 1 items
drwxr-xr-x   - root supergroup          0 2025-05-15 15:20 /hello_world
```
可以看到我们刚刚所创建的文件夹。

### 2.2.2 操作 HDFS 集群

**（1）使用命令行访问**

&emsp;&emsp;你可以使用`hdfs dfs`来查看所有操作 HDFS 集群的指令。其中大部分文件操作指令和 Linux 下的文件操作保持一致。如`ls,mkdir,rm`等。
&emsp;&emsp;你可以使用`put`命令来将Linux下的文件上传到 HDFS 中。其格式为：

&emsp;&emsp;`hdfs dfs -put <local_file_path> <hdfs_directory_path>`

&emsp;&emsp;例如，要将本地文件`/home/user/data.txt`上传到HDFS目录`/user/hadoop/data`中，可以执行以下命令:

&emsp;&emsp;`hdfs dfs -put /home/user/data.txt /user/hadoop/data`

&emsp;&emsp;同样的，你可以使用`get`命令来将 HDFS 中文件下载到本地的Linux系统中，其格式为：

&emsp;&emsp;`hdfs dfs -get <hdfs_file_path> <local_directory_path>`

**（2）Web 可视化访问**

&emsp;&emsp;HDFS 也可通过 Web 可视化的方式访问，默认开放在NameNode的50070端口。在浏览器上打开，如图2-3所示。我们可以在主界面看到一些基本的内容：

&emsp;&emsp;“Overview”右边的“m2:9000”表示当前 HDFS 集群的基本路径，这个值是从配置`core-site.xml`中的`fs.defaultFS`获取到的；接下来紧接着的表格：

1. 第1行-“Started”：表示集群启动的时间；
2. 第2行-“Version”：表示我们使用的Hadoop的版本；
3. 第3行-“Compiled”：表示Hadoop的安装包编译打包的时间，以及编译的作者等信息；
4. 第4行-“Cluster ID”：表示当前HDFS集群的唯一ID；
5. 第5行-“Block Pool ID”：表示当前HDFS的当前的NameNode的ID。


<p align="center">
    <img src="/pic/2/2-3 NameNode网页端.png" width="50%">
    <br/>
    <em>图2-3 NameNode网页端</em>
</p>


&emsp;&emsp;在图中所示页面中，点击上方最右边的`Utilities`项目，再点击`Browse the file system`，就可以在网页上可视化地浏览 HDFS 的目录结构和文件信息。同时在此也可以远程操控，可以将文件远程上传到集群中，或是做修改、移动文件等操作。和`Utilities`同行还有许多项目，读者可自行打开了解。

**（3）JAVA API访问**

&emsp;&emsp;创建maven项目，导入依赖：

```xml
<dependencies>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>3.3.6</version>
    </dependency>

    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>3.3.6</version>
    </dependency>

    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-hdfs</artifactId>
        <version>3.3.6</version>
    </dependency>
</dependencies>
```

&emsp;&emsp;接着需要配置好HDFS集群的一些信息，注意配置信息根据实际情况修改之后连接。

```java
public class HDFSConnection {

    public static void main(String[] args) {

        Configuration conf = new Configuration();

        System.setProperty("HADOOP_USER_NAME", "root");

        conf.set("fs.defaultFS", "hdfs://hdfs-cluster");
        conf.set("dfs.nameservices", "hdfs-cluster");
        conf.set("dfs.ha.namenodes.hdfs-cluster", "nn1,nn2");
        conf.set("dfs.namenode.rpc-address.hdfs-cluster.nn1", "124.71.159.110:9000");
        conf.set("dfs.namenode.rpc-address.hdfs-cluster.nn2", "1.94.115.238:9000");
        conf.set("dfs.client.failover.proxy.provider.hdfs-cluster",
                "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider");
        try {
            FileSystem fs = FileSystem.newInstance(conf);
            System.out.println("Connected to HDFS successfully!");
            fs.close();
        } catch (Exception e) {
            System.err.println("Failed to connect to HDFS:");
            e.printStackTrace();
        }
    }
```

&emsp;&emsp;使用`FileSystem`中的`mkdirs`创建新目录。

```java
public static void ensureParentDirectory(FileSystem fs, String hdfsPath) throws IOException {
    Path dstPath = new Path(hdfsPath);
    Path parentPath = dstPath.getParent();
    if (!fs.exists(parentPath)) {
        fs.mkdirs(parentPath);
        System.out.println("Created directory: " + parentPath.toString());
    }
}
```

&emsp;&emsp;使用`FileSystem`中的`copyToLocalFile`将HDFS为文件系统中的文件下载到本地。

```java
public static void downloadFileFromHDFS(FileSystem fs, String hdfsSrc, String localDst) throws IOException {
        Path srcPath = new Path(hdfsSrc);
        Path dstPath = new Path(localDst);
        fs.copyToLocalFile(false, srcPath, dstPath);
    }
```

&emsp;&emsp;使用`FileSystem`中的`copyFromLocalFile`将本地文件上传到HDFS文件系统中。

```java
public static void uploadFileToHDFS(FileSystem fs, String localSrc, String hdfsDst) throws IOException {
        Path srcPath = new Path(localSrc);
        Path dstPath = new Path(hdfsDst);
        fs.copyFromLocalFile(srcPath, dstPath);
    }
```

_完整的项目文件存储在附录的github仓库中。_

### 2.2.3 HDFS存储文件机制

&emsp;&emsp;经过前面章节的学习，我们已经安装，启动了 HDFS 集群，并使用多种方式来操控这个文件系统，我们可以发现，HDFS集群背后的原理都被隐藏在了几句API调用、几条类Linux命令或者直观的可视化操作后，我们操作HDFS时都把他当成了一个单一的系统来看待，但实际上 HDFS 由多台主机构成，在这其中必然涉及到如何分配数据的存储，如何保证数据不出错，不丢失等一系列问题。而这一系列问题都是由NameNode 和 DataNode的交互来解决。那么接下来，让我们结合实验来探索NameNode和DataNode的交互机制。

**（1）NameNode**

&emsp;&emsp;NameNode 是 HDFS的主控节点，主要负责维护整个文件系统的元数据（MataData），这主要包括：

* 文件与目录结构（类似 Unix 文件系统）
* 文件与块（Block）的映射关系

&emsp;&emsp;这些数据的维护主要由一个数据结构完成，叫做`fsimage`。当我们去添加，修改数据或目录的时候，`fsimage`也应该相应的发生变化。而且，当我们每次启动HDFS集群的时候，这些目录消息不应该消失，而是维持上一次关机时的状态，这就需要`fsimage`持久化地保存在磁盘中。

&emsp;&emsp;可如果`fsimage`只保存在磁盘中，当我们进行操作时，需要频繁地、随机地访问`fsimage`，随机存取磁盘的数据效率必然不高。`fsimage`所在磁盘的性能会制约整个集群的性能，这是我们不希望看到的，因此，当集群启动时，我们可以将`fsimage`读入内存中，后续的操作就只需要和内存交互了，当集群关闭时，再把内存中的`fsimage`写回磁盘中。

&emsp;&emsp;可这样会引入一个新的问题：当NameNode所在主机由于各种原因意外宕机时（如突然断电），内存中的`fsimage`就会直接丢失而来不及保存到磁盘，那么这段时间内的所有对目录和文件的修改就都会丢失了，因为下次开启时将会读取磁盘中未被同步的`fsimage`，也就是发生了一致性问题。

&emsp;&emsp;如果我们每次修改时同步更新磁盘和内存中的`fsimage`，那就丧失了性能优势。因此，在 HDFS 中引入了`Edits`，其保存在磁盘中，每当元数据更新或修改时，其都会记录对应的操作（增删改查等），若NameNode突然宕机，内存中的`fsimage`会丢失，但因为`Edits`保存在磁盘中，其内容不会丢失，当下次启动时，只需将`Edits`上所有记录过的但在`fsimage`上丢失的命令重新执行一次，`fsimage`的内容就能恢复到宕机前的状态，这一步即`Edits`和`fsimage`的合并。

&emsp;&emsp;`Edits`保存在磁盘中，如果每次元数据更新也修改`Edits`，不也会导致和修改磁盘中`fsimage`一样的性能问题吗？其实不然，因为对`Edits`的修改其实只是“追加”，只是依次记录用户的命令而言，是顺序写入，相比随机修改`fsimage`，其性能要好得多。

&emsp;&emsp;`fsimage`和`Edits`保存在`hdfs-site.xml`所配置的`dfs.name.dir`下，若读者配置和本书一致，则其应在`/root/hdfs/namenode/current`下，名为`edits_xxx-xxx`，这些文件后面的数字为其存储的事务记录的ID（TxID）范围。每隔一段时间便会触发一次`Edits`和`fsimage`的合并。`fsimage`文件存储了多个，其后面的数字代表其存储的元数据从初始化起经过了多少次事务，也就是说数字越大的`fsimage`文件越新。

&emsp;&emsp;当NameNode重启时，会按以下顺序恢复元数据：


1. 加载最新的fsimage作为基础状态

2. 按TxID顺序应用所有比该fsimage新的edits
   
&emsp;&emsp;我们可以重启 HDFS 集群来观察此过程，重启后，在NameNode节点的网页端点击`Utilities-Logs`，可看见集群所产生的所有日志文件。我们查看NameNode对应的日志文件，如图2-6。在其中，查找`Loading fsimage`字样，结果如下：
```log
2025-05-15 14:00:55,075 INFO  Loaded image for txid 154 from /root/hdfs/namenode/current/fsimage_0000000000000000154
...
2025-05-15 14:00:55,083 INFO  Start loading edits file ... to transaction ID 155
```

&emsp;&emsp;举例来说，顾客指定 NameNode 这个大厨做了许多道菜，其做出来之后还未呈送给顾客便遭遇意外丢失了，但是有个叫`Edits`的帮工拿笔记下了所有的菜名，待大厨复工后，再把笔记上记录的菜依次完成即可。

**（2）DataNode**

&emsp;&emsp;DataNode 是 HDFS 的工作节点，主要负责存储和管理实际的数据块（Block），并定期向 NameNode 汇报存储状态。它的核心职责包括：

* 将文件数据切分成固定大小的 Block并存储多个副本
* 心跳机制，定时与 NameNode 通信报告状态（默认为三秒）
* 块汇报，定时与NameNode通报节点所存储的块信息

&emsp;&emsp;文件的Block会存储在`hdfs-site.xml`所配置的`dfs.data.dir`下，若读者配置和本书一致，则其应在`/root/hdfs/datanode/current/BP-xxxx/current/finalized`下下，名为`blk_xxxx`的文件，后面的数字为其块ID。当DataNode心跳超时（默认10分钟NameNode未收到该DataNode心跳），NameNode会将该节点状态从Live变为Dead。


**（3）一些小实验**

&emsp;&emsp;为了更好地了解`fsimage`如何管理目录信息，我们准备了一个具有多层结构的文件夹，在其中随机产生了一些数据，其目录结构如下：
```
test_files
    |
    |--maths
    |   |--maths_students    //size = 700MB
    |   |--maths_scores      //size = 200MB
    |
    |--all
        |--teachers
            |
            |--list          //size = 1GB
        

```

&emsp;&emsp;现在，我们将其使用指令`hdfs dfs -put test_files/ /`上传至 HDFS 集群的根目录下，登录NameNode网页端，在图2-4中可看见我们上传的文件。



&emsp;&emsp;在图中，有一些必要重要的信息，首先是`size`标识了文件的大小；接着是`Last Modified`标识了该文件最后修改的时间；`Replication`表示该文件被存储的份数（在图中为3，代表该文件在集群中存储了三份）；`Block Size`代表块大小，前面我们了解了文件会被分块存储（例如`maths_srocres`文件为200MB，则被分为两块）。

&emsp;&emsp;继续点击文件`maths_scores`，可以看见文件的详细信息，如图2-4。在其中，我们可以看见文件各个块的 id，以及 stamp（这是按照产生时间严格递增的），以及存储在哪些DataNode上。

<p align="center">
    <img src="/pic/2/2-5 maths_scores详细信息.png" width="50%">
    <br/>
    <em>图2-4 maths_scores详细信息</em>
</p>

&emsp;&emsp;现在，让我们看看上传该文件夹所对应的事务是如何被记录在`Edits`文件中的，首先，我们需要先找到其对应的事务ID。在NameNode网页端中点击`Utilities-Logs`，可看见集群所产生的所有日志文件。我们查看NameNode对应的日志文件，如图2-5。

<p align="center">
    <img src="/pic/2/2-6 HDFS产生日志.png" width="50%">
    <br/>
    <em>图2-5 HDFS产生日志</em>
</p>

&emsp;&emsp;在其中，我们搜索上传的文件夹名`test_files`，查看首次出现该词汇的附近，从中，我们可以查找到大致的事务ID。可以看到，在事务ID为452时，触发了新建`Edits`操作，而后记录了上传指令，故该上传指令对应的事务记录应该在以452起始的`Edits`文件中。

```log
2025-05-18 17:06:22,005 INFO  Starting log segment at 452
...
2025-05-18 17:06:56,982 INFO  BLOCK* allocate blk_1073741825_1001, replicas=192.168.0.2:9866, 192.168.0.4:9866, 192.168.0.3:9866 for /test_files/maths/maths_students._COPYING_

```

&emsp;&emsp;找到包含事务ID为452的`Edits`，在本机中为`edits_0000000000000000452-0000000000000000514`，使用HDFS自带的命令将该文件解析为XML格式：

`hdfs oev -i ./edits_0000000000000000452-0000000000000000514 -o ./edits.xml -p XML`

&emsp;&emsp;打开该文件，可以看到一个事务记录的组成，首先是事务对应的操作`OPCODE`，在这里，上传该文件夹最先发生的是创建目录；接着是事务ID，操作的路径，操作的用户名，用户组等。
```xml
<RECORD>
    <OPCODE>OP_MKDIR</OPCODE>
    <DATA>
      <TXID>453</TXID>
      <LENGTH>0</LENGTH>
      <INODEID>16390</INODEID>
      <PATH>/test_files</PATH>
      <TIMESTAMP>1747559216761</TIMESTAMP>
      <PERMISSION_STATUS>
        <USERNAME>root</USERNAME>
        <GROUPNAME>supergroup</GROUPNAME>
        <MODE>493</MODE>
      </PERMISSION_STATUS>
    </DATA>
  </RECORD>
```

&emsp;&emsp;后续事务不再展出，紧接着的是创建`test_file/maths`目录，然后便开始存储具体的文件，总的来说，文件被一块一块地存放，大概过程如下：

```xml
<OPCODE>OP_ADD</OPCODE>
<TXID>455</TXID>
<PATH>/test_files/maths/maths_students._COPYING_</PATH>

<OPCODE>OP_ALLOCATE_BLOCK_ID</OPCODE>
<TXID>456</TXID>
<BLOCK_ID>1073741825</BLOCK_ID>

<OPCODE>OP_SET_GENSTAMP_V2</OPCODE>
<TXID>457</TXID>
<GENSTAMPV2>1001</GENSTAMPV2>
```

1. 首先新建（后续为追加）该文件
2. 为该文件申请块ID
3. 申请对应STAMP（对应前文所述）
4. 若该文件大小大于一块，则重复此过程直至存储结束

&emsp;&emsp;接下来，我们模拟一下数据块出错的过程，我们手动删除`math_scores`文件的Block0在NameNode节点所在主机上对应的块，其块ID为`1073741831`：

&emsp;&emsp;`rm rm -f datanode/.../blk_1073741836`

&emsp;&emsp;等待一段时间触发恢复后，在图2-6所示界面中进入`m2`的DataNode日志，搜索`1073741831`，可见到从`m3`中拷贝了一分该数据块到`m2`中，进入`m2`中存储数据块目录，也可看见该块已经恢复。

```
2025-05-18 20:20:44,796 INFO ... Receiving ...blk_1073741831_1007 src: /192.168.0.4:58162 dest: /192.168.0.3:9866
```

### 2.2.4 总结

&emsp;&emsp;HDFS作为Hadoop生态系统的核心存储组件，其设计充分考虑了分布式存储的需求，通过主从架构（NameNode和DataNode）实现了高可靠性、高扩展性的数据存储。在本节中，我们详细介绍了HDFS的基本架构、部署步骤、操作方式。在2.2.3中，我们设计了部分实验简单讨论了 HDFS 集群的底层原理和主从机的交互机制。

&emsp;&emsp;NameNode作为整个集群的核心，存储了文件系统的元数据，若是NameNode节点突然宕机，必会导致整个系统的不可用。在下一节中，我们将介绍Hadoop生态中的一个关键组件——ZooKeeper，看它是如何解决刚刚所提到的问题。