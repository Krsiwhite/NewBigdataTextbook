## 2.2 ZooKeeper：分布式系统的协调者
&emsp;&emsp;Zookeeper是一个开源的分布式协调服务，由Apache基金会开发和维护，主要用于分布式系统的协调管理和通信，其最初是作为Hadoop项目的一部分开发的，目的是解决分布式系统中常见的协调问题，如数据同步、配置管理等。随着分布式系统的发展，Zookeeper逐渐成为一个独立的项目，并被广泛应用于各种分布式系统架构中。

&emsp;&emsp;试想一下，你的餐厅后厨有一名大厨和一名学徒，大厨负责调配后厨中其他厨师有序工作，学徒每天刻苦努力地跟着大厨学习，突然有一天，大厨工作到一半家里有事请假了，那此时作为餐厅老板的你是不是应该马上让学徒接管大厨的工作，继续维持后厨的工作呢？其实ZooKeeper在分布式系统中，也起到了餐厅老板的作用，当某节点出现故障时，让其他节点接替故障节点的工作，保障分布式系统的正常运行，实现高可用性。目前，ZooKeeper已被大量开源系统采用，包括HDFS、YARN、HBase等，本节将通过实验来向大家演示ZooKeeper是如何实现高可用性的，你可以记住上述大厨和学徒的场景与ZooKeeper的高可用性类比学习以加深理解。

### 2.2.1 ZooKeeper与HDFS
&emsp;&emsp;在HDFS中，NameNode负责管理文件系统的元数据，是整个HDFS的核心，如果NameNode所在的机器发生故障，那么整个分布式系统将出现问题，对于这种情况，我们可以通过Zookeeper实现NameNode的高可用性，也就是当NameNode所在的机器出现故障时，选出另一个NameNode来接替其工作，从而避免单点故障，实现高可用性，实现了高可用性的HDFS我们就可以称之为HA-HDFS（High Available HDFS），那接下来我们开始体验一下ZooKeeper是如何实现HDFS的高可用性的吧。

&emsp;&emsp;在正式开始前，我们还是先给我们要完成的操作列个清单：
1. 安装与配置ZooKeeper  
2. 验证ZooKeeper启动效果
3. 初始化HA-HDFS配置
4. 首次启动HA-HDFS
5. 再次启动HA-HDFS
6. 验证HA-HDFS高可用性
7. 回顾组件都做了什么


&emsp;&emsp;在2.2中，我们已经部署了HDFS集群，在HDFS集群中，NameNode就像后厨场景中的大厨，负责管理，如果NameNoode挂掉了，那么整个集群将出现问题，因此我们需要对其进行改进，实现其高可用性。

**（1）安装与配置ZooKeeper**

_先在m1主机进行如下安装与配置，完成后同步到主机m2和m3_

&emsp;&emsp;本书实验采用ZooKeeper的3.8.4版本，我们可以从清华镜像源获取下载，下载在`/root`目录下。

&emsp;&emsp;`wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.8.4/apache-zookeeper-3.8.4-bin.tar.gz`

&emsp;&emsp;接着将其解压到当前目录即可。

&emsp;&emsp;`tar -zxvf apache-zookeeper-3.8.4-bin.tar.gz`
 
&emsp;&emsp;接下来我们需要为安装的ZooKeeper创建`zoo.cfg`配置文件，将`/root/apache-zookeeper-3.8.4-bin/conf`中提供的`zoo_sample.cfg`模板复制一份并将其重命名为`zoo.cfg`。

&emsp;&emsp;`cp /root/apache-zookeeper-3.8.4-bin/conf/zoo_sample.cfg /root/apache-zookeeper-3.8.4-bin/conf/zoo.cfg`

&emsp;&emsp;修改`zoo.cfg`配置文件,可设置`dataDir=/data/zookeeper/data`项为指定的data目录，同时添加ZooKeeper需要用到的主机信息。

&emsp;&emsp;主机信息的配置格式为server.A=B:C:D，其中A表示这是第几号服务器，B是这个服务器的地址，这里是m1、m2和m3的原因是我们设置过ip地址的映射，C是这个服务器Follower与集群中的Leader服务器交换信息的端口，D是万一集群中的Leader服务器挂了，需要一个端口来重新选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。
```
dataDir=/root/apache-zookeeper-3.8.4-bin/zkData
server.1=m1:2888:3888
server.2=m2:2888:3888
server.3=m3:2888:3888
```

&emsp;&emsp;创建上述步骤中指定的用于存储数据的data目录。

&emsp;&emsp;`mkdir /root/apache-zookeeper-3.8.4-bin/zkData`

&emsp;&emsp;编辑环境变量⽂件/etc/profile.d/hadoopenv.sh,文件尾部加⼊以下配置。
```
# ZK_HOME 
export ZK_HOME=/root/apache-zookeeper-3.8.4-bin
export PATH=$PATH:$ZK_HOME/bin
```

&emsp;&emsp;将环境变量和zookeeper目录同步到主机m2和m3。
```
rsync -av /root/apache-zookeeper-3.8.4-bin m2:/root 
rsync -av /root/apache-zookeeper-3.8.4-bin m3:/root 
sudo rsync -av /etc/profile.d/hadoopenv.sh m2:/etc/profile.d
sudo rsync -av /etc/profile.d/hadoopenv.sh m3:/etc/profile.d
```

_三台主机都需要重新加载环境变量_

&emsp;&emsp;对于已经开启的终端,需要重新加载环境变量。

&emsp;&emsp;`source /etc/profile`

_主机m1执行下面的命令_

&emsp;&emsp;将1写入`myid`文件，设置主机m1投票编号为1。

&emsp;&emsp;`echo 1 >> /root/apache-zookeeper-3.8.4-bin/zkData/myid`

_主机m2执行下面的命令_

&emsp;&emsp;将2写入`myid`文件，设置主机m2的投票编号为2。

&emsp;&emsp;`echo 2 >> /root/apache-zookeeper-3.8.4-bin/zkData/myid`

_主机m3执行下面的命令_

&emsp;&emsp;将3写入`myid`文件，设置主机m3的投票编号为3。

&emsp;&emsp;`echo 3 >> /root/apache-zookeeper-3.8.4-bin/zkData/myid`

**（2）验证ZooKeeper启动效果**

_三台主机都需要执行如下命令_

&emsp;&emsp;执行如下命令开启ZooKeeper.

&emsp;&emsp;`zkServer.sh start`

&emsp;&emsp;在各主机查看ZooKeeperk状态，发现其中有一台主机为Leader，另外两台主机为Follower。

&emsp;&emsp;`zkServer.sh status`

```
root@0001:~/apache-zookeeper-3.8.4-bin/zkData# zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /root/apache-zookeeper-3.8.4-bin/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: follower
```

``` 
root@0002:~/apache-zookeeper-3.8.4-bin/zkData# zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /root/apache-zookeeper-3.8.4-bin/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: leader
```

```
root@0003:~/apache-zookeeper-3.8.4-bin/zkData# zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /root/apache-zookeeper-3.8.4-bin/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: follower
```

&emsp;&emsp;验证后使用如下命令关闭ZooKeeper。

&emsp;&emsp;`zkServer.sh stop`
 
**（3）初始化HA-HDFS配置**

_三台主机都需要执行如下操作_

&emsp;&emsp;全新的Haddoop集群，要删除原先的`data`目录和`logs`目录。
```
rm -rf /root/hdfs
rm -rf /root/hadoop-3.3.6/logs
```

_主机m1执行如下操作_

&emsp;&emsp;修改`/root/hadoop-3.3.6/etc/hadoop/core-site.xml`中内容为。
```
<configuration>
    <!--hdfs入口，设置虚拟地址，具体地址后面配置-->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hdfs-cluster</value>
    </property>
  <!-- HDFS web loggin static user -->
  <property>
    <name>hadoop.http.staticuser.user</name>
    <value>bigdata_chef</value>
  </property> 
    <!--hdfs要访问zookeeper集群-->
    <property>
        <name>ha.zookeeper.quorum</name>
        <value>m1:2181,m2:2181,m3:2181</value>
    </property>
</configuration>
```

&emsp;&emsp;修改`/root/hadoop-3.3.6/etc/hadoop/hdfs-site.xml`中内容为。
```
<configuration>
    <property>
    <name>dfs.name.dir</name>
    <value>/root/hdfs/namenode</value>
    </property>
    <property>
    <name>dfs.data.dir</name>
    <value>/root/hdfs/datanode</value>
    </property>
    <!-- 副本数 -->
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <!-- 定义dhfs入口的命名服务 -->
    <property>
        <name>dfs.nameservices</name>
        <value>hdfs-cluster</value>
    </property>
    <!-- 定义hdfs入口的命名服务下虚拟ip-->
    <property>
        <name>dfs.ha.namenodes.hdfs-cluster</name>
        <value>nn1,nn2</value>
    </property>
    <!-- 虚拟ip地址1 RPC入口 -->
    <property>
        <name>dfs.namenode.rpc-address.hdfs-cluster.nn1</name>
        <value>m1:9000</value>
    </property>
    <!-- 虚拟ip地址1 HTTP入口 -->
    <property>
        <name>dfs.namenode.http-address.hdfs-cluster.nn1</name>
        <value>m1:50070</value>
    </property>
    <!-- 虚拟ip地址2 PRC入口 -->
    <property>
        <name>dfs.namenode.rpc-address.hdfs-cluster.nn2</name>
        <value>m2:9000</value>
    </property>
    <!-- 虚拟ip地址1 HTTP入口 -->
    <property>
        <name>dfs.namenode.http-address.hdfs-cluster.nn2</name>
        <value>m2:50070</value>
    </property>
    
    <!-- 定义QJN在linux中保存文件磁盘目录 -->
    <property>
        <!-- Journal Edit Files 的存储目录:() -->
        <name>dfs.journalnode.edits.dir</name>
        <value>/root/journalnode/data/</value>
    </property>
    <!-- namenode要向zk的QJN写入editslog，所以要明确入口地址 -->
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://m1:8485;m2:8485;m3:8485/hdfs-cluster</value>
    </property>

    <!-- 是否开启故障切换 -->
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>

    <!-- 基于zookeeper的故障切换的代码类 -->
    <property>
        <name>dfs.client.failover.proxy.provider.hdfs-cluster</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    
    
    <!-- 远程杀死namenode方式(防止namenode假死，导致双主出现) -->
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>shell(/bin/true)</value>
    </property>
    
    <!-- 指定私钥的文件目录，使用免密登录杀死NN进程 -->
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/root/.ssh/id_rsa</value>
    </property> 
</configuration>
```

&emsp;&emsp;将配置分发到其他节点。
```
scp  -r  /root/hadoop-3.3.6/etc/hadoop root@m2:/root/hadoop-3.3.6/etc/
scp  -r  /root/hadoop-3.3.6/etc/hadoop root@m3:/root/hadoop-3.3.6/etc/
```

**（4）首次启动HA-HDFS**

_三台主机都需要执行如下操作_

&emsp;&emsp;3台主机启动ZooKeeper集群。

&emsp;&emsp;`zkServer.sh start`

_主机m1执行如下操作_

&emsp;&emsp;执行初始化ZKFC在ZooKeeper中的Znode信息。

&emsp;&emsp;`hdfs zkfc -formatZK`

&emsp;&emsp;修改`/root/hadoop-3.3.6/sbin/start-dfs.sh`和`/root/hadoop-3.3.6/sbin/stop-dfs.sh`，在文件头部都加入以下内容。

```
HDFS_ZKFC_USER=root
HDFS_JOURNALNODE_USER=root
```

_三台主机都需要执行如下操作_

&emsp;&emsp;启动journalnode(QJN将作为NameNode存储持久化文件的空间，要先启动才能格式化)。

&emsp;&emsp;`hdfs --daemon start journalnode`

_主机m1执行如下操作_

&emsp;&emsp;格式化namenode主机。

&emsp;&emsp;`hdfs namenode -format`

&emsp;&emsp;启动整个namenode集群。

&emsp;&emsp;`start-dfs.sh`

_主机m2执行如下操作_

&emsp;&emsp;声明此机器为namenode备机。

&emsp;&emsp;`hdfs namenode -bootstrapStandby`

&emsp;&emsp;启动namenode备机。

&emsp;&emsp;`hdfs --daemon.sh start namenode`

**（5）再次启动HA-HDFS**

_主机m1执行如下操作_

&emsp;&emsp;使用如下命令关闭HA-Hadoop。

&emsp;&emsp;`stop-dfs.sh`

_三台主机都需要执行如下操作_

&emsp;&emsp;使用如下命令关闭ZooKeeper。

&emsp;&emsp;`zkServer.sh stop`


_三台主机都需要执行如下操作_

&emsp;&emsp;此时集群已关闭，接下来开始重新打开集群，使用如下命令打开ZooKeeper。

&emsp;&emsp;`zkServer.sh start`

_主机m1执行如下操作_

&emsp;&emsp;使用如下命令打开HA-Hadoop。

&emsp;&emsp;`start-dfs.sh`

_三台主机都需要执行如下操作_

&emsp;&emsp;此时我们在m1、m2和m3查看服务，发现m1和m2均为namenode。

&emsp;&emsp;`jps`

```
root@0001:~/hadoop-3.3.6/sbin# jps
143137 Jps
142548 DataNode
142186 QuorumPeerMain
143017 DFSZKFailoverController
142399 NameNode
142780 JournalNode
```

```
root@0002:~# jps
133584 Jps
133489 DFSZKFailoverController
133078 NameNode
133191 DataNode
133319 JournalNode
132939 QuorumPeerMain
```

```
root@0003:~# jps
136770 DataNode
136898 JournalNode
137029 Jps
136639 QuorumPeerMain
```

**（6）验证HA-HDFS高可用性**

&emsp;&emsp;在2.2节中，只有m1是namenode，而此时我们m1和m2均为namenode，m2即为m1的备用节点，此时如果我们杀死m1主机，那么m2会顶替m1的位置吗？接下来我们验证一下。

_主机m3执行如下操作_

&emsp;&emsp;查看m1主机和m2主机节点的状态，发现m1主机的节点状态为active，m2主机的节点状态为standby。

&emsp;&emsp;`hdfs haadmin -getServiceState nn1`

&emsp;&emsp;`hdfs haadmin -getServiceState nn2`

```
root@0003:~# hdfs haadmin -getServiceState nn1
active
```

```
root@0003:~# hdfs haadmin -getServiceState nn2
standby
```

_主机m1执行如下操作_

&emsp;&emsp;关闭namenode，模拟主节点出现故障。

&emsp;&emsp;`hdfs --daemon stop namenode`

```
root@0001:~/hadoop-3.3.6/sbin# jps
144561 Jps
142548 DataNode
142186 QuorumPeerMain
143017 DFSZKFailoverController
142780 JournalNode
```

_主机m3执行如下操作_

&emsp;&emsp;查看m1主机和m2主机节点的状态，发现m1主机的节点状态无法查看，m2主机的节点状态为active，这说明备用节点接替了主节点的工作。

&emsp;&emsp;`hdfs haadmin -getServiceState nn1`

&emsp;&emsp;`hdfs haadmin -getServiceState nn2`

```
root@0003:~# hdfs haadmin -getServiceState nn1
2025-05-18 12:15:55,525 INFO ipc.Client: Retrying connect to server: m1/192.168.0.2:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)
Operation failed: Call From 0003/0.0.0.3 to m1:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
```

```
root@0003:~# hdfs haadmin -getServiceState nn2
active
```