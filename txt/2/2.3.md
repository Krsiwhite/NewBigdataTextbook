## 2.3 ZooKeeper：分布式系统的协调者
&emsp;&emsp;Zookeeper是一个开源的分布式协调服务，由Apache基金会开发和维护，主要用于分布式系统的协调管理和通信，其最初是作为Hadoop项目的一部分开发的，目的是解决分布式系统中常见的协调问题，如数据同步、配置管理等。随着分布式系统的发展，Zookeeper逐渐成为一个独立的项目，并被广泛应用于各种分布式系统架构中。

&emsp;&emsp;试想一下，你的餐厅后厨有一名大厨和一名学徒，大厨负责调配后厨中其他厨师有序工作，学徒每天刻苦努力地跟着大厨学习，突然有一天，大厨工作到一半家里有事请假了，那此时作为餐厅老板的你是不是应该马上让学徒接管大厨的工作，继续维持后厨的工作呢？其实ZooKeeper在分布式系统中，也起到了餐厅老板的作用，当某节点出现故障时，让其他节点接替故障节点的工作，保障分布式系统的正常运行，这样的一种特性就叫做高可用性（High Available, HA）。目前，ZooKeeper已被大量开源的分布式系统采用，包括HDFS、YARN、HBase等，本节将通过实验来向大家演示ZooKeeper是如何实现高可用性的，你可以记住上述大厨和学徒的场景与ZooKeeper的高可用性类比学习以加深理解。

### 2.3.1 ZooKeeper与HDFS

&emsp;&emsp;在HDFS中，NameNode负责管理文件系统的元数据，是整个HDFS的核心，存储了整个文件系统的目录结构等，它就像后厨场景中的大厨，负责管理，如果NameNode所在的机器发生故障，那么整个分布式系统将出现问题，对于这种在一个系统中，由于某一部件的故障，导致整个系统或网络无法正常工作的情况，叫做单点故障（Single Point of Failure）。


&emsp;&emsp;我们可以通过 ZooKeeper 实现NameNode的高可用性，ZooKeeper集群会和 HDFS 集群协同工作：HDFS集群中会同时存在多个 NameNode节点，但只有一个是在真正工作的活跃（Active）状态，其他都处于待命（Standby）状态。与此同时，Zookeeper集群中存在一个Leader节点和多个Follower节点来保证工作，Follower节点运行在集群的其余所有主机上，当Active的NameNode节点故障时，会将一个Standby的NameNode节点转变为Active来继续工作，从而避免单点故障，实现了高可用性。实现了高可用性的HDFS我们就可以称之为HA-HDFS（High Available HDFS），接下来我们将配置ZooKeeper集群，同时修改 HDFS 集群相关配置使二者协同工作，从而实现HA-HDFS。

&emsp;&emsp;在正式开始前，我们首先给要完成的操作列个清单：

1. 安装与配置ZooKeeper  
2. 单独验证ZooKeeper集群
3. 初始化HA-HDFS配置
4. 首次启动HA-HDFS
5. 再次启动HA-HDFS
6. 验证HA-HDFS高可用性


**（1）安装与配置ZooKeeper**

_先在m1主机进行如下安装与配置，完成后同步到主机m2和m3_

&emsp;&emsp;本书实验采用ZooKeeper的3.8.4版本，我们可以从清华镜像源获取下载，下载在`/root`目录下。

&emsp;&emsp;`wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.8.4/apache-zookeeper-3.8.4-bin.tar.gz`

&emsp;&emsp;接着将其解压到当前目录即可。

&emsp;&emsp;`tar -zxvf apache-zookeeper-3.8.4-bin.tar.gz`
 
&emsp;&emsp;接下来我们需要为安装的ZooKeeper创建`zoo.cfg`配置文件，将`/root/apache-zookeeper-3.8.4-bin/conf`中提供的`zoo_sample.cfg`模板复制一份并将其重命名为`zoo.cfg`。

&emsp;&emsp;`cp /root/apache-zookeeper-3.8.4-bin/conf/zoo_sample.cfg /root/apache-zookeeper-3.8.4-bin/conf/zoo.cfg`

&emsp;&emsp;修改`zoo.cfg`配置文件,可设置`dataDir=/data/zookeeper/data`项为指定的data目录，同时添加ZooKeeper需要用到的主机信息。

&emsp;&emsp;主机信息的配置格式为server.A=B:C:D，其中A表示这是第几号服务器，B代表这个服务器的地址，这里是m1、m2和m3的原因是我们设置过ip地址的映射，C指定了该服务器Follower与集群中的Leader服务器交换信息的端口，D指定了执行选举时服务器间相互通信的端口，当集群中的Leader节点产生故障时，需要该端口来重新选举一个新的Leader。


```
dataDir=/root/apache-zookeeper-3.8.4-bin/zkData
server.1=m1:2888:3888
server.2=m2:2888:3888
server.3=m3:2888:3888
```

&emsp;&emsp;创建上述步骤中指定的用于存储数据的data目录。

&emsp;&emsp;`mkdir /root/apache-zookeeper-3.8.4-bin/zkData`

&emsp;&emsp;编辑环境变量⽂件/etc/profile.d/hadoopenv.sh,文件尾部加⼊以下配置。
```
# ZK_HOME 
export ZK_HOME=/root/apache-zookeeper-3.8.4-bin
export PATH=$PATH:$ZK_HOME/bin
```

&emsp;&emsp;将环境变量和zookeeper目录同步到主机m2和m3。
```
rsync -av /root/apache-zookeeper-3.8.4-bin m2:/root 
rsync -av /root/apache-zookeeper-3.8.4-bin m3:/root 
sudo rsync -av /etc/profile.d/hadoopenv.sh m2:/etc/profile.d
sudo rsync -av /etc/profile.d/hadoopenv.sh m3:/etc/profile.d
```

_三台主机都需要重新加载环境变量_

&emsp;&emsp;对于已经开启的终端,需要重新加载环境变量。

&emsp;&emsp;`source /etc/profile`

_主机m1执行下面的命令_

&emsp;&emsp;将1写入`myid`文件，设置主机m1投票编号为1。

&emsp;&emsp;`echo 1 >> /root/apache-zookeeper-3.8.4-bin/zkData/myid`

_主机m2执行下面的命令_

&emsp;&emsp;将2写入`myid`文件，设置主机m2的投票编号为2。

&emsp;&emsp;`echo 2 >> /root/apache-zookeeper-3.8.4-bin/zkData/myid`

_主机m3执行下面的命令_

&emsp;&emsp;将3写入`myid`文件，设置主机m3的投票编号为3。

&emsp;&emsp;`echo 3 >> /root/apache-zookeeper-3.8.4-bin/zkData/myid`

**（2）单独验证ZooKeeper集群**

_三台主机都需要执行如下命令_

&emsp;&emsp;执行如下命令开启ZooKeeper.

&emsp;&emsp;`zkServer.sh start`

&emsp;&emsp;在各主机查看ZooKeeperk状态，发现其中有一台主机为Leader，另外两台主机为Follower。

&emsp;&emsp;`zkServer.sh status`

```
root@0001:~/apache-zookeeper-3.8.4-bin/zkData# zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /root/apache-zookeeper-3.8.4-bin/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: follower
```

``` 
root@0002:~/apache-zookeeper-3.8.4-bin/zkData# zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /root/apache-zookeeper-3.8.4-bin/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: leader
```

```
root@0003:~/apache-zookeeper-3.8.4-bin/zkData# zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /root/apache-zookeeper-3.8.4-bin/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: follower
```

&emsp;&emsp;验证后使用如下命令关闭ZooKeeper集群。

&emsp;&emsp;`zkServer.sh stop`
 
**（3）初始化HA-HDFS配置**

_三台主机都需要执行如下操作_

&emsp;&emsp;全新的Haddoop集群，需删除原先的`data`目录和`logs`目录。
```
rm -rf /root/hdfs
rm -rf /root/hadoop-3.3.6/logs
```

_主机m1执行如下操作_

&emsp;&emsp;修改`/root/hadoop-3.3.6/etc/hadoop/core-site.xml`中内容为：

```
<configuration>
    <!--hdfs入口，设置虚拟地址，具体地址后面配置-->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hdfs-cluster</value>
    </property>
  <!-- HDFS web loggin static user -->
  <property>
    <name>hadoop.http.staticuser.user</name>
    <value>bigdata_chef</value>
  </property> 
    <!--hdfs要访问zookeeper集群-->
    <property>
        <name>ha.zookeeper.quorum</name>
        <value>m1:2181,m2:2181,m3:2181</value>
    </property>
</configuration>
```

&emsp;&emsp;修改`/root/hadoop-3.3.6/etc/hadoop/hdfs-site.xml`中内容为：

```
<configuration>
    <property>
    <name>dfs.name.dir</name>
    <value>/root/hdfs/namenode</value>
    </property>
    <property>
    <name>dfs.data.dir</name>
    <value>/root/hdfs/datanode</value>
    </property>
    <!-- 副本数 -->
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <!-- 定义dhfs入口的命名服务 -->
    <property>
        <name>dfs.nameservices</name>
        <value>hdfs-cluster</value>
    </property>
    <!-- 定义hdfs入口的命名服务下虚拟ip-->
    <property>
        <name>dfs.ha.namenodes.hdfs-cluster</name>
        <value>nn1,nn2</value>
    </property>
    <!-- 虚拟ip地址1 RPC入口 -->
    <property>
        <name>dfs.namenode.rpc-address.hdfs-cluster.nn1</name>
        <value>m1:9000</value>
    </property>
    <!-- 虚拟ip地址1 HTTP入口 -->
    <property>
        <name>dfs.namenode.http-address.hdfs-cluster.nn1</name>
        <value>m1:50070</value>
    </property>
    <!-- 虚拟ip地址2 PRC入口 -->
    <property>
        <name>dfs.namenode.rpc-address.hdfs-cluster.nn2</name>
        <value>m2:9000</value>
    </property>
    <!-- 虚拟ip地址1 HTTP入口 -->
    <property>
        <name>dfs.namenode.http-address.hdfs-cluster.nn2</name>
        <value>m2:50070</value>
    </property>
    
    <!-- 定义QJN在linux中保存文件磁盘目录 -->
    <property>
        <!-- Journal Edit Files 的存储目录:() -->
        <name>dfs.journalnode.edits.dir</name>
        <value>/root/journalnode/data/</value>
    </property>
    <!-- namenode要向zk的QJN写入editslog，所以要明确入口地址 -->
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://m1:8485;m2:8485;m3:8485/hdfs-cluster</value>
    </property>

    <!-- 是否开启故障切换 -->
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>

    <!-- 基于zookeeper的故障切换的代码类 -->
    <property>
        <name>dfs.client.failover.proxy.provider.hdfs-cluster</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    
    
    <!-- 远程杀死namenode方式(防止namenode假死，导致双主出现) -->
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>shell(/bin/true)</value>
    </property>
    
    <!-- 指定私钥的文件目录，使用免密登录杀死NN进程 -->
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/root/.ssh/id_rsa</value>
    </property> 
</configuration>
```

&emsp;&emsp;将配置分发到其他节点。
```
scp  -r  /root/hadoop-3.3.6/etc/hadoop root@m2:/root/hadoop-3.3.6/etc/
scp  -r  /root/hadoop-3.3.6/etc/hadoop root@m3:/root/hadoop-3.3.6/etc/
```

**（4）首次启动HA-HDFS**

_三台主机都需要执行如下操作_

&emsp;&emsp;3台主机启动ZooKeeper集群：

&emsp;&emsp;`zkServer.sh start`

_主机m1执行如下操作_

&emsp;&emsp;执行初始化ZKFC在ZooKeeper中的Znode信息：

&emsp;&emsp;`hdfs zkfc -formatZK`

&emsp;&emsp;修改`/root/hadoop-3.3.6/sbin/start-dfs.sh`和`/root/hadoop-3.3.6/sbin/stop-dfs.sh`，在文件头部都加入以下内容：

```
HDFS_ZKFC_USER=root
HDFS_JOURNALNODE_USER=root
```

_三台主机都需要执行如下操作_

&emsp;&emsp;启动journalnode（QJN将作为NameNode存储持久化文件的空间，要先启动才能格式化）。

&emsp;&emsp;`hdfs --daemon start journalnode`

_主机m1执行如下操作_

&emsp;&emsp;格式化NameNode主机。

&emsp;&emsp;`hdfs namenode -format`

&emsp;&emsp;启动整个NameNode集群。

&emsp;&emsp;`start-dfs.sh`

_主机m2执行如下操作_

&emsp;&emsp;声明此机器为NameNode备机。

&emsp;&emsp;`hdfs namenode -bootstrapStandby`

&emsp;&emsp;启动NameNode备机。

&emsp;&emsp;`hdfs --daemon.sh start namenode`

**（5）再次启动HA-HDFS**

&emsp;&emsp;经过首次启动并初始化后，我们已经搭建好了HA-HDFS。之后的启动过程如下：

&emsp;&emsp;首先关闭在（4）中初始化好的HA-HDFS。关闭时，使用`stop-dfs.sh`命令关闭HA-Hadoop集群。

&emsp;&emsp;使用`zkServer.sh stop`命令关闭主机中的ZooKeeper集群相关节点，在所有主机上执行以关闭ZooKeeper集群，至此完整关闭了HA-HDFS。

&emsp;&emsp;在今后的启动中，执行命令如下：首先在集群所有主机中执行`zkServer.sh start`启动ZooKeeper集群。而后在一主机上执行`start-dfs.sh`以启动HA-HDFS。

&emsp;&emsp;若配置正确，此时我们在m1、m2和m3使用`jps`命令，结果如下：


```
root@0001:~/hadoop-3.3.6/sbin# jps
143137 Jps
142548 DataNode
142186 QuorumPeerMain
143017 DFSZKFailoverController
142399 NameNode
142780 JournalNode
```

```
root@0002:~# jps
133584 Jps
133489 DFSZKFailoverController
133078 NameNode
133191 DataNode
133319 JournalNode
132939 QuorumPeerMain
```

```
root@0003:~# jps
136770 DataNode
136898 JournalNode
137029 Jps
136639 QuorumPeerMain
```











**（6）验证HA-HDFS高可用性**

&emsp;&emsp;在2.2节中，只有m1是namenode，而此时我们m1和m2均为namenode，m2即为m1的备用节点，此时如果我们杀死m1主机，那么m2会顶替m1的位置吗？接下来我们验证一下。

_主机m3执行如下操作_

&emsp;&emsp;查看m1主机和m2主机节点的状态，发现m1主机的节点状态为active，m2主机的节点状态为standby。

&emsp;&emsp;`hdfs haadmin -getServiceState nn1`

&emsp;&emsp;`hdfs haadmin -getServiceState nn2`

```
root@0003:~# hdfs haadmin -getServiceState nn1
active
```

```
root@0003:~# hdfs haadmin -getServiceState nn2
standby
```

_主机m1执行如下操作_

&emsp;&emsp;关闭namenode，模拟主节点出现故障。

&emsp;&emsp;`hdfs --daemon stop namenode`

```
root@0001:~/hadoop-3.3.6/sbin# jps
144561 Jps
142548 DataNode
142186 QuorumPeerMain
143017 DFSZKFailoverController
142780 JournalNode
```

_主机m3执行如下操作_

&emsp;&emsp;查看m1主机和m2主机节点的状态，发现m1主机的节点状态无法查看，m2主机的节点状态为active，这说明备用节点接替了主节点的工作。

&emsp;&emsp;`hdfs haadmin -getServiceState nn1`

&emsp;&emsp;`hdfs haadmin -getServiceState nn2`

```
root@0003:~# hdfs haadmin -getServiceState nn1
2025-05-18 12:15:55,525 INFO ipc.Client: Retrying connect to server: m1/192.168.0.2:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)
Operation failed: Call From 0003/0.0.0.3 to m1:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
```

```
root@0003:~# hdfs haadmin -getServiceState nn2
active
```

   ###   2.2.2、Zookeeper和Yarn
&emsp;&emsp;Yarn是一个快速、可靠且安全的依赖管理工具，与npm类似，你可以通过Yarn使用全世界开发者的代码，也可以通过其分享你自己的代码。代码将会通过包（package）或者模块（module）的方式来共享。一个包中包含所有需要共享的代码，以及该包信息的描述性文件，称其为package.json。

&emsp;&emsp;Yarn官网（https://classic.yarnpkg.com/）的介绍，Yarn具有速度快、可靠、安全的特点。

1. 快速（Fast）：Yarn 缓存了每个下载过的包，所以再次使用时无需重复下载。同时利用并行下载以最大化资源利用率，因此安装速度更快.
2. 可靠(Reliable): 使用详细、简洁的锁文件格式和明确的安装算法，Yarn能够保证在不同系统上无差异的工作。
3. 安全(secure): 在执行代码之前，Yarn 会通过算法校验每个安装包的完整性。

<p align="center">
    <img src="pic/2/2-3 Yarn官网.jpg" width="50%">
    <br/>
    <em>图2-3 Yarn官网</em>
</p>

&emsp;&emsp;Yarn作为一个通用的资源管理系统，其目标是将短作业和长服务混合部署到一个集群中，并为它们提供统一的资源管理和任务调度功能。

&emsp;&emsp;资源管理：将多台机器的资源进行整合，构建成一个整体。
    整个集群中所有资源的分配。
    运行这个程序分配多少资源给它运行。

&emsp;&emsp;任务调度：提交运行一个分布式程序，利用分布式的资源来实现运行。
    当提交了多个程序时，谁先执行，谁后执行的问题。

&emsp;&emsp;Yarn总体上采用master/slave架构，其中，ResourceManager 为master，NodeManager为slave, ResourceManager负责对各个NodeManager上的资源进行统一管理和调度。

<p align="center">
    <img src="pic/2/2-4Yarn架构.jpg" width="50%">
    <br/>
    <em>图2-4 Yarn架构</em>
</p>

&emsp;&emsp;ResourceManager (RM) ：是一个全局的资源管理器，负责整个系统的资源管理和分配。由两个组件构成：调度器(Scheduler) 和应用管理器(Applications Manager, ASM)。

1. 调度器： 主要功能是根据资源容量，队列等方面的限制条件，将系统中的资源分配给各个应用程序；
2. 应用管理器：负责管理整个系统中的所有应用程序。

&emsp;&emsp;ApplicationMaster (AM) ：用户提交的每个应用程序均包含一个独立的AM,其主要功能包括：

1. 与RM调度器协商以获取资源（用Container表示）；
2. 将得到的资源进一步分配给内部的任务；
3. 与NodeManager通信以启动/停止任务；
4. 监控所有任务的运行状态，并在任务运行失败时重新为任务申请资源以重启任务。

&emsp;&emsp;NodeManager (NM) ： NM是每个节点上的资源管理器。其主要功能包括：

1. 会定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态；
2. 接收并处理来自AM的任务启动/停止等各种请求。在一个集群中，NM通常存在多个，由于Yarn内置了容错机制，单个NM的故障不会对集群中的应用程序运行产生严重影响。

&emsp;&emsp;Container ： 是Yarn中的基本资源分配单位，是对应用程序运行环境的抽象，并为应用程序提供资源隔离环境。Container最终是由ContainerExecutor启动和运行的，Yarn提供了三种可选的ContainerExecutor:

1. DefaultContainerExecutor：默认ContainerExecutor实现，直接以进程方式启动Container,不提供任何隔离机制和安全机制。
2. LinuxContainerExecutor：提供了安全和Cgroups隔离的ContainerExecutor，它以应用程序提交者的身份运行Container，且使用Cgroups为Container提供CPU和内存隔离的运行环境。
3. DockerContainerExecutor：基于Docker实现的ContainerExecutor,可直接在YARN集群中运行Docker Container。

&emsp;&emsp;Yarn工作流程：

1. 提交应用程序：用户通过客户端与YARN ResourceManager通信，以提交应用程序，应用程序中需包含ApplicationMaster可执行代码、启动命令和资源需求、应用程序可执行代码和资源需求、优先级、提交到的队列等信息。
2. 启动ApplicationMaster： ResourceManager为该应用程序分配第一个Container,并与对应的NodeManager通信，要求它在这个Container中启动应用程序的ApplicationMaster，之后ApplicationMaster的生命周期直接被ResourceManager管理。
3. ApplicationMaster注册： ApplicationMaster启动后，首先向ResourceManager注册，这样，用户可以直接通过ResourceManager查看应用程序的运行状态，然后，它将初始化应用程序，并按照一定的策略为内部任务申请资源，监控它们的运行状态，直到运行结束，即重复步骤
4. 资源获取: ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源。
5. 请求启动Container ：一旦ApplicationMaster申请到资源后，则与对应的NodeManager通信，请求为其启动任务（NodeManager会将任务放到Container中）。
6. 启动Container：NodeManager为任务设置好运行环境(包括环境变量、jar包、二进制程序等)后，将任务启动命令写到一个脚本中，并通过ContainerExecutor运行该脚本启动任务。
7. Container监控：ApplicationMaster可通过两种方式获取各个Container的运行状态，以便在任务失败时重新启动任务。
ApplicationMaster与ResourceManager间维护了周期性心跳信息，每次通信可获取自己分管的Container的运行状态。
各个Container可通过某个RPC协议向ApplicationMaster汇报自己的状态和进度。
8. 注销ApplicationMaster：应用程序运行完成后，ApplicationMaster 向ResourceManager注销，并退出执行。

<p align="center">
    <img src="pic/2/2-5 Yarn工作流程.jpg" width="50%">
    <br/>
    <em>图2-5 Yarn工作流程</em>
</p>

&emsp;&emsp;Yarn资源调度器：资源调度器是HadoopYARN中最核心的组件之一，它是ResourceManager中的一个插拔式服务组件，负责整个集群资源的管理和分配。

&emsp;&emsp;Hadoop最初是为批处理作业而设计的，当时(MRv1) 仅提供了一种简单的FIFO (First InFirst Out)调度机制分配任务。在Hadoop 0.20.x版本或者更早的版本，Hadoop采用了平级队列组织方式：管理员将用户和资源分到若干个扁平队列中，在每个队列中，可指定一个或几个队列管理员管理这些用户和资源，比如杀死任意用户的应用程序，修改任意用户应用程序的优先级等。随着Hadoop应用越来越广泛，扁平化的队列组织方式已不能满足实际需求，从而出现了层级队列组织方式。

&emsp;&emsp;下面给出一个层级队列管理的例子：
在一个Hadoop集群中，管理员将所有计算资源划分给了两个队列，每个队列对应一个“组织”，其中有一个组织叫 “Engineering”，占用系统总资源的60%，它内部包含两个子队列“Development”和“QA”，分别占用80%和20%的资源；另一个组织叫”Marketing“，占用系统总资源的40%，它内部也包含两个子队列“Sales”和“Advertising”，分别占用30%和70%的资源。

<p align="center">
    <img src="pic/2/2-6 层次队列组织方式.jpg" width="50%">
    <br/>
    <em>图2-6 层次队列组织方式</em>
</p>

&emsp;&emsp;在实际生产环境中，对于“Engineering”队列而言，管理员可能想更有效地控制这60%资源，比如将大部分资源分配给“Development”队列的同时，能够让“QA”有最少资源保证，当“Development”中80%基础资源有剩余时，可优先共享给同父子队列“QA”，但为了防止“QA”一次性获得全部资源以至于“Development”需要资源时无法第一时间回收它们，可将“QA”最多可获得资源设为35%，为此，一种可能的配置方式如下：

<p align="center">
    <img src="pic/2/2-7 层级队列配置方式.jpg" width="50%">
    <br/>
    <em>图2-7 层级队列配置方式</em>
</p>

&emsp;&emsp;层级队列组织方式的特点:

1. 子队列：队列可以嵌套，每个队列均可以包含子队列。用户只能将应用程序提交到最底层的队列，即叶子队列。
2. 最少容量：每个子队列均有一个“最少容量比”属性，表示可以使用父队列的容量百分比。调度器总是优先选择当前资源使用率最低的队列，并为之分配资源。
3. 最大容量：为了防止一个队列超量使用资源，可以为队列设置一个最大容量，这是一个资源使用上限，任何时刻使用的资源总量不能超过该值。

&emsp;&emsp;Hadoop最初的设计目的是支持大数据批处理作业，如日志挖掘、Web索引等作业，为此，Hadoop仅提供了一个非常简单的调度机制： FIFO， 即先来先服务，在该调度机制下，所有作业被统一提交到一个队列中，Hadoop按照提交顺序依次运行这些作业。但随着Hadoop的普及，单个Hadoop集群的用户量越来越大，不同用户提交的应用程序往往具有不同的服务质量要求(Quality Of Service,简称QoS)，简单的FIFO调度策略不仅不能满足多样化需求，也不能充分利用硬件资源。为了克服单队列FIFO调度器的不足，多用户多队列调度器诞生了。当前主要有两种多用户资源调度器设计思路：

1. 第一种是在一个物理集群上虚拟多个Hadoop集群，这些集群各自拥有全套独立的Hadoop服务，典型的代表是HOD (Hadoop On Demand)调度器。
2. 另一种是扩展Hadoop调度器，使之支持多个队列多用户，这种调度器允许管理员按照应用需求对用户或者应用程序分组，并为不同的分组分配不同的资源量，同时通过添加各种约束防止单个用户或者应用程序独占资源，进而能够满足各种QoS需求，典型代表是Yahoo!的Capacity Scheduler和Facebook的Fair Scheduler。

&emsp;&emsp;Capacity Scheduler：以队列为单位划分资源，每个队列可设定一定比例的资源最低保证和使用上限，同时，每个用户也可设定一定的资源使用上限以防止资源滥用，而当一个队列的资源有剩余时，可暂时将剩余资源共享给其他队列。其特点如下：
1. 容量保证：管理员可为每个队列设置资源最低保证和资源使用上限，而所有提交到该队列的应用程序共享这些资源。
2. 灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列释放资源后会归还给该队列。
3. 多重租赁：支持多用户共享集群和多应用程序同时运行，为防止单个应用程序、用户或者队列独占集群中的资源，管理员可为之增加多重约束（比如单个用户最多使用的资源量）。
4. 安全保证：管理员可通过ACL限制每个队列的访问控制列表，普通用户可为自己的应用程序指定其他哪些用户可管理它。
5. 动态更新配置文件：管理员可根据需要动态修改各种资源调度器相关配置参数而无需重启集群。

&emsp;&emsp;Capacity Scheduler允许用户在配置文件 capacity-scheduler.xml 中设置队列层级关系、队列资源占用比等信息，以层次队列组织方式为例，Capacity Scheduler的配置方式下图所示。

<p align="center">
    <img src="pic/2/2-8 Capacity Scheduler配置方式.jpg" width="50%">
    <br/>
    <em>图2-8 Capacity Scheduler配置方式</em>
</p>

&emsp;&emsp;Fair Scheduler：同Capacity Scheduler类似，以队列为单位划分资源，每个队列可设定一定比例的资源最低保证和使用上限，同时，每个用户也可设定一定的资源使用上限以防止资源滥用；当一个队列的资源有剩余时，可暂时将剩余资源共享给其他队列。Fair Scheduler与Capacity Scheduler不同之处：

1. 资源公平共享：在每个队列中，Fair Scheduler可选择按照FIFO、Fair等为应用程序分配资源，其中Fair策略是一种基于最大最小公平算法实现的资源多路复用方式；默认情况下，每个队列内部采用该方式分配资源。这意味着，如果一个队列中有两个应用程序同时运行，则每个应用程序可得到1/2的资源；如果三个应用程序同时运行，则每个应用程序可得到1/3的资源。
2. 调度策略配置灵活：Fair Scheduler允许管理员为每个队列单独设置调度策略（当前支持FIFO、Fair和DRF三种）。
3. 提高小应用程序响应时间：由于采用了最大最小公平算法，小作业可以快速获取资源并运行完成。
4. 应用程序在队列间转移：用户可动态将一个正在运行的应用从一个队列转移到另外一个队列中。

<p align="center">
    <img src="pic/2/2-9 Capacity Scheduler和Fair Scheduler对比.jpg" width="50%">
    <br/>
    <em>图2-9 Capacity Scheduler和Fair Scheduler对比</em>
</p>

&emsp;&emsp;基于节点标签的调度：是从2.6.0版本开始，Yarn引入的一种新的调度策略。该机制的主要引入动机是更好地让Yarn运行在异构集群中，进而更好地管理和调度混合类型的应用程序。基于标签的调度是一种调度策略，就像基于优先级的调度一样，是调度器中众多调度策略中的一种，可以跟其他调度策略混合使用。基本思想是：用户可为每个NodeManager打上标签，比如highmem，highdisk等， 以作为NodeManager的基本属性；同时，用户可以为调度器中的队列设置若干标签，以限制该队列只能占用包含对应标签的节点资源，这样，提交到某个队列中的作业，只能运行在特定的一些节点上。

&emsp;&emsp;一个简单的应用案例：公司A最初Hadoop集群共有20个节点，硬件资源是32GB内存，4TB磁盘；后来，随着Spark计算框架的流行，公司希望引入Spark技术，而为了更好地运行Spark程序，公司特地买了10个大内存节点，内存是64GB。为了让Spark与MapReduce等不同类型的程序“和谐”地运行在一个集群中，公司A规定:：Spark程序只运行在后来的10个大内存节点上，而之前的MapReduce程序既可以运行在之前的20个节点上，也可以运行在后来的10个大内存节点上，如下图所示:

<p align="center">
    <img src="pic/2/2-10 公司A计算资源情况.jpg" width="50%">
    <br/>
    <em>图2-10 公司A计算资源情况</em>
</p>

基于标签的调度机制：
    1） 为20个旧节点打上normal标签，为10个新节点打上highmem标签。设置系统级别label：yarn rmadmin -addToClusterNodeLabels “normal, highmem”为节点打label，比如节点node1的label为normal：
    yarn rmadmin -replacelabelsOnNode “node1-address, normal"
    2）在Capacity Scheduler中，创建两个队列，分别是hadoop和spark，其中hadoop队列可使用的标签是normal和highmem，其中normal默认为label，而spark则是highmem，并配置两个队列的capacity和max-capacity等属性。
    资源容量配置如下：
        capacity(hadoop) = 50 #hadoop队列可使用的无label资源比例为50%
        capacity(hadoop, label=normal) = 100 #hadoop队列可使用的normal标签资源比例为100%
        capacity(hadoop, label= highmem)=10 #hadoop队列可使用的highmem标签资源比例为10%
        capacity(spark) = 50 #spark队列可使用的无标签资源比例为50%
        capacity(spark, label= highmem) =90 #spark队列可使用的highmem标签资源比例为90%
    3）将Spark作业提交到spark队列中，MapReduce作业提交到hadoop队列中(需指定使用的哪种label资源，否则spark资源永远无法得到使用，默认是normal)。
        #提交Spark作业到spark队列中
        spark- submit --queue spark –class XXX...
        #提交MapReduce作业到hadoop队列中
        hadoop jar x.jar -Dmapreduce.job.queuename=hadoop…

&emsp;&emsp;Yarn的安装：

&emsp;&emsp;建议通过npm package manager来安装Yarn，安装npm后运行以下命令来安装和升级Yarn:
```
npm install --global yarn
```
&emsp;&emsp;通过运行以下命令检查 Yarn 是否已安装：
```
yarn --version
```
&emsp;&emsp;安装完成后，你可能需要对 Yarn 进行一些基本配置，以便更好地使用。
1. 配置全局缓存目录
&emsp;&emsp;如果需要更改缓存目录，可以使用以下命令：
    ```
    yarn config set cache-folder D:\Softs\yarn\caches
    ```
2. 配置网络代理
&emsp;&emsp;如果你的网络环境需要使用代理，可以使用以下命令配置代理：
    ```
    yarn config set proxy http://proxy.example.com:8080
    yarn config set https-proxy http://proxy.example.com:8080
    ```
3. 配置镜像源
&emsp;&emsp;为了提高下载速度，你可以配置国内的镜像源：
    ```
    yarn config set registry https://registry.npm.taobao.org
    ```
&emsp;&emsp;Yarn的常用命令
1. 项目初始化
    ```
    yarn init
    ```
2. 添加依赖项
    ```
    yarn add [package]
    yarn add [package]@[version]
    yarn add [package]@[tag]
    ```
3. 将依赖项添加到不同类别的依赖项
    ```
    yarn add [package] --dev
    yarn add [package] --peer
    yarn add [package] --optional
    ```
4. 升级依赖项
    ```
    yarn upgrade [package]
    yarn upgrade [package]@[version]
    yarn upgrade [package]@[tag]
    ```
5. 删除依赖项
    ```
    yarn remove [package]
    ```
6. 安装项目的所有依赖项
    ```
    yarn 
    ```
    或
    ```
    yarn install
    ```