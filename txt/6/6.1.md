## 6.1 电影评论情感倾向与用户评价关联性初探

### 6.1.1 实验背景

&emsp;&emsp;在电影行业，院线电影的票房高度依赖观众口碑。及时准确地把握观影观众对电影的情感反馈，对于电影宣发、排片策略、后续创作乃至整个行业的健康发展都至关重要。我们可以使用爬虫结合大数据技术爬取电影网站各电影的评论来分析观影观众的情感倾向，从而调整后续上映策略。

&emsp;&emsp;但同时，在观影后写下评论的用户或许是少数，但多数用户都会习惯性地在购票APP上进行打分，而此评分是一个好像更为有用、更为直观的指标，打分的高低直接决定了电影口碑的好坏，直接统计观众的均分似乎更为有效。然而，仅仅依赖数字化的平均评分，可能无法全面地捕捉到电影真实口碑。

&emsp;&emsp;首先，数字评分的“颗粒度”相对较粗。一个简单的3星、4星或5星，背后可能隐藏着截然不同的观影体验。例如，同样是给出4星评价，有的观众可能是因为整体满意但略有瑕疵，有的则可能是被某一突出亮点（如演员表现或视觉特效）所打动，但对剧情本身并不完全认可。平均分抹平了这些差异，使得我们难以洞察观众满意的具体原因和不满意的具体环节。

&emsp;&emsp;其次，评分行为本身可能受到多种因素的影响，其客观性和深度值得进一步探究。例如，部分购票平台可能存在“默认好评”的机制，或者用户在完成购票、观影流程后，出于习惯或图方便，随手给出一个大致的评分，并未经过深思熟虑。这种“便捷性”带来的评分，其所承载的真实情感反馈的“浓度”和“准确度”可能会打折扣。一个典型的例子是，我们观察到如猫眼电影这类平台的平均评分，往往普遍高于像豆瓣这样以深度影评和社区讨论为核心的平台。单纯的平均分可能存在“虚高”或未能反映真实口碑全貌的风险，而像豆瓣这样的平台，其均分在电影上映初期是不予公布的，无法为上映策略提供参考。

&emsp;&emsp;相比之下，用户花费时间和精力写下的评论文本，往往蕴含着更为丰富和真实的情感信息。评论者通常是那些有较强表达意愿的观众，他们愿意沉下心来，用文字描述自己的观影感受、具体的好恶点、甚至是引发的思考，如此的影评应该说对于反映整个电影的口碑更具参考价值。

&emsp;&emsp;那么落实到算法，我们如何来通过评论来得知电影口碑如何呢？

&emsp;&emsp;评论文本一般而言包含了整体的情感倾向（喜欢、不喜欢、一般），我们可以通过设立情感字典来识别评论文本中反映的情感偏向，例如出现一个正面情绪词则情感分加一分，出现一个负面情绪词则情感分减一，最终得到每条评论的情感分，一部电影所有评论的情感分可以算得该电影的情感均分。按照我们之前的说法，电影的情感均分应能反映电影口碑的好坏，也就是其和豆瓣平台上该电影的评分应呈现显著的正相关。

&emsp;&emsp;那么，这就是我们的第一次实验，我们需要利用大数据技术统计电影评论的情感偏向，在最后我们预期的结果是电影的情感分应该和其评分成正相关。

### 6.1.2 实验准备

&emsp;&emsp;本次实验所使用的数据集包含豆瓣电影网站 28 部电影的超过 200 万条短评论，其为`csv`格式，内容大致如图6-1所示：

<p align="center">
    <img src="/pic/6/6-1 所用数据集形式.png" width="50%">
    <br/>
    <em>图6-1 所用数据集形式</em>
</p>

&emsp;&emsp;在其中，每行为一条评论，一行中有以下属性：

1. 该条评论在数据集中的编号：`ID`，`Number`
2. 该评论对应的电影英文名：`Movie_Name_EN`
3. 该评论对应的电影中文名：`Movie_Name_CN`
4. 该评论对应的用户名：`UserName`
5. 该评论产生日期：`Date`
6. 该评论的打分：`Star`
7. 该评论的具体内容：`Comment`
8. 该评论被点赞数量：`Like`

&emsp;&emsp;在进行评论的情感偏向统计时，我们会用到三个字典，字典为`txt`文本文件，为`UTF-8`编码，每行一个词汇，如图6-2所示。

* 停用词字典：‌Stopwords（停用词）是指在自然语言处理（NLP）和信息检索中被预先过滤掉的常见高频词汇‌，如介词、连词、代词等，因其信息量低且可能干扰分析效果。例如中文的“的”、“在”、“和”或英文的“the”、“a”、“and”等。
* 正面评价词字典：包含用于统计的所有正面情感词，当评论文本中出现字典包含词语时，情感分加一。该字典来自知网Hownet情感词典。
* 负面评价词字典：包含用于统计的所有负面情感词，当评论文本中出现字典包含词语时，情感分减一。该字典来自知网Hownet情感词典。

<p align="center">
    <img src="/pic/6/6-2 字典格式.png" width="50%">
    <br/>
    <em>图6-2 字典格式</em>
</p>

&emsp;&emsp;在准备好了所有所需数据后，我们来接着思考接下来会需要哪些组件。首先，我们肯定需要一个文件系统来存储原始数据和最终产生的结果，故需要HDFS集群。接着，由于处理的是已有的两百万条数据，并且我们对耗时和实时性无需求，那么我们需要的便是一个批处理框架，在这里，我们选择使用Spark引擎，期望其能带来更快的处理速度。

&emsp;&emsp;Spark处理完成后，我们需要决定输出结果如何存储，是存储到文件中，还是存储到数据仓库中。这首先取决于我们想要哪些输出：

1. 由于我们的原始数据是格式化的`csv`，那么我们可以在处理时将原数据存储进Hive数据仓库中，便于后续可能的进一步分析。同时对于每一条数据（每一行），我们也可以存储其评论对应的情感打分，接着对应该打分的情感偏向(positive, negative, neutral)也可以一并纪录。
2. 另外，我们最终的目的是得到每个电影的情感均分，以及按电影分类的各个信息，这需要我们新创建一个表来存储。
3. 这些数据同样也可以以格式化的形式输出到`csv`文件中。
4. 额外做一些附加任务，我们也可以统计每部电影点赞数前五的正面/负面情感评论，这可以保存在`json`文件中。

&emsp;&emsp;总结一下，需要在Hive中存储上述两个表，并将按电影分类的各信息同时存储到`csv`文件中，最后做一些额外的工作，统计每部电影点赞数前五的正面/负面情感评论，保存在`json`文件。那么用到的所有组件如下：

* Spark
* Hive
* HDFS
* YARN
* ZooKeeper

### 6.1.3 正式开始实验

**（1） 准备数据**

&emsp;&emsp;将我们准备好的数据上传到HDFS集群，这里统一上传到文件夹`/exp_data/1`：

```shell
hdfs dfs -mkdir /exp_data/1
hdfs dfs -put negative_words.txt /exp_data/1
hdfs dfs -put positive_words.txt /exp_data/1
hdfs dfs -put stopwords.txt /exp_data/1
hdfs dfs -put DMSC.csv /exp_data/1
```



**（2） 理清代码思路**

&emsp;&emsp;接着我们需要规划好代码中需要哪些函数，这些函数应该完成怎样的功能。这需要从我们的流程出发，看看我们按序需要完成哪些操作。总体来说，我们的程序流程应该如下：

1. 初始化与环境配置：加载必要的外部资源，如情感词典和停用词词典。

2. 数据加载与预处理：从HDFS读取原始的电影评论CSV数据到RDD中。对原始数据进行初步清洗和转换，例如选择需要的列、统一列名、转换数据类型，并处理可能存在的空值或无效数据。

3. 核心情感分析：
    * 文本预处理：对评论文本进行标准化处理，包括去除无关字符（如标点符号、非中文字符）、中文分词（将连续的文本切分成词语序列）、以及去除停用词。
    * 情感打分：基于预处理后的词语列表和加载的情感词典（正面词典、负面词典），为每条评论计算一个量化的情感得分。并据计算出的情感得分，将每条评论归类为“正面”、“负面”或“中性”等情感标签。

4. 结果聚合与分析：按电影ID分组，统计每部电影的正面评论数、负面评论数、中性评论数、总评论数、平均情感得分、平均原始星级评分以及总点赞数等摘要信息。
   
5. 数据持久化与输出：将详细的每条评论的情感分析结果（包括原始评论、情感得分、情感标签等）存入一个Hive分区表中，便于后续的细粒度查询和历史追溯。将聚合得到的电影情感摘要信息存入另一个Hive表中，供快速查询和报表分析使用。

6. 写入HDFS文本文件：将电影情感摘要信息以CSV格式输出到HDFS；将每部电影的Top N条热门（例如按点赞数排序）正面和负面评论以JSON格式输出到HDFS。

&emsp;&emsp;综合该流程，我们规划出以下几个主要函数：

1. `load_words_from_hdfs(spark, path)`，从指定的路径加载文本文件（如停用词典、情感词典），函数内容为去除每行首尾空白，过滤空行，并将所有词语存入一个Python set集合中（利用set自动去重）。最后返回该集合。
2. `preprocess_text_udf_py(text)`，对单条评论文本做预处理，例如去除非中文字符，分词，去除停用词等。返回一个包含预处理后词语的Python列表。
3. `calculate_sentiment_score_udf_py(words)`，根据预处理后的词语列表和情感词典，计算该评论的情感得分。返回一个整数，代表该评论的情感总分。
4. `classify_sentiment_udf_py(score)`根据情感得分，返回相对的情感标签。

**（3） 创建Hive表**

&emsp;&emsp;由于我们需要将数据存储到Hive中，我们需要预先创建好表。创建`.sql`文件，写入一下内容，并通过`beeline -u jdbc:hive2://your_ip:10000 -n root -f xxx.sql`来运行。这里我们创建了`review_sentiments_detail`和`movie_sentiment_summary`两个表。

```sql
CREATE DATABASE IF NOT EXISTS douban_analytics;
USE douban_analytics;

-- 1. 存储详细的评论情感分析结果
CREATE TABLE IF NOT EXISTS review_sentiments_detail (
  review_id STRING,                -- 评论的唯一ID (来自CSV的ID列)
  movie_id STRING,                 -- 电影ID (使用 Movie_Name_CN)
  movie_name_en STRING,            -- 电影英文名
  username STRING,                 -- 用户名
  review_date DATE,                -- 评论日期 (来自CSV的Date列)
  original_star INT,               -- 原始评分 (来自CSV的Star列)
  likes_count INT,                 -- 点赞数 (来自CSV的Like列)
  review_text STRING,              -- 原始评论文本
  sentiment_score INT,             -- 计算得到的情感得分
  sentiment_label STRING,          -- 情感标签 (positive, negative, neutral)
  processing_timestamp TIMESTAMP   -- Spark处理该记录的时间戳
)
PARTITIONED BY (dt STRING)         -- 按日期分区 (例如 'YYYY-MM-DD')
STORED AS PARQUET;                 -- 推荐使用Parquet或ORC等列式存储

-- 2. 存储电影的聚合情感摘要信息
CREATE TABLE IF NOT EXISTS movie_sentiment_summary (
  movie_id STRING,                 -- 电影ID (使用 Movie_Name_CN)
  movie_name_en STRING,            -- 电影英文名
  positive_reviews BIGINT,         -- 该电影的正面评论数
  negative_reviews BIGINT,         -- 该电影的负面评论数
  neutral_reviews BIGINT,          -- 该电影的中性评论数
  total_reviews BIGINT,            -- 该电影的总评论数
  avg_sentiment_score_calculated DOUBLE, -- 该电影计算出的平均情感得分
  avg_original_star_rating DOUBLE, -- 该电影原始平均星级评分
  total_likes BIGINT,              -- 该电影所有评论的总点赞数
  last_updated TIMESTAMP           -- 此摘要信息的最后更新时间
)
STORED AS PARQUET;
```


**（4） 编写代码**

&emsp;&emsp;在明确了实验流程和主要函数功能后，我们现在开始逐步编写Spark应用程序的Python代码。我们将按照之前规划的顺序，结合Spark的API来实现每个步骤。

&emsp;&emsp;首先，我们需要导入所有将要用到的Python库和Spark相关的模块，并定义一些全局的配置变量，如文件路径、表名等，以方便后续的调用和修改。

```python
import jieba  # 中文分词库
from pyspark.sql import SparkSession # Spark SQL核心，用于创建DataFrame和执行SQL
from pyspark.sql.functions import udf, col, count, current_timestamp, lit, sum, avg, when, monotonically_increasing_id, regexp_replace, trim, length, row_number, desc, concat # Spark SQL内置函数
from pyspark.sql.types import StringType, ArrayType, IntegerType, DateType, LongType # Spark SQL数据类型
from pyspark.sql.window import Window # Spark SQL窗口函数
import re     # Python正则表达式库
import datetime # Python日期时间库

# --- 全局配置信息 ---
# 应用程序名称，将显示在YARN UI或Spark UI上
APP_NAME = "DoubanMovieSentimentAnalysis" 

# HDFS输入数据路径配置
HDFS_REVIEW_DATA_PATH = "hdfs:///exp_data/1/DMSC.csv"         # 原始评论CSV文件路径
HDFS_STOPWORDS_PATH = "hdfs:///exp_data/1/stopwords.txt"       # 停用词词典路径
HDFS_POSITIVE_WORDS_PATH = "hdfs:///exp_data/1/positive_words.txt" # 正面情感词词典路径
HDFS_NEGATIVE_WORDS_PATH = "hdfs:///exp_data/1/negative_words.txt" # 负面情感词词典路径

# Hive数据库及表名配置
HIVE_DB_NAME = "douban_analytics"                                   # Hive数据库名称
HIVE_DETAIL_TABLE = f"{HIVE_DB_NAME}.review_sentiments_detail"      # 存储详细情感结果的Hive表
HIVE_SUMMARY_TABLE = f"{HIVE_DB_NAME}.movie_sentiment_summary"     # 存储电影情感摘要的Hive表


# HDFS文本文件输出路径配置
HDFS_OUTPUT_BASE_PATH = "hdfs:///exp_out/1" # 输出文件的HDFS基础路径
HDFS_MOVIE_PROFILE_PATH = f"{HDFS_OUTPUT_BASE_PATH}/movie_profile_sentiment" # 电影画像CSV输出路径
HDFS_TOP_COMMENTS_PATH = f"{HDFS_OUTPUT_BASE_PATH}/movie_top_comments"     # Top N评论JSON输出路径
```

&emsp;&emsp;接着编写读取数据的`load_words_from_hdfs(spark, path)`函数，该函数有两个参数，第一个参数为一个SparkSession实例，在`main()`中创建，其是所有Spark程序的起点（在3.5中已提及过），第二个参数是路径。函数中的操作如下：

1. `spark.sparkContext.textFile(path)`: 使用SparkContext读取HDFS上的文本文件，每一行文本成为RDD中的一个元素。
2. `map(lambda x: x.strip())`: 对RDD中的每一行（字符串x），调用strip()方法去除其两端的空格和换行符。
3. `filter(lambda x: x)`: 过滤掉经过strip()后变成空字符串的行。
4. `collect()`: 这是一个RDD的行动操作，它将RDD中所有处理后的词语收集到Driver程序的内存中，形成一个标准的Python列表。
5. `words = set(words_list)`: 将收集到的词语列表转换为Python的set。set数据结构会自动去除列表中的重复元素。

```python
def load_words_from_hdfs(spark, path):
    try:
        words_list = spark.sparkContext.textFile(path).map(lambda x: x.strip()).filter(lambda x: x).collect()
        words = set(words_list)
        print(f"成功从 {path} 加载 {len(words)} 个词")
        return words
    except Exception as e:
        print(f"从HDFS加载词典 {path} 失败: {e}")
        return set()
```

&emsp;&emsp;然后是清洗评论的函数，其接受评论，返回分词的结果。该函数流程较为简单，如果评论为空则返回空集，不为空则进入清洗流程。首先使用正则过滤掉所有非中文字符，接着调用`jieba`库分词，然后使用停用词字典过滤分词结果，最后返回得到的列表即可。

```python
def preprocess_text_udf_py(text):
    if text is None:
        return []
    text = re.sub(r"[^\u4e00-\u9fa5]", "", str(text))
    seg_list = jieba.lcut(text, cut_all=False)
    processed_words = [word for word in seg_list if word.strip() and word not in stopwords_bd.value]
    return processed_words
```

&emsp;&emsp;在清洗完成后，就需要根据传入的单词列表给该评论打分。

```python
def calculate_sentiment_score_udf_py(words):
    if not words:
        return 0
    score = 0
    for word in words:
        if word in positive_words_bd.value:
            score += 1
        elif word in negative_words_bd.value:
            score -= 1          
    return score
```

&emsp;&emsp;根据情感得分返回对应标签。

```python
def classify_sentiment_udf_py(score):
    if  score == 0: return "neutral"
    elif score > 0: return "positive"
    elif score < 0: return "negative"
```

&emsp;&emsp;辅助函数到此编写完成，接下来我们来完成主函数的编写。在此之前，我们先预定义广播变量。


>广播变量作用：有一个相对较小但所有任务都需要访问的只读数据集（例如此时的停用词词典、情感词典），如果不用广播变量，这个数据集可能会被每个Task都尝试从一个共享位置独立读取，这会导致大量的网络I/O或重复的数据加载，效率低下。注册成为广播变量后，每个节点会将这份数据缓存在其内存中。当该节点上运行的任何Task需要访问这个数据时，它们可以直接从本地内存中快速读取。

```python
stopwords_bd = None
positive_words_bd = None
negative_words_bd = None
```

&emsp;&emsp;在主函数中，我们首先生成SparkSession。

```python
    spark = SparkSession.builder \
        .appName(APP_NAME) \
        .enableHiveSupport() \
        .getOrCreate()
    sc = spark.sparkContext
```
&emsp;&emsp;然后加载字典数据，并使用方法`broadcast()`将字典注册成为广播变量。

```python
    # 1. 加载并广播词典
    stopwords = load_words_from_hdfs(spark, HDFS_STOPWORDS_PATH)
    positive_words = load_words_from_hdfs(spark, HDFS_POSITIVE_WORDS_PATH)
    negative_words = load_words_from_hdfs(spark, HDFS_NEGATIVE_WORDS_PATH)

    stopwords_bd = sc.broadcast(stopwords)
    positive_words_bd = sc.broadcast(positive_words)
    negative_words_bd = sc.broadcast(negative_words)
```

&emsp;&emsp;第二步，我们需要注册UDFs，`udf()`方法的第二个参数为所绑定函数的返回值类型。

>UDF允许我们将自定义的函数注册到Spark SQL中，使其能够像内置函数一样被调用，作用于DataFrame的列数据。这样，就能使用我们的自定义逻辑方便地处理每一行或每一列的数据。

```python
    # 2. 注册UDFs
    udf_preprocess = udf(preprocess_text_udf_py, ArrayType(StringType()))
    udf_calculate_score = udf(calculate_sentiment_score_udf_py, IntegerType())
    udf_classify = udf(classify_sentiment_udf_py, StringType())
```

&emsp;&emsp;第三步，加载我们的影评数据集csv，重命名列名，并做一些简单的清洗，如去除空行等。

```python
# 3. 从HDFS加载原始评论数据
    try:
        df_raw = spark.read.csv(HDFS_REVIEW_DATA_PATH, header=True, inferSchema=False, encoding="UTF-8", multiLine=True, escape='"')
        df_reviews = df_raw.select(
            col("ID").alias("review_id").cast(StringType()),
            col("Movie_Name_CN").alias("movie_id").cast(StringType()),
            col("Movie_Name_EN").alias("movie_name_en").cast(StringType()),
            col("Username").alias("username").cast(StringType()),
            col("Date").alias("review_date").cast(DateType()),
            col("Star").alias("original_star").cast(IntegerType()),
            col("Like").alias("likes_count").cast(IntegerType()),
            when(col("Comment").isNull() | (trim(col("Comment")) == ""), "").otherwise(col("Comment")).alias("review_text")
        ).filter(col("movie_id").isNotNull() & (trim(col("movie_id")) != "") & \
                 col("review_id").isNotNull() & (trim(col("review_id")) != "") & \
                 (length(trim(col("review_text"))) > 0) )
    except Exception as e:
        print(f"加载或处理原始评论数据失败: {e}")
        spark.stop()
        exit(1)
```

&emsp;&emsp;第四步，对数据做情感分析。`withColumn()`方法会给原有数据新增一列。

&emsp;&emsp;我们来看看这段代码的逻辑，我们首先新增了`processed_words`列，该新列的值是通过对原有的`review_text`列的每一行应用我们之前注册的`udf_preprocess`得到的。回顾一下，`udf_preprocess()`函数会进行去除非中文字符、Jieba分词、去除停用词等操作，并返回一个词语列表。因此，`processed_words`列的每个单元格将包含对应原始评论文本处理后的词语数组。

&emsp;&emsp;而后再新增了一列`sentiment_score`，对上一步新增的`processed_words`列应用`udf_calculate_score()`，该新列的值就是对应评论的情感分数。

&emsp;&emsp;然后再新增一列`sentiment_label`，对上一步新增的`sentiment_score`列应用`udf_classify()`，该新列的值就是对应评论的情感标签。

&emsp;&emsp;最后新增`sentiment_label`列，代表处理时间。

```python
# 4. 进行情感分析
    df_processed = df_reviews.withColumn("processed_words", udf_preprocess(col("review_text")))
    df_sentiment = df_processed.withColumn("sentiment_score", udf_calculate_score(col("processed_words")))
    df_classified = df_sentiment.withColumn("sentiment_label", udf_classify(col("sentiment_score"))) \
                                .withColumn("processing_timestamp", current_timestamp())
```

&emsp;&emsp;情感分析完成后，我们将把不同粒度的分析结果持久化到预先在Hive中创建好的表中，以便于后续更深入的分析。在5a中，我们将上一步情感分析完得到的表直接写入Hive中，而5b的过程则要复杂些，其流程大致如下：

1. `groupBy("movie_id", "movie_name_en")`: 将表按照电影ID (movie_id) 和电影英文名 (movie_name_en) 进行分组。有多少个分组该表中就有多少行。
2. `agg(...)`: 对每个电影分组执行一系列聚合操作，计算出该电影的正面评论数 (positive_reviews)、负面评论数 (negative_reviews)、中性评论数 (neutral_reviews)、总评论数 (total_reviews)、平均情感得分 (avg_sentiment_score_calculated)、平均原始星级 (avg_original_star_rating) 和总点赞数 (total_likes)，并写入对应的列中。
3. `saveAsTable(HIVE_SUMMARY_TABLE)`：将数据保存为一个新的Hive管理的表。

```python
# 5. Hive 集成 
    current_date_partition_str = datetime.datetime.now().strftime("%Y-%m-%d")
    spark.conf.set("hive.exec.dynamic.partition", "true")
    spark.conf.set("hive.exec.dynamic.partition.mode", "nonstrict")

    # 5a. 将详细情感分析结果写入Hive
    df_to_hive_detailed = df_classified.withColumn("dt", lit(current_date_partition_str)) \
        .select("review_id", "movie_id", "movie_name_en", "username", "review_date",
                "original_star", "likes_count", "review_text", "sentiment_score",
                "sentiment_label", "processing_timestamp", "dt")
    print(f"正在将详细情感分析结果写入Hive表: {HIVE_DETAIL_TABLE}，分区 dt={current_date_partition_str}")
    try:
        df_to_hive_detailed.write.mode("append").insertInto(HIVE_DETAIL_TABLE)
        print("成功将详细情感分析结果写入Hive。")
    except Exception as e:
        print(f"写入详细情感分析结果到Hive失败: {e}")

    # 5b. 计算并写入电影情感摘要信息到Hive
    df_movie_summary_calc = df_classified.groupBy("movie_id", "movie_name_en") \
        .agg(
            sum(when(col("sentiment_label") == "positive", 1).otherwise(0)).alias("positive_reviews"),
            sum(when(col("sentiment_label") == "negative", 1).otherwise(0)).alias("negative_reviews"),
            sum(when(col("sentiment_label") == "neutral", 1).otherwise(0)).alias("neutral_reviews"),
            count("*").alias("total_reviews"),
            avg("sentiment_score").alias("avg_sentiment_score_calculated"),
            avg("original_star").alias("avg_original_star_rating"),
            sum("likes_count").alias("total_likes")
        ).withColumn("last_updated", current_timestamp())
    print(f"正在将电影情感摘要写入Hive表: {HIVE_SUMMARY_TABLE}")
    try:
        df_movie_summary_calc.write.mode("overwrite").saveAsTable(HIVE_SUMMARY_TABLE)
        print("成功将电影情感摘要写入Hive。")
    except Exception as e:
        print(f"写入电影情感摘要到Hive失败: {e}")
```

&emsp;&emsp;写完Hive表后，我们就到了最后一步，首先我们应该把在5b中的表写入到本地，保存为csv，首先选中要保存的列，然后调整一下输出编号，最后写入csv即可。

```python
    # 6. 写入HDFS文本文件
    # 6a. 将电影画像情感信息写入HDFS (例如CSV格式)
    # df_movie_summary_calc 是之前为Hive摘要表准备的DataFrame
    df_profile_to_hdfs = df_movie_summary_calc.select(
        "movie_id", "movie_name_en", "positive_reviews", "negative_reviews", "neutral_reviews",
        "total_reviews", "avg_sentiment_score_calculated", "avg_original_star_rating",
        "total_likes")
    try:
        df_profile_to_hdfs.coalesce(1) \
            .write.mode("overwrite").option("header", "true").option("encoding", "UTF-8").csv(HDFS_MOVIE_PROFILE_PATH)
        print(f"成功将电影画像信息写入HDFS路径: {HDFS_MOVIE_PROFILE_PATH}")
    except Exception as e:
        print(f"写入电影画像信息到HDFS失败: {e}")
```

&emsp;&emsp;写json文件，流程如下：
1. `Window.partitionBy("movie_id", "sentiment_label")`: 定义窗口规范，指定了分区的依据。这意味着后续的窗口函数将在具有相同movie_id（电影ID）和相同sentiment_label（情感标签）的行组内独立进行计算。例如，所有“电影A”的正面评论会形成一个分区，所有“电影A”的负面评论会形成另一个分区，以此类推。
2. `orderBy(desc("likes_count"), desc("review_date"))`: 指定了在每个分区内部的排序规则。首先按照likes_count（点赞数）进行降序（desc）排序；如果点赞数相同，则进一步按照review_date（评论日期）进行降序排序。
3. `withColumn("rank", row_number().over(window_spec_likes))`: 添加一个名为"rank"的新列，该列的值
来自根据分区规范`over(window_spec_likes)`分区后各行在该分区中对应的行数，也即该行在该分区内的排名`row_number()`。
4. `filter(col("rank") <= N_TOP_COMMENTS)`: 只保留`rank`列值小于3的行，也既筛选出每个分区内排名在前N_TOP_COMMENTS（即前3名）的评论。
5. `filter(col("sentiment_label").isin("positive", "negative"))`: 进一步筛选，只保留那些情感标签是 "positive" 或 "negative" 的行。
6. 接着选中要写入的列，规定以json格式写入对应路径即可。

```python
# 6b. 将电影的Top N正面/负面评论写入HDFS (例如JSON Lines格式)
    N_TOP_COMMENTS = 3 # 每部电影每种情感类型取前N条评论
    window_spec_likes = Window.partitionBy("movie_id", "sentiment_label").orderBy(desc("likes_count"), desc("review_date"))

    df_top_comments = df_classified.withColumn("rank", row_number().over(window_spec_likes)) \
        .filter(col("rank") <= N_TOP_COMMENTS) \
        .filter(col("sentiment_label").isin("positive", "negative"))

    # 准备写入HDFS的DataFrame
    df_top_comments_to_hdfs = df_top_comments.select(
        "movie_id",
        "sentiment_label",
        "review_id",
        "review_text",
        "likes_count",
        "original_star",
        "sentiment_score",
    )
    print(f"正在将电影Top N评论写入HDFS路径: {HDFS_TOP_COMMENTS_PATH} (JSON Lines格式)")
    try:
        df_top_comments_to_hdfs.write.mode("overwrite").json(HDFS_TOP_COMMENTS_PATH) # JSON Lines格式，每个JSON对象一行
        print(f"成功将电影Top N评论写入HDFS路径: {HDFS_TOP_COMMENTS_PATH}")
    except Exception as e:
        print(f"写入电影Top N评论到HDFS失败: {e}")
```

### 6.1.4 执行程序并分析结果


&emsp;&emsp;使用命令`spark-submit --master yarn --deploy-mode cluster exp1.py`执行Spark程序，在Spark WebUI中我们可以看见该作业的执行，如图6-3。

<p align="center">
    <img src="/pic/6/6-3 实验1WebUI界面.png" width="50%">
    <br/>
    <em>图6-3 实验1WebUI界面</em>
</p>

&emsp;&emsp;等待作业完成，我们可以在对应目录中看见输出的csv和json文件，如图6-4、图6-5所示，可以看到csv文件中有每个电影的正/负/中面评论数和平均情感分等，json文件中则是统计了各个电影点赞数最多的评论。写入Hive表中的数据可用Spark-Sql进入命令行查看。

<p align="center">
    <img src="/pic/6/6-4 csv文件输出.png" width="50%">
    <br/>
    <em>图6-4 csv文件输出</em>
</p>

<p align="center">
    <img src="/pic/6/6-4 json文件输出.png" width="50%">
    <br/>
    <em>图6-5 json文件输出</em>
</p>

&emsp;&emsp;既然我们已经得到了结果，那么在最后我们应该验证一下我们在最开头提出的假设，评论的情感得分应该和该电影的评分成正相关关系。

&emsp;&emsp;为了验证假设，我们提取出每个电影的平均情感分和打分的均分，以x为情感分，y为均分生成散点图。在这里我们采用Python语言和`matplotlib`库来实现，具体代码存放在本书github仓库中。

>Matplotlib 是一个 Python 的 2D绘图库，它以各种硬拷贝格式和跨平台的交互式环境生成出版质量级别的图形。


```shell
root@m1:~# python3 linear.py 
线性回归方程: y = 3.1272x + 2.0852
决定系数 (R-squared): 0.6761
图像已保存为: linear_regression_sentiment_vs_rating.png
```

&emsp;&emsp;结果如图6-6所示，可以看到数据点大致沿着一条从左下向右上倾斜的趋势线分布，直观地表明“计算出的平均情感得分”与“平均原始打分”之间可能存在正相关关系。为了更精确地量化这种关系，我们进行了线性回归分析。拟合得到的线性回归方程为 y = 3.13x + 2.09，该线性回归方程的决定系数为0.68。

<p align="center">
    <img src="/pic/6/6-4 json文件输出.png" width="50%">
    <br/>
    <em>图6-5 json文件输出</em>
</p>

>决定系数R²的取值范围在0到1之间，越接近1，表明回归直线对观测数据的拟合程度越好，自变量对因变量的解释能力越强；反之，越接近0，则表示自变量对因变量的解释能力越弱。

&emsp;&emsp;在社会科学和涉及人类行为（如观影评价）的研究领域，由于影响因素的复杂性和多样性，R²值通常难以达到极高的水平（如0.9以上）。因此，0.68的R²值可以被认为是一个中等偏上的解释力度，表明我们通过分析评论文本提取的情感倾向，与用户最终给出的数字评分之间，确实存在一个显著且具有一定强度的线性正相关关系，这验证了我们最初的猜想。

&emsp;&emsp;然而，也需要注意到，仍有一些点于线性回归方程相距过远。这提示我们，电影的最终评分是一个多因素综合作用的结果，除了评论文本中直接表达的情感外，还可能受到诸多因素的影响。

&emsp;&emsp;例如情感分析模型存在局限性，采用的基于词典法的情感分析模型相对简单，可能未能完全准确捕捉到所有评论的细微情感、反讽、以及复杂语境下的真实意图。例如，大量被归为“中性”的评论可能实际上蕴含着未被识别的情感。

&emsp;&emsp;到此，本次的实验就结束了，在后续，读者可以通过例如机器学习的手段，加强对评论情感倾向的判别，或是给不同的词语加上不同的情感分权重来进行细化。

