## 6.1 网站日志信息处理案例

### 6.1.1 实验背景

&emsp;&emsp;设想你有一个网站，用户在这个网站里面可以有很多行为，比如注册，登录，查看，点击，双击，购买东西，加入购物车，添加记录，修改记录，删除记录，评论，登出等一系列我们熟悉的操作。这些操作都被记录在日志信息里面。我们想分析出用户在什么时候喜欢购买东西，什么时候喜欢加入购物车，从而，在相应的时间采取行动，激励用户购买东西，推荐商品给用户，毕竟网站盈利才是我们希望达到的目的，对吧

&emsp;&emsp;如果我们想要进行上述的一个分析，那我们首先做的就是将日志信息进行清洗，挑出用户购买东西和加入购物车这两个行为的日志信息，然后将用户购买东西的时间点和加入购物车的时间点保存下来，以便我们后续的统计和推荐等一系列的操作。

&emsp;&emsp;在本章，我们就是要对网站中这样的日志信息进行清洗和保存。

### 6.1.2 实验准备

&emsp;&emsp;本次实验所使用的数据需要采集网站中用户行为，但是我们没有网站就不能实验了吗？当然不是，车到山前必有路，既然没有真实的网站，那我们就模拟一个网站，不断的随机产生用户行为，再交给我们熟悉的Flume、Kafka和Storm处理，最后再利用Excel生成报告，在这个过程中，我们依旧使用Java操作相关的API。

&emsp;&emsp;我们产生的日志格式规定如下，分别包含了ip地址、归属地、产生时间、产生时间对应的时间戳、用户id、网站地址和用户的行为。

&emsp;&emsp;`String log = ip + "\t" + address + "\t" + d + "\t" + timestamp + "\t" + userid + "\t" + Common.WEB_SITE + "\t" + action;`

&emsp;&emsp;其中用户的行为我们抽象以下几个：

&emsp;&emsp;`"Register", "Login", "View", "Click", "Double_Click", "Buy", "Shopping_Car", "Add", "Edit", "Delete", "Comment", "Logout" `

&emsp;&emsp;举一个例子，下面产生的数据代表这样的含义：ip地址为115.19.62.102，来自海南，产生时间为2025-12-20，产生时间对应的时间戳为1749139200000，用户的id为1735787074662918890，网站地址为www.xxx.com，用户的行为是Edit。

&emsp;&emsp;`115.19.62.102    海南    2025-06-06    1749139200000    1735787074662918890    www.xxx.com    Edit`

&emsp;&emsp;整个实验架构如下图所示：

<p align="center">
    <img src="/pic/6/6-6 实验架构图.png" width="50%">
    <br/>
    <em>图6-6 实验架构图</em>
</p>

### 6.1.3 正式开始实验

**（1） 配置Flume**

_在m1主机修改Flume配置_

&emsp;&emsp;修改Flume数据源类型为与topic名称，将`/root/apache-flume-1.11.0-bin/conf/flume.conf`中内容改为：

```
# avro.conf 配置⽂件
agent1.sources = r1
agent1.sinks = k1
agent1.channels = c1

# 配置 Source 监听端⼝为 4141 的 avro 服务
agent1.sources.r1.type = avro
agent1.sources.r1.bind = 0.0.0.0
agent1.sources.r1.port = 4141
agent1.sources.r1.channels = c1

# 配置 Sink 
agent1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
agent1.sinks.k1.topic = all_my_log
agent1.sinks.k1.brokerList = m1:9092,m2:9092,m3:9092
agent1.sinks.k1.requiredAcks = 1
agent1.sinks.k1.batchSize = 20
agent1.sinks.k1.channel = c1

# 配置 Sink 
# agent1.sinks.k1.type = logger 
# agent1.sinks.k1.channel = c1  

# 配置 Channel
# Use a channel which buffers events in memory
agent1.channels.c1.type = memory
agent1.channels.c1.capacity = 1000 
agent1.channels.c1.transactionCapacity = 100
```

**（2） 启动ZooKeeper**

_在三台主机执行如下操作_

&emsp;&emsp;启动ZooKeeper。

&emsp;&emsp;`zkServer.sh start`

**（3） 启动Kafka**

_在三台主机执行如下操作_

&emsp;&emsp;启动Kafka。

```
cd ${KAFKA_HOME}
bin/kafka-server-start.sh -daemon config/server.properties
```

**（4） 启动Flume**

_在m1主机执行如下操作_

&emsp;&emsp;启动Flume。

```
cd ${FLUME_HOME}
flume-ng agent --conf conf --conf-file ${FLUME_HOME}/conf/flume.conf --name agent1 -Dflume.root.logger=INFO,console
```

**（5） 产生日志信息并写入到Flume**

&emsp;&emsp;接下来，我们将用java程序模拟产生用户行为，并将这些行为的日志信息传给Flume来模拟Flume采集数据，然后再将日志信息传入Kafka，保存到all_my_log这个topic中。

&emsp;&emsp;相关代码如下：

```java 
// 创建一个 RPC 客户端，连接到本地 Flume Agent 的 4141 端口
RpcClient client = RpcClientFactory.getDefaultInstance("localhost", 4141);

// 创建随机数生成器
Random random = new Random();

// 用户可能的行为列表
String[] action_list = { "Register", "Login", "View", "Click", "Double_Click", "Buy", "Shopping_Car", "Add", "Edit", "Delete", "Comment", "Logout" };

// IP 地址生成的上限
int maxIpNumber=224;

// 地址（省份、直辖市、自治区）列表
String[] address_list = { "北京", "天津", "上海", "广东", "重庆", "河北", "山东", "河南", "云南", "山西", "甘肃", "安徽", "福建", "黑龙江", "海南", "四川", "贵州", "宁夏", "新疆", "湖北", "湖南", "山西", "辽宁", "吉林", "江苏", "浙江", "青海", "江西", "西藏", "内蒙", "广西", "香港", "澳门", "台湾", };

// 模拟网站地址
String web_site="www.xxx.com";

try {
    while (true) {
        // 获取当前系统时间并格式化为字符串
        Date date = new Date();
        SimpleDateFormat simpleDateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
        String d = simpleDateFormat.format(date);

        // 获取当前时间戳（毫秒）
        Long timestamp = new Date().getTime();

        // 生成随机 IP 地址
        String ip = random.nextInt(maxIpNumber) + "." + random.nextInt(maxIpNumber) + "." + random.nextInt(maxIpNumber) + "." + random.nextInt(maxIpNumber);

        // 随机选择一个地址（地理位置）
        String address = address_list[random.nextInt(address_list.length)];

        // 生成一个正整数的用户 ID
        Long userid = Math.abs(random.nextLong());

        // 随机选择一个用户行为
        String action = action_list[random.nextInt(action_list.length)];

        // 构造日志数据：IP 地址、地址、时间、时间戳、用户 ID、网站、行为（用制表符分隔）
        // 示例：199.80.45.117 云南 2018-12-20 1545285957720 3086250439781555145 www.xxx.com Buy
        String data = ip + "\t" + address + "\t" + d + "\t" + timestamp + "\t" + userid + "\t" + web_site + "\t" + action;

        // 创建一个 Flume 事件，设置日志内容和编码
        Event event = EventBuilder.withBody(data, Charset.forName("UTF-8"));

        // 发送事件到 Flume
        client.append(event);

        // 每隔 1 秒发送一次数据
        Thread.sleep(1000);
    }
} catch (EventDeliveryException e) {
    // 处理 Flume 发送失败的异常
    System.err.println("Failed to send event to Flume: " + e.getMessage());
} catch (InterruptedException e) {
    // 处理线程中断异常
    System.err.println("Thread was interrupted: " + e.getMessage());
} finally {
    // 最终关闭 RPC 客户端连接
    client.close();
}
```

&emsp;&emsp;由于我们需要用到Flume的API，所以我们需要添加如下依赖，同时需要用打包插件将其打包到jar包中

```
    <dependencies>
        <dependency>
            <groupId>org.apache.flume</groupId>
            <artifactId>flume-ng-sdk</artifactId>
            <version>1.11.0</version>
        </dependency>
    </dependencies>
```

_在m2主机执行如下操作_

&emsp;&emsp;创建Topic。
```
cd ${KAFKA_HOME}
bin/kafka-topics.sh --create --bootstrap-server m1:9092,m2:9092,m3:9092 --replication-factor 2 --topic all_my_log --partitions 1
```

&emsp;&emsp;为了看到java程序产生的模拟数据，可以在m2主机执行如下指令启动Kafka消费者，以此来查看我们是否成功产生数据并写入到Flume中。

```
cd ${KAFKA_HOME}
bin/kafka-console-consumer.sh --bootstrap-server m1:9092,m2:9092,m3:9092 --topic all_my_log --from-beginning
```

_在m1主机执行如下操作_

&emsp;&emsp;新开一个终端：

&emsp;&emsp;将打包后的flume-1.0.jar放入`/root`下，并在`/root`目录中执行如下指令运行java程序：

&emsp;&emsp;`java -jar flume-1.0.jar`


&emsp;&emsp;可以看到java程序在源源不断的生成模拟数据。

```
205.202.216.132 西藏    2025-06-06 11:52:13     1749181933508   1575118096067576913     www.xxx.com     Add
215.71.29.198   内蒙    2025-06-06 11:52:14     1749181934648   8085481671476708579     www.xxx.com     Delete
27.179.57.100   安徽    2025-06-06 11:52:15     1749181935651   8761868746779775237     www.xxx.com     Comment
163.204.25.82   重庆    2025-06-06 11:52:16     1749181936656   5296735721083876963     www.xxx.com     Click
119.35.210.201  香港    2025-06-06 11:52:17     1749181937659   8407441108851234857     www.xxx.com     Edit
182.216.168.31  山东    2025-06-06 11:52:18     1749181938662   957585683426560284      www.xxx.com     Shopping_Car
123.161.195.144 河北    2025-06-06 11:52:19     1749181939665   784399186589498682      www.xxx.com     Register
58.99.184.152   安徽    2025-06-06 11:52:20     1749181940668   1449311844059526711     www.xxx.com     Delete
```

**（6） 清洗数据**


&emsp;&emsp;我们已经有了用户执行各种行为的日志信息，比如Add、Delete、Click、Buy和Shopping_Car等，但是我们关心只有购买和加购物车，也就是Buy和Shopping_Car两个操作，所以，我们只要保留这两种行为的日志信息，也就是说，接下来我们要把all_my_log这个topic中复杂的日志信息给过滤，去除我们不关心的日志信息，并把过滤后的日志信息保存到filtered_log这个topic中去

_在m1主机执行如下操作_

&emsp;&emsp;新开一个终端：

&emsp;&emsp;开启nimbus。

`storm nimbus &`

_在m2主机执行如下操作_

&emsp;&emsp;开启supervisor。

`storm supervisor &`

&emsp;&emsp;创建Topic。
```
cd ${KAFKA_HOME}
bin/kafka-topics.sh --create --bootstrap-server m1:9092,m2:9092,m3:9092 --replication-factor 2 --topic filtered_log --partitions 1
```

_在m1主机执行如下操作_

&emsp;&emsp;接下来我们要写Java程序，相关代码如下：

```java
// Kafka Broker 列表（m1、m2、m3 是 Kafka 集群中的主机名）
String bootstrapServers = "m1:9092,m2:9092,m3:9092";

// 创建 Storm 拓扑构建器
TopologyBuilder builder = new TopologyBuilder();

// 配置 KafkaSpout，用于从 Kafka 的 "all_my_log" 主题中读取消息
KafkaSpoutConfig<String, String> kafkaSpoutConfig = KafkaSpoutConfig.builder(bootstrapServers, "all_my_log")
        .setProp("group.id", "kafka-group")// 设置消费者组 ID
        .build();

// 将 KafkaSpout 添加到拓扑中，名字为 kafkaSpout
builder.setSpout("kafkaSpout", new KafkaSpout<>(kafkaSpoutConfig));

// 添加自定义的 FilterBolt（过滤 Bolt），从 kafkaSpout 接收数据
builder.setBolt("filterBolt", new FilterBolt()).shuffleGrouping("kafkaSpout");

// KafkaBolt 用于将处理后的数据写回 Kafka（写入 filtered_log 主题）
Properties props = new Properties();
props.put("bootstrap.servers", "m1:9092,m2:9092,m3:9092");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

// 配置 KafkaBolt：指定目标主题，字段映射方式，Kafka 生产者参数
KafkaBolt<String, String> kafkaBolt = new KafkaBolt<String, String>()
        .withTopicSelector(new DefaultTopicSelector("filtered_log"))
        .withTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper<>())
        .withProducerProperties(props);

// 将 kafkaBolt 添加到拓扑，接收来自 filterBolt 的数据
builder.setBolt("kafkaBolt", kafkaBolt).shuffleGrouping("filterBolt");

// 配置 Storm 参数
Config config = new Config();
config.setDebug(true);// 打开调试模式

// 提交拓扑到集群中运行，拓扑名称为 "kafka"
StormSubmitter.submitTopology("kafka", config, builder.createTopology());

System.out.println("拓扑提交成功");
```

```java
// 自定义的过滤 Bolt，用于筛选包含 "Buy" 或 "Shopping_Car" 的日志
class FilterBolt extends BaseBasicBolt {
    @Override
    public void execute(Tuple input, BasicOutputCollector basicOutputCollector) {
        // 从 Kafka 消息中提取 value 字段（KafkaSpout 默认会包含 key 和 value 字段）
        String logMessage = input.getStringByField("value");

        // 过滤逻辑：只保留包含 "Buy" 或 "Shopping_Car" 的日志
        if (logMessage.contains("Buy") || logMessage.contains("Shopping_Car")) {

            // 发射过滤后的日志，字段名为 "message"（FieldNameBasedTupleToKafkaMapper 会自动识别）
            basicOutputCollector.emit(new Values(logMessage));
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) {
        // 声明输出字段名，KafkaBolt 默认使用 "message" 作为 value 字段
        outputFieldsDeclarer.declare(new Fields(FieldNameBasedTupleToKafkaMapper.BOLT_MESSAGE));
    }


}
```

&emsp;&emsp;由于我们需要用到Storm和Kafka的API，所以我们需要添加如下依赖，同时需要用打包插件将其打包到jar包中

```
    <dependencies>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-core</artifactId>
            <version>2.5.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>3.9.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-kafka-client</artifactId>
            <version>2.5.0</version>
        </dependency>
    </dependencies>
```

&emsp;&emsp;将打包后的kafka-1.0.jar放入`/root`下，并在`/root`目录中执行如下指令运行java程序：

&emsp;&emsp;`storm jar kafka-1.0.jar bigdata.Main`

_在m2主机执行如下操作_

&emsp;&emsp;为了看到java程序产生的模拟数据，可以在m2主机执行如下指令启动Kafka消费者，以此来查看我们是否成功产生数据并写入到Flume中。

```
cd ${KAFKA_HOME}
bin/kafka-console-consumer.sh --bootstrap-server m1:9092,m2:9092,m3:9092 --topic filtered_log --from-beginning
```

&emsp;&emsp;可以看到java程序在源源不断的将Action为Buy和Shopping_Car的数据过滤到`filtered_log`中。

```
206.157.216.156 贵州    2025-06-06 16:11:42     1749197502478   5084954962393300491     www.xxx.com     Buy
16.154.168.132  湖南    2025-06-06 16:11:43     1749197503479   5817257465258089752     www.xxx.com     Buy
16.74.53.158    重庆    2025-06-06 16:12:06     1749197526496   6126857654638531200     www.xxx.com     Buy
120.179.2.202   香港    2025-06-06 16:12:12     1749197532499   4344669149507618959     www.xxx.com     Shopping_Car
80.165.54.161   贵州    2025-06-06 16:12:15     1749197535501   4297301815858128684     www.xxx.com     Buy
145.84.200.154  湖北    2025-06-06 16:12:27     1749197547508   5782105993588636542     www.xxx.com     Shopping_Car
151.129.57.168  重庆    2025-06-06 16:12:33     1749197553510   8693469032956428268     www.xxx.com     Shopping_Car
179.29.0.7      辽宁    2025-06-06 16:12:37     1749197557512   3265866635631797190     www.xxx.com     Shopping_Car
```

**（7） Storm再次消费Kafka数据处理后保存数据到Hbase**

&emsp;&emsp;此时，在filtered_log这个topic中仅剩我们关心的日志信息了，接下来我们将日志信息进一步的简化，然后保存到Hbase中。

_在m1主机执行如下操作_

&emsp;&emsp;启动hdfs。

&emsp;&emsp;`start-dfs.sh`

_在m3主机执行如下操作_

&emsp;&emsp;启动hbase。

&emsp;&emsp;`start-hbase.sh`

&emsp;&emsp;进入控制台。

&emsp;&emsp;`hbase shell`

&emsp;&emsp;创建表`web`，后续用于存储数据。

&emsp;&emsp;`create 'web', 'cf'`

_在m1主机执行如下操作_

&emsp;&emsp;编写java程序，相关代码如下：

```java
String bootstrapServers = "m1:9092,m2:9092,m3:9092"; // Kafka Broker地址

// 创建 Storm 拓扑构建器
TopologyBuilder builder = new TopologyBuilder();

// 配置 KafkaSpout：读取 Kafka 中主题 filtered_log 的数据
KafkaSpoutConfig<String, String> kafkaSpoutConfig = KafkaSpoutConfig.builder(bootstrapServers, "filtered_log")
        .setProp("group.id", "hbase-group")
        .build();
builder.setSpout("kafkaSpout", new KafkaSpout<>(kafkaSpoutConfig));// 添加 Spout

// 添加处理 Bolt（提取 timestamp 和 action）
builder.setBolt("processBolt", new ProcessBolt()).shuffleGrouping("kafkaSpout");

// 添加 HBase 存储 Bolt（将数据写入 HBase）
builder.setBolt("hbaseBolt", new HBaseStorageBolt()).shuffleGrouping("processBolt");

// 创建并配置 Storm 配置对象
Config config = new Config();
config.setDebug(true);// 打开调试模式

// 提交拓扑到 Storm 集群
StormSubmitter.submitTopology("hbase", config, builder.createTopology());
System.out.println("拓扑提交成功");
```

```java
// 自定义FilterBolt类
class ProcessBolt extends BaseBasicBolt {
    @Override
    public void execute(Tuple input, BasicOutputCollector basicOutputCollector) {
        // 从 Kafka 消息中取出日志内容
        String logMessage = input.getStringByField("value");

        if (logMessage != null) {
            String infos[] = logMessage.split("\\t");// 使用制表符分割日志字段

            // 示例日志结构:
            // IP     地区    日期     时间戳         用户ID                  网站         操作
            // 180... 贵州   2018-..  1545...    5270...        www.xxx.com   Buy
            // 我们只取时间戳 infos[3] 和用户行为 infos[6]
            basicOutputCollector.emit(new Values(infos[3], infos[6]));
        }

    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) {
        // 输出字段名（使用 BOLT_MESSAGE）
        outputFieldsDeclarer.declare(new Fields(FieldNameBasedTupleToKafkaMapper.BOLT_MESSAGE));
    }


}
```

```java
// 自定义 Bolt：将日志中的时间戳与用户操作写入 HBase
class HBaseStorageBolt extends BaseRichBolt {
    private Connection connection;// HBase 连接
    private Table table;// HBase 表对象

    // 初始化方法：建立 HBase 连接
    @Override
    public void prepare(Map<String, Object> map, TopologyContext topologyContext, OutputCollector outputCollector) {
        try {
            connection = ConnectionFactory.createConnection(HBaseConfiguration.create());
            table = connection.getTable(org.apache.hadoop.hbase.TableName.valueOf("web"));// 使用 web 表

        } catch (Exception e) {
            throw new RuntimeException("无法连接到HBase", e);
        }
    }

    // 核心处理方法：将数据写入 HBase
    @Override
    public void execute(Tuple tuple) {
        try {
            if (tuple != null) {
                String time = tuple.getString(0);// 时间戳
                String userAction = tuple.getString(1);// 用户操作行为

                    // 构造 HBase 的 Put 对象，RowKey 使用 操作_时间戳，例如 Buy_1545290594752
                    Put put = new Put(Bytes.toBytes(userAction+"_"+time));
                    put.addColumn(Bytes.toBytes("cf"), // 列族
                            Bytes.toBytes(userAction),// 列名（行为名）
                            Bytes.toBytes( time ));// 值（时间戳）
                    table.put(put);// 写入 HBase

            }

        } catch (Exception e) {
            throw new RuntimeException("无法将数据存入Hbase", e);
        }
    }


    // 不声明输出字段（最终 Bolt）
    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        // No output fields to declare for HBase storage
    }

    // 清理资源
    @Override
    public void cleanup() {
        try {
            if (table != null) table.close();
            if (connection != null) connection.close();
        } catch (Exception e) {
            throw new RuntimeException("无法关闭Hbase资源", e);
        }
    }
}
```

&emsp;&emsp;由于我们需要用到Storm、Kafka和Hbase的API，所以我们需要添加如下依赖，同时需要用打包插件将其打包到jar包中

```
    <dependencies>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-core</artifactId>
            <version>2.5.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>3.9.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-kafka-client</artifactId>
            <version>2.5.0</version>
        </dependency>
        <!-- HBase Client -->
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-client</artifactId>
            <version>2.5.11</version>
        </dependency>
    </dependencies>
```

&emsp;&emsp;将打包后的hbase-1.0.jar放入`/root`下，并在`/root`目录中执行如下指令运行java程序：

&emsp;&emsp;`storm jar hbase-1.0.jar bigdata.Main`

_在m3主机执行如下操作_

&emsp;&emsp;此时在m3查看web表，会发现有源源不断的数据保存进去

&emsp;&emsp;`scan 'web'`

```
ROW                                                           COLUMN+CELL                                                                                                                                                                         
 Buy_1749183823162                                            column=cf:Buy, timestamp=2025-06-06T18:55:41.395, value=1749183823162                                                                                                               
 Buy_1749183828173                                            column=cf:Buy, timestamp=2025-06-06T18:55:41.413, value=1749183828173                                                                                                               
 Buy_1749183863266                                            column=cf:Buy, timestamp=2025-06-06T18:55:41.459, value=1749183863266                                                                                                                                          
 Shopping_Car_1749183838200                                   column=cf:Shopping_Car, timestamp=2025-06-06T18:55:41.424, value=1749183838200                                                                                                      
 Shopping_Car_1749183849231                                   column=cf:Shopping_Car, timestamp=2025-06-06T18:55:41.438, value=1749183849231                                                                                                      
 Shopping_Car_1749183850235                                   column=cf:Shopping_Car, timestamp=2025-06-06T18:55:41.446, value=1749183850235                                                                                                      
```

**（8） 结语**

&emsp;&emsp; 此时我们已经保存到了我们后续需要的日志信息，接下来我们可以通过统计的方法得知用户喜欢在什么时间最有购买欲，再通过推荐算法对用户进行推荐，这超出了我们本书的范畴，不再详述。