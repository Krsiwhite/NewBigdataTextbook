## 6.1 网站日志信息处理案例

### 6.1.1 实验背景

&emsp;&emsp;设想你有一个网站，用户在这个网站里面可以有很多行为，比如注册，登录，查看，点击，双击，购买东西，加入购物车，添加记录，修改记录，删除记录，评论，登出等一系列我们熟悉的操作。这些操作都被记录在日志信息里面。我们想分析出用户在什么时候喜欢购买东西，什么时候喜欢加入购物车，从而，在相应的时间采取行动，激励用户购买东西，推荐商品给用户，毕竟网站盈利才是我们希望达到的目的，对吧

&emsp;&emsp;如果我们想要进行上述的一个分析，那我们首先做的就是将日志信息进行清洗，挑出用户购买东西和加入购物车这两个行为的日志信息，然后将用户购买东西的时间点和加入购物车的时间点保存下来，以便我们后续的统计和推荐等一系列的操作。

&emsp;&emsp;在本章，我们就是要对网站中这样的日志信息进行清洗和保存。

### 6.1.2 实验准备

&emsp;&emsp;本次实验所使用的数据需要采集网站中用户行为，但是我们没有网站就不能实验了吗？当然不是，车到山前必有路，既然没有真实的网站，那我们就模拟一个网站，不断的随机产生用户行为，再交给我们熟悉的Flume、Kafka和Storm处理，最后再利用Excel生成报告，在这个过程中，我们依旧使用Java操作相关的API。

&emsp;&emsp;我们产生的日志格式规定如下，分别包含了ip地址、归属地、产生时间、产生时间对应的时间戳、用户id、网站地址和用户的行为。

&emsp;&emsp;`String log = ip + "\t" + address + "\t" + d + "\t" + timestamp + "\t" + userid + "\t" + Common.WEB_SITE + "\t" + action;`

&emsp;&emsp;其中用户的行为我们抽象以下几个：

&emsp;&emsp;`"Register", "Login", "View", "Click", "Double_Click", "Buy", "Shopping_Car", "Add", "Edit", "Delete", "Comment", "Logout" `

&emsp;&emsp;举一个例子，下面产生的数据代表这样的含义：ip地址为115.19.62.102，来自海南，产生时间为2025-12-20，产生时间对应的时间戳为1749139200000，用户的id为1735787074662918890，网站地址为www.xxx.com，用户的行为是Edit。

&emsp;&emsp;`115.19.62.102    海南    2025-06-06    1749139200000    1735787074662918890    www.xxx.com    Edit`

&emsp;&emsp;整个实验架构如下图所示：

<p align="center">
    <img src="/pic/6/6-3 实验架构图.png" width="50%">
    <br/>
    <em>图6-6 实验架构图</em>
</p>

### 6.1.3 正式开始实验

**（1） 配置Flume**

_在m1主机修改Flume配置_

&emsp;&emsp;修改Flume数据源类型为与topic名称，将`/root/apache-flume-1.11.0-bin/conf/flume.conf`中内容改为：

```
# avro.conf 配置⽂件
agent1.sources = r1
agent1.sinks = k1
agent1.channels = c1
# 配置 Source 监听端⼝为 4141 的 avro 服务
agent1.sources.r1.type = avro
agent1.sources.r1.bind = 0.0.0.0
agent1.sources.r1.port = 4141
agent1.sources.r1.channels = c1
# 配置 Sink 
agent1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
agent1.sinks.k1.topic = all_my_log
agent1.sinks.k1.brokerList = m1:9092,m2:9092,m3:9092
agent1.sinks.k1.requiredAcks = 1
agent1.sinks.k1.batchSize = 20
agent1.sinks.k1.channel = c1

# 配置 Sink 
# agent1.sinks.k1.type = logger 
# agent1.sinks.k1.channel = c1  



# 配置 Channel
# Use a channel which buffers events in memory
agent1.channels.c1.type = memory
agent1.channels.c1.capacity = 1000 
agent1.channels.c1.transactionCapacity = 100
 
```

**（2） 启动ZooKeeper**

_在三台主机执行如下操作_

&emsp;&emsp;启动ZooKeeper。

&emsp;&emsp;`zkServer.sh start`

**（3） 启动Kafka**

_在三台主机执行如下操作_

&emsp;&emsp;启动Kafka。

```
cd ${KAFKA_HOME}
bin/kafka-server-start.sh -daemon config/server.properties
```

**（4） 启动Flume**

_在m1主机执行如下操作_

&emsp;&emsp;启动Flume。

```
cd ${FLUME_HOME}
flume-ng agent --conf conf --conf-file ${FLUME_HOME}/conf/flume.conf --name agent1 -Dflume.root.logger=INFO,console
```

**（5） 产生日志信息并写入到Flume**

&emsp;&emsp;接下来，我们将用java程序模拟产生用户行为，并将这些行为的日志信息传给Flume来模拟Flume采集数据，然后再将日志信息传入Kafka，保存到all_my_log这个topic中。

&emsp;&emsp;编写java程序并打包。

&emsp;&emsp;bigdata/Main.java如下：

```
package bigdata;

import org.apache.flume.Event;
import org.apache.flume.EventDeliveryException;
import org.apache.flume.api.RpcClient;
import org.apache.flume.api.RpcClientFactory;
import org.apache.flume.event.EventBuilder;

import java.nio.charset.Charset;
import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.Random;

public class Main {


    public static void main(String[] args) {
        RpcClient client = RpcClientFactory.getDefaultInstance("localhost", 4141);
        Random random = new Random();
        String[] action_list = { "Register", "Login", "View", "Click", "Double_Click", "Buy", "Shopping_Car", "Add", "Edit", "Delete", "Comment", "Logout" };
        int maxIpNumber=224;
        String[] address_list = { "北京", "天津", "上海", "广东", "重庆", "河北", "山东", "河南", "云南", "山西", "甘肃", "安徽", "福建", "黑龙江", "海南", "四川", "贵州", "宁夏", "新疆", "湖北", "湖南", "山西", "辽宁", "吉林", "江苏", "浙江", "青海", "江西", "西藏", "内蒙", "广西", "香港", "澳门", "台湾", };
        String web_site="www.xxx.com";

        try {
            while (true) {
                // 获取当前时间
                Date date = new Date();
                SimpleDateFormat simpleDateFormat = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
                String d = simpleDateFormat.format(date);
                Long timestamp = new Date().getTime();
                // ip地址生成
                String ip = random.nextInt(maxIpNumber) + "." + random.nextInt(maxIpNumber) + "." + random.nextInt(maxIpNumber) + "." + random.nextInt(maxIpNumber);
                // ip地址对应的address(这里是为了构造数据，并没有按照真实的ip地址，找到对应的address)
                String address = address_list[random.nextInt(address_list.length)];
                Long userid = Math.abs(random.nextLong());

                String action = action_list[random.nextInt(action_list.length)];
                // 日志信息构造
                // example : 199.80.45.117 云南 2018-12-20 1545285957720 3086250439781555145 www.xxx.com Buy
                String data = ip + "\t" + address + "\t" + d + "\t" + timestamp + "\t" + userid + "\t" + web_site + "\t" + action;

                // 创建事件
                Event event = EventBuilder.withBody(data, Charset.forName("UTF-8"));

                // 发送事件到Flume
                client.append(event);
                // 每隔1秒发送一次
                Thread.sleep(1000);
            }
        } catch (EventDeliveryException e) {
            System.err.println("Failed to send event to Flume: " + e.getMessage());
        } catch (InterruptedException e) {
            System.err.println("Thread was interrupted: " + e.getMessage());
        } finally {
            // 关闭客户端
            client.close();
        }
    }
}
```

&emsp;&emsp;pom.xml如下：

```
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>bigdata</groupId>
    <artifactId>flume</artifactId>
    <version>1.0</version>

    <properties>
        <maven.compiler.source>8</maven.compiler.source>
        <maven.compiler.target>8</maven.compiler.target>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>
    <dependencies>
        <dependency>
            <groupId>org.apache.flume</groupId>
            <artifactId>flume-ng-sdk</artifactId>
            <version>1.11.0</version>
        </dependency>
    </dependencies>
    <build>
        <plugins>
            <!-- 打包插件 -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>3.2.4</version>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                        <configuration>
                            <transformers>
                                <!-- 指定主类 -->
                                <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                    <manifestEntries>
                                        <Main-Class>bigdata.Main</Main-Class>
                                    </manifestEntries>
                                </transformer>
                            </transformers>
                            <!-- 排除签名文件 -->
                            <filters>
                                <filter>
                                    <artifact>*:*</artifact>
                                    <excludes>
                                        <exclude>META-INF/*.SF</exclude>
                                        <exclude>META-INF/*.DSA</exclude>
                                        <exclude>META-INF/*.RSA</exclude>
                                        <exclude>defaults.yaml</exclude>
                                        <exclude>storm.yaml</exclude>
                                    </excludes>
                                </filter>
                            </filters>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
```

_在m2主机执行如下操作_

&emsp;&emsp;创建Topic。
```
cd ${KAFKA_HOME}
bin/kafka-topics.sh --create --bootstrap-server m1:9092,m2:9092,m3:9092 --replication-factor 2 --topic all_my_log --partitions 1
```

&emsp;&emsp;为了看到java程序产生的模拟数据，可以在m2主机执行如下指令启动Kafka消费者，以此来查看我们是否成功产生数据并写入到Flume中。

```
cd ${KAFKA_HOME}
bin/kafka-console-consumer.sh --bootstrap-server m1:9092,m2:9092,m3:9092 --topic all_my_log --from-beginning
```

_在m1主机执行如下操作_

&emsp;&emsp;新开一个终端：

&emsp;&emsp;将打包后的flume-1.0.jar放入`/root`下，并在`/root`目录中执行如下指令运行java程序：

&emsp;&emsp;`java -jar flume-1.0.jar`


&emsp;&emsp;可以看到java程序在源源不断的生成模拟数据。

```
205.202.216.132 西藏    2025-06-06 11:52:13     1749181933508   1575118096067576913     www.xxx.com     Add
215.71.29.198   内蒙    2025-06-06 11:52:14     1749181934648   8085481671476708579     www.xxx.com     Delete
27.179.57.100   安徽    2025-06-06 11:52:15     1749181935651   8761868746779775237     www.xxx.com     Comment
163.204.25.82   重庆    2025-06-06 11:52:16     1749181936656   5296735721083876963     www.xxx.com     Click
119.35.210.201  香港    2025-06-06 11:52:17     1749181937659   8407441108851234857     www.xxx.com     Edit
182.216.168.31  山东    2025-06-06 11:52:18     1749181938662   957585683426560284      www.xxx.com     Shopping_Car
123.161.195.144 河北    2025-06-06 11:52:19     1749181939665   784399186589498682      www.xxx.com     Register
58.99.184.152   安徽    2025-06-06 11:52:20     1749181940668   1449311844059526711     www.xxx.com     Delete
```

**（6） 清洗数据**


&emsp;&emsp;我们已经有了用户执行各种行为的日志信息，比如Add、Delete、Click、Buy和Shopping_Car等，但是我们关心只有购买和加购物车，也就是Buy和Shopping_Car两个操作，所以，我们只要保留这两种行为的日志信息，也就是说，接下来我们要把all_my_log这个topic中复杂的日志信息给过滤，去除我们不关心的日志信息，并把过滤后的日志信息保存到filtered_log这个topic中去

_在m1主机执行如下操作_

&emsp;&emsp;新开一个终端：

&emsp;&emsp;开启nimbus。

`storm nimbus &`

_在m2主机执行如下操作_

&emsp;&emsp;开启supervisor。

`storm supervisor &`

&emsp;&emsp;创建Topic。
```
cd ${KAFKA_HOME}
bin/kafka-topics.sh --create --bootstrap-server m1:9092,m2:9092,m3:9092 --replication-factor 2 --topic filtered_log --partitions 1
```

_在m1主机执行如下操作_

&emsp;&emsp;编写java程序并打包。

&emsp;&emsp;bigdata/Main.java如下：

```
package bigdata;

import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Table;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.storm.Config;
import org.apache.storm.LocalCluster;
import org.apache.storm.StormSubmitter;
import org.apache.storm.generated.AlreadyAliveException;
import org.apache.storm.generated.AuthorizationException;
import org.apache.storm.generated.InvalidTopologyException;
import org.apache.storm.kafka.spout.KafkaSpout;
import org.apache.storm.kafka.spout.KafkaSpoutConfig;
import org.apache.storm.kafka.bolt.KafkaBolt;
import org.apache.storm.kafka.bolt.mapper.FieldNameBasedTupleToKafkaMapper;
import org.apache.storm.kafka.bolt.selector.DefaultTopicSelector;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.thrift.TException;
import org.apache.storm.topology.BasicOutputCollector;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.TopologyBuilder;
import org.apache.storm.topology.base.BaseBasicBolt;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;

import java.util.HashMap;
import java.util.Map;
import java.util.Properties;

public class Main {

    public static void main(String[] args) throws Exception {
        String bootstrapServers = "m1:9092,m2:9092,m3:9092"; // Kafka broker地址

        // 构建Topology
        TopologyBuilder builder = new TopologyBuilder();

        // 配置KafkaSpout
        KafkaSpoutConfig<String, String> kafkaSpoutConfig = KafkaSpoutConfig.builder(bootstrapServers, "all_my_log")
                .setProp("group.id", "kafka-group")
                .build();
        builder.setSpout("kafkaSpout", new KafkaSpout<>(kafkaSpoutConfig));


        builder.setBolt("filterBolt", new FilterBolt()).shuffleGrouping("kafkaSpout");

        // 配置KafkaBolt
        Properties props = new Properties();
        props.put("bootstrap.servers", "m1:9092,m2:9092,m3:9092");
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        KafkaBolt<String, String> kafkaBolt = new KafkaBolt<String, String>()
                .withTopicSelector(new DefaultTopicSelector("filtered_log"))
                .withTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper<>())
                .withProducerProperties(props);
        builder.setBolt("kafkaBolt", kafkaBolt).shuffleGrouping("filterBolt");


        Config config = new Config();

        config.setDebug(true);

        StormSubmitter.submitTopology("kafka", config, builder.createTopology());

        System.out.println("拓扑提交成功");
    }
}

// 自定义FilterBolt类
class FilterBolt extends BaseBasicBolt {
    @Override
    public void execute(Tuple input, BasicOutputCollector basicOutputCollector) {


        String logMessage = input.getStringByField("value");

//         在这里实现过滤逻辑
        if (logMessage.contains("Buy") || logMessage.contains("Shopping_Car")) {
            basicOutputCollector.emit(new Values(logMessage));
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) {
        outputFieldsDeclarer.declare(new Fields(FieldNameBasedTupleToKafkaMapper.BOLT_MESSAGE));
    }


}

```

&emsp;&emsp;pom.xml如下：

```
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>bigdata</groupId>
    <artifactId>kafka</artifactId>
    <version>1.0</version>

    <properties>
        <maven.compiler.source>8</maven.compiler.source>
        <maven.compiler.target>8</maven.compiler.target>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>
    <dependencies>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-core</artifactId>
            <version>2.5.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>3.9.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-kafka-client</artifactId>
            <version>2.5.0</version>
        </dependency>
 
    </dependencies>
    <build>
        <plugins>
            <!-- 打包插件 -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>3.2.4</version>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                        <configuration>
                            <transformers>
                                <!-- 指定主类 -->
                                <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                    <manifestEntries>
                                        <Main-Class>bigdata.Main</Main-Class>
                                    </manifestEntries>
                                </transformer>
                            </transformers>
                            <!-- 排除签名文件 -->
                            <filters>
                                <filter>
                                    <artifact>*:*</artifact>
                                    <excludes>
                                        <exclude>META-INF/*.SF</exclude>
                                        <exclude>META-INF/*.DSA</exclude>
                                        <exclude>META-INF/*.RSA</exclude>
                                        <exclude>defaults.yaml</exclude>
                                        <exclude>storm.yaml</exclude>
                                    </excludes>
                                </filter>
                            </filters>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
```

&emsp;&emsp;将打包后的flume-1.0.jar放入`/root`下，并在`/root`目录中执行如下指令运行java程序：

&emsp;&emsp;`storm jar kafka-1.0.jar bigdata.Main`

_在m2主机执行如下操作_

&emsp;&emsp;为了看到java程序产生的模拟数据，可以在m2主机执行如下指令启动Kafka消费者，以此来查看我们是否成功产生数据并写入到Flume中。

```
cd ${KAFKA_HOME}
bin/kafka-console-consumer.sh --bootstrap-server m1:9092,m2:9092,m3:9092 --topic filtered_log --from-beginning
```

&emsp;&emsp;可以看到java程序在源源不断的将Action为Buy和Shopping_Car的数据过滤到`filtered_log`中。

```
206.157.216.156 贵州    2025-06-06 16:11:42     1749197502478   5084954962393300491     www.xxx.com     Buy
16.154.168.132  湖南    2025-06-06 16:11:43     1749197503479   5817257465258089752     www.xxx.com     Buy
16.74.53.158    重庆    2025-06-06 16:12:06     1749197526496   6126857654638531200     www.xxx.com     Buy
120.179.2.202   香港    2025-06-06 16:12:12     1749197532499   4344669149507618959     www.xxx.com     Shopping_Car
80.165.54.161   贵州    2025-06-06 16:12:15     1749197535501   4297301815858128684     www.xxx.com     Buy
145.84.200.154  湖北    2025-06-06 16:12:27     1749197547508   5782105993588636542     www.xxx.com     Shopping_Car
151.129.57.168  重庆    2025-06-06 16:12:33     1749197553510   8693469032956428268     www.xxx.com     Shopping_Car
179.29.0.7      辽宁    2025-06-06 16:12:37     1749197557512   3265866635631797190     www.xxx.com     Shopping_Car
```

**（7） Storm再次消费Kafka数据处理后保存数据到Hbase**

&emsp;&emsp;此时，在filtered_log这个topic中仅剩我们关心的日志信息了，接下来我们将日志信息进一步的简化，然后保存到Hbase中。

_在m1主机执行如下操作_

&emsp;&emsp;启动hdfs。

&emsp;&emsp;`start-dfs.sh`

_在m3主机执行如下操作_

&emsp;&emsp;启动hbase。

&emsp;&emsp;`start-hbase.sh`

&emsp;&emsp;进入控制台。

&emsp;&emsp;`hbase shell`

&emsp;&emsp;创建表`web`，后续用于存储数据。

&emsp;&emsp;`create 'web', 'cf'`

_在m1主机执行如下操作_

&emsp;&emsp;编写java程序并打包。

&emsp;&emsp;bigdata/Main.java如下：

```
package bigdata;

import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Table;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.storm.Config;
import org.apache.storm.StormSubmitter;
import org.apache.storm.kafka.spout.KafkaSpout;
import org.apache.storm.kafka.spout.KafkaSpoutConfig;
import org.apache.storm.kafka.bolt.mapper.FieldNameBasedTupleToKafkaMapper;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.BasicOutputCollector;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.TopologyBuilder;
import org.apache.storm.topology.base.BaseBasicBolt;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.Values;

import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.Map;

public class Main {

    public static void main(String[] args) throws Exception {
        String bootstrapServers = "m1:9092,m2:9092,m3:9092"; // Kafka broker地址

        // 构建Topology
        TopologyBuilder builder = new TopologyBuilder();

        // 配置KafkaSpout
        KafkaSpoutConfig<String, String> kafkaSpoutConfig = KafkaSpoutConfig.builder(bootstrapServers, "filtered_log")
                .setProp("group.id", "hbase-group")
                .build();
        builder.setSpout("kafkaSpout", new KafkaSpout<>(kafkaSpoutConfig));


        builder.setBolt("processBolt", new ProcessBolt()).shuffleGrouping("kafkaSpout");

        builder.setBolt("hbaseBolt", new HBaseStorageBolt()).shuffleGrouping("processBolt");


        Config config = new Config();

        config.setDebug(true);

        StormSubmitter.submitTopology("hbase", config, builder.createTopology());

        System.out.println("拓扑提交成功");
    }
}

// 自定义FilterBolt类
class ProcessBolt extends BaseBasicBolt {
    @Override
    public void execute(Tuple input, BasicOutputCollector basicOutputCollector) {

        String logMessage = input.getStringByField("value");
        if (logMessage != null) {
            String infos[] = logMessage.split("\\t");
            //180.21.100.66    贵州    2018-12-20    1545290594752    5270357330431599426    www.xxx.com    Buy
            //1545290594752 Buy
            basicOutputCollector.emit(new Values(infos[3], infos[6]));
        }

    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) {
        outputFieldsDeclarer.declare(new Fields(FieldNameBasedTupleToKafkaMapper.BOLT_MESSAGE));
    }


}

class HBaseStorageBolt extends BaseRichBolt {
    private Connection connection;
    private Table table;



    @Override
    public void prepare(Map<String, Object> map, TopologyContext topologyContext, OutputCollector outputCollector) {
        try {
            connection = ConnectionFactory.createConnection(HBaseConfiguration.create());
            table = connection.getTable(org.apache.hadoop.hbase.TableName.valueOf("web"));

        } catch (Exception e) {
            throw new RuntimeException("无法连接到HBase", e);
        }
    }

    @Override
    public void execute(Tuple tuple) {
        try {


            if (tuple != null) {
                // base one ProcessBolt.declareOutputFields()
                String time = tuple.getString(0);
                String userAction = tuple.getString(1);

                    //往hbase中写入数据
                    Put put = new Put(Bytes.toBytes(userAction+"_"+time)); // Replace "row_key" with dynamic key logic if needed
                    put.addColumn(Bytes.toBytes("cf"), Bytes.toBytes(userAction), Bytes.toBytes( time ));
                    table.put(put);

            }

        } catch (Exception e) {
            throw new RuntimeException("无法将数据存入Hbase", e);
        }
    }


    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        // No output fields to declare for HBase storage
    }

    @Override
    public void cleanup() {
        try {
            if (table != null) table.close();
            if (connection != null) connection.close();
        } catch (Exception e) {
            throw new RuntimeException("无法关闭Hbase资源", e);
        }
    }
}

```

&emsp;&emsp;pom.xml如下：

```
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>bigdata</groupId>
    <artifactId>hbase</artifactId>
    <version>1.0</version>

    <properties>
        <maven.compiler.source>8</maven.compiler.source>
        <maven.compiler.target>8</maven.compiler.target>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    </properties>
    <dependencies>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-core</artifactId>
            <version>2.5.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>3.9.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-kafka-client</artifactId>
            <version>2.5.0</version>
        </dependency>
        <!-- HBase Client -->
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-client</artifactId>
            <version>2.5.11</version>
        </dependency>
    </dependencies>
    <build>
        <plugins>
            <!-- 打包插件 -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>3.2.4</version>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                        <configuration>
                            <transformers>
                                <!-- 指定主类 -->
                                <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                    <manifestEntries>
                                        <Main-Class>bigdata.Main</Main-Class>
                                    </manifestEntries>
                                </transformer>
                            </transformers>
                            <!-- 排除签名文件 -->
                            <filters>
                                <filter>
                                    <artifact>*:*</artifact>
                                    <excludes>
                                        <exclude>META-INF/*.SF</exclude>
                                        <exclude>META-INF/*.DSA</exclude>
                                        <exclude>META-INF/*.RSA</exclude>
                                        <exclude>defaults.yaml</exclude>
                                        <exclude>storm.yaml</exclude>
                                    </excludes>
                                </filter>
                            </filters>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
```

&emsp;&emsp;将打包后的hbase-1.0.jar放入`/root`下，并在`/root`目录中执行如下指令运行java程序：

&emsp;&emsp;`storm jar hbase-1.0.jar bigdata.Main`

_在m3主机执行如下操作_

&emsp;&emsp;此时在m3查看web表，会发现有源源不断的数据保存进去

&emsp;&emsp;`scan 'web'`

```
ROW                                                           COLUMN+CELL                                                                                                                                                                         
 Buy_1749183823162                                            column=cf:Buy, timestamp=2025-06-06T18:55:41.395, value=1749183823162                                                                                                               
 Buy_1749183828173                                            column=cf:Buy, timestamp=2025-06-06T18:55:41.413, value=1749183828173                                                                                                               
 Buy_1749183863266                                            column=cf:Buy, timestamp=2025-06-06T18:55:41.459, value=1749183863266                                                                                                               
 Buy_1749183895352                                            column=cf:Buy, timestamp=2025-06-06T18:55:41.494, value=1749183895352                                                                                                               
 Buy_1749183903369                                            column=cf:Buy, timestamp=2025-06-06T18:55:41.523, value=1749183903369                                                                                                               
 Buy_1749183915401                                            column=cf:Buy, timestamp=2025-06-06T18:55:41.543, value=1749183915401                                                                                                                                    
 Shopping_Car_1749183838200                                   column=cf:Shopping_Car, timestamp=2025-06-06T18:55:41.424, value=1749183838200                                                                                                      
 Shopping_Car_1749183849231                                   column=cf:Shopping_Car, timestamp=2025-06-06T18:55:41.438, value=1749183849231                                                                                                      
 Shopping_Car_1749183850235                                   column=cf:Shopping_Car, timestamp=2025-06-06T18:55:41.446, value=1749183850235                                                                                                      
 Shopping_Car_1749183858255                                   column=cf:Shopping_Car, timestamp=2025-06-06T18:55:41.450, value=1749183858255                                                                                                      
 Shopping_Car_1749183864269                                   column=cf:Shopping_Car, timestamp=2025-06-06T18:55:41.465, value=1749183864269                                                                                                      

```

**（8） 后续**

&emsp;&emsp; 此时我们已经保存到了我们后续需要的日志信息，接下来我们可以通过统计的方法得知用户喜欢在什么时间最有购买欲，再通过推荐算法对用户进行推荐，这超出了我们本书的范畴，不再详述。