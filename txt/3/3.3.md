## 3.2 MapReduce案例

&emsp;&emsp;对于学习MapReduce，那一定听过它最经典的案例，WordCount单词统计任务。
>需求：在一堆给定的文本文件中统计输出每一个单词出现的总次数

**（1）准备工作**
1.创建一个文件
```
vim 单词计数待用.txt
```
2.上传到HDFS
```
hdfs dfs -mkdir /wordcount/
hdfs dfs -put 单词计数待用.txt /wordcount/
```
3.创建java-Maven项目。

3.1创建Maven项目，jdk版本选择1.8。

3.2创建包
右击src-main-java，选择new-package
名字为cn.itcast.mapreduce
然后右击cn.itcast.mapreduce，选择new-java class
建三个java类，分别为JobMain,WordCountMapper,WordCountReducer。

3.3构建pom.xml文件
```
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <!-- 项目坐标 -->
    <groupId>org.example</groupId>
    <artifactId>WordCounts</artifactId>
    <version>1.0-SNAPSHOT</version>
    <packaging>jar</packaging>
    <!-- 编译和 Java 版本配置 -->
    <properties>
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <hadoop.version>3.3.6</hadoop.version>
    </properties>

    <!-- 依赖项 -->
    <dependencies>
        <!-- Hadoop 核心库 -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>${hadoop.version}</version>
        </dependency>

        <!-- MapReduce 客户端核心 -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-mapreduce-client-core</artifactId>
            <version>${hadoop.version}</version>
        </dependency>

        <!-- HDFS 客户端 -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-hdfs</artifactId>
            <version>${hadoop.version}</version>
        </dependency>
    </dependencies>

    <!-- 构建插件 -->
    <build>
        <plugins>
            <!-- 编译插件 -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.8.1</version>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>

            <!-- 打包插件 -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-jar-plugin</artifactId>
                <version>3.2.0</version>
                <configuration>
                    <archive>
                        <manifest>
                            <addClasspath>true</addClasspath>
                            <mainClass>cn.itcast.mapreduce.JobMain</mainClass>
                        </manifest>
                    </archive>
                </configuration>
            </plugin>
        </plugins>
    </build>
</project>
```

4编写WordCountMapper
```
package cn.itcast.mapreduce;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

public class WordCountMapper extends Mapper<LongWritable, Text, Text, LongWritable> {

    // 输出的 key 和 value 对象，避免重复创建对象，提高性能
    private final Text word = new Text();
    private final LongWritable one = new LongWritable(1);
    // 正则表达式：匹配字母、数字和空格，替换掉其他字符
    private static final Pattern CLEAN_PATTERN = Pattern.compile("[^a-zA-Z0-9\\s]");

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        // 获取输入的一行文本
        String line = value.toString();
        // 清洗文本：去掉特殊字符，转小写
        String cleanedLine = cleanText(line);
        // 按空格分割成单词
        String[] words = cleanedLine.split("\\s+");
        // 遍历所有单词，输出 <word, 1>
        for (String wordStr : words) {
            if (!wordStr.isEmpty()) {
                word.set(wordStr);
                context.write(word, one);
            }
        }
    }

    //清洗文本的方法
    private String cleanText(String text) {
        // 替换非字母、数字和空格的字符为空格
        Matcher matcher = CLEAN_PATTERN.matcher(text);
        text = matcher.replaceAll(" ");
        // 转小写
        text = text.toLowerCase();
        // 多个空格合并为一个
        text = text.replaceAll("\\s+", " ");
        // 去除首尾空格
        return text.trim();
    }
}
```
5编写WordCountReducer
```
package cn.itcast.mapreduce;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class WordCountReducer extends Reducer<Text, LongWritable, Text, LongWritable> {

    // 用于存储最终输出的 value
    private final LongWritable result = new LongWritable();

    @Override
    protected void reduce(Text key, Iterable<LongWritable> values, Context context)
            throws IOException, InterruptedException {
        long sum = 0;
        // 遍历所有值，进行累加
        for (LongWritable value : values) {
            sum += value.get(); // 累加每个单词出现的次数
        }
        // 设置结果并写出
        result.set(sum);
        context.write(key, result); // 输出 <单词, 总次数>
    }
}
```

6编写JobMain类
```
package cn.itcast.mapreduce;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class JobMain extends Configured implements Tool {
    @Override
    public int run(String[] strings) throws Exception {
        //创建 Job 对象
        Job job = Job.getInstance(super.getConf(), "Word Count");

        //设置读取文件的类
        job.setInputFormatClass(TextInputFormat.class);
        TextInputFormat.addInputPath(job, new Path("hdfs://m1:8020/wordcount/单词计数待用"));

        //设置主类
        job.setJarByClass(JobMain.class);
        //设置Mapper类及输出类型
        job.setMapperClass(WordCountMapper.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(LongWritable.class);

        //设置Reducer类及输出类型
        job.setReducerClass(WordCountReducer.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(LongWritable.class);
        //设置输出类及输出类型
        job.setOutputFormatClass(TextOutputFormat.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(LongWritable.class);
        //设置输出的路径
        TextOutputFormat.setOutputPath(job, new Path("hdfs://m1:8020/wordcount/output"));

        boolean b = job.waitForCompletion(true);
        return b ? 0 : 1;
    }

    public static void main(String[] args) throws Exception {
        //创建配置对象
        Configuration conf = new Configuration();
        //启动一个任务
        int run = ToolRunner.run(conf, new JobMain(), args);
        System.exit(run);
    }
}
```
7添加分区（如果需要分区可以看这里，如果不需要的话可以忽略）
7.1新建类AlphabeticalPartitioner

7.2编写分类规则，这里选择按照单词首字母进行分区。
```
package cn.itcast.mapreduce;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Partitioner;

public class AlphabeticalPartitioner extends Partitioner<Text, IntWritable> {

    @Override
    public int getPartition(Text key, IntWritable value, int numPartitions) {
        // 获取单词的第一个字符
        String word = key.toString();
        if (word == null || word.isEmpty()) {
            return 0; // 如果单词为空或null，返回第0个分区
        }
        char firstChar = Character.toUpperCase(word.charAt(0));

        // 计算分区编号，A-Z 对应 0-25
        if (firstChar >= 'A' && firstChar <= 'Z') {
            return firstChar - 'A';
        } else {
            // 对于非A-Z的字符，可以将其分配到最后一个分区或者特殊处理
            return 25; // 这里我们将它们分配到最后一个分区
        }
    }
}
```
7.3在JobMain里添加对应的设置
在run函数里添加
```
        //设置Partitioner类
        job.setPartitionerClass(AlphabeticalPartitioner.class);
        //设置Reducer数量为26，对应26个字母
        job.setNumReduceTasks(26);
```

8.打包
点击maven，选择Lifecycle，先点击clean，清楚上次打包的内容，然后点击test，点击屏蔽，我们在打包时跳过测试，然后点击oackage进行打包。
将打包好的，target里的jar文件复制到桌面，并上传到服务器。
```
scp 'C:\Users\Administrator\Desktop\WordCounts-1.0-SNAPSHOT.jar' root@119.3.208.111:/root/
```