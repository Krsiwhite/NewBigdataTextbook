## 3.3 MapReduce案例

&emsp;&emsp;MapReduce最经典的入门案例是单词统计（WordCount），它用于统计文本中每个单词出现的次数。
>需求：在一堆给定的文本文件中统计输出每一个单词出现的总次数

**（1）准备工作**
1. 创建一个文件
```
vim wordcount.txt
```
2. 上传到HDFS
```
hdfs dfs -mkdir /wordcount/
hdfs dfs -put wordcount.txt /wordcount/
```
3. 创建java-Maven项目。

3.1.创建Maven项目，jdk版本选择1.8。

3.2. 创建包

&emsp;&emsp;右击src-main-java，选择new-package，名字为cn.itcast.mapreduce，然后右击cn.itcast.mapreduce，选择new-java class，建三个java类，分别为JobMain,WordCountMapper,WordCountReducer。

3.3. 构建pom.xml文件
&emsp;&emsp;pom.xml中引入3.3.6版本的hadoop，包括核心库hadoop-common，MapReduce客户端核心hadoop-mapreduce-client-core，HDFS客户端hadoop-hdfs


4. 编写Mapper类——WordCountMapper.java。

&emsp;&emsp;设置Java包路径，并引入必要的包
```
package cn.itcast.mapreduce;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
```
&emsp;&emsp;定义WordCountMapper类，继承Mapper。类型都是Hadoop针对任务自己编写的类型。LongWritable相当于Java的long。Text相当于Java的string。

&emsp;&emsp;输入 key 类型：LongWritable（偏移量）
&emsp;&emsp;输入 value 类型：Text（一行文本）
&emsp;&emsp;输出 key 类型：Text（单词）
&emsp;&emsp;输出 value 类型：LongWritable（计数 1）
```
public class WordCountMapper extends Mapper<LongWritable, Text, Text, LongWritable> {
```
&emsp;&emsp;通过new定义两个常量对象，避免每次创建新对象，提高性能。
```
    private final Text word = new Text();
    private final LongWritable one = new LongWritable(1);
```
&emsp;&emsp;使用正则表达式自定义清洗规则，去除非字母数字和空格的其他字符。
```
    private static final Pattern CLEAN_PATTERN = Pattern.compile("[^a-zA-Z0-9\\s]");
```
&emsp;&emsp;定义清洗函数，使用正则表达式清洗数据，toLowerCase()转化为小写。"\\s+"表示多个空格，我们通过replaceAll将多个空格合并为一个，最后通过trim()去除首尾空格。
```
    private String cleanText(String text) {
        Matcher matcher = CLEAN_PATTERN.matcher(text);
        text = matcher.replaceAll(" ");
        text = text.toLowerCase();
        text = text.replaceAll("\\s+", " ");
        return text.trim();
    }
```
&emsp;&emsp;重写map方法，这是每个Mapper的核心方法，处理每行输入。
```
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
```
&emsp;&emsp;将输入类型转化为字符串类型，并调用清洗函数清洗文本，按空格将单词分割出来。
```
        String line = value.toString();
        String cleanedLine = cleanText(line);
        String[] words = cleanedLine.split("\\s+");
```
&emsp;&emsp;使用for循环，枚举所有单词，放到我们已经创建好的常量里。context是上下文变量，用于和MapReduce的其他组件沟通，这里通过context.write()将输出传递给上下文对象。
```
        for (String wordStr : words) {
            if (!wordStr.isEmpty()) {
                word.set(wordStr);
                context.write(word, one);
            }
        }
    }
}
```
5. 编写WordCountReducer

&emsp;&emsp;设置Java包路径，并引入必要的包
```
package cn.itcast.mapreduce;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;
```
&emsp;&emsp;定义 WordCountReducer 类，继承 Reducer，参数含义如下：

&emsp;&emsp;输入 key 类型：Text（单词）；
&emsp;&emsp;输入 value 类型：LongWritable（计数值）；
&emsp;&emsp;输出 key 类型：Text（单词）；
&emsp;&emsp;输出 value 类型：LongWritable（总次数）。
```
public class WordCountReducer extends Reducer<Text, LongWritable, Text, LongWritable> {
```
&emsp;&emsp;缓存输出结果对象，提高性能。
```
    private final LongWritable result = new LongWritable();
```
&emsp;&emsp;重写reduce()方法，接受参数为：
&emsp;&emsp;Text key（单词）；
&emsp;&emsp;Iterable<LongWritable> values（一个集合，里面有若干计数值）。
```
    @Override
    protected void reduce(Text key, Iterable<LongWritable> values, Context context)
            throws IOException, InterruptedException {
```
&emsp;&emsp;遍历所有计数值，累加。
```
        long sum = 0;
        for (LongWritable value : values) {
            sum += value.get();
        }
```
&emsp;&emsp;将结果放到预先创建的常量对象里，然后输出给上下文对象。
```
        result.set(sum);
        context.write(key, result);
```
6. 编写AlphabeticalPartitioner类自定义分区规则（可选）

&emsp;&emsp;回顾3.1所提的shuffle部分，包含了分区，排序，分组三步骤。这里是自定义的分区规则，我们自定义按照单词的首字母进行分区，划分为26个区。
设置包路径和必要依赖。
```
package cn.itcast.mapreduce;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Partitioner;
```
&emsp;&emsp;定义AlphabeticalPartitioner类，继承分区父类Partitioner。
```
    public int getPartition(Text key, LongWritable value, int numPartitions) {
```
&emsp;&emsp;重写getPartition()获取分区函数。
```
    @Override
    public int getPartition(Text key, LongWritable value, int numPartitions) {
```
&emsp;&emsp;考虑特殊情况，如果单词为空或者null，返回0分区。
&emsp;&emsp;然后转化为大写，按照首字母进行分区，如果第一个字符不是字母，那么分到最后一个分区。
```
        String word = key.toString();
        if (word == null || word.isEmpty()) {
            return 0; 
        }
        char firstChar = Character.toUpperCase(word.charAt(0));

        if (firstChar >= 'A' && firstChar <= 'Z') {
            return firstChar - 'A';
        } else {
            return 25; 
        }
```

7. 编写JobMain类。

&emsp;&emsp;JobMain是MapReduce作业的入口类，负责配置和启动作业。

7.1 引入必要的Hadoop库、配置。
```
package cn.itcast.mapreduce;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
```
7.2 写主类，继承Configured implements Tool。
```
public class JobMain extends Configured implements Tool {
```
7.2.1在主类里写main函数。

&emsp;&emsp;main函数这里的任务，就是启动一个任务，具体包括创建配置微信，并且利用run来启动一个任务。
```
public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        int run = ToolRunner.run(conf, new JobMain(), args);
        System.exit(run);
    }
```
7.2.2接下来重写run函数，我们的主要流程将在这里体现，包括创建Job对象，设置各个类。
```
    @Override
    public int run(String[] strings) throws Exception {
```
&emsp;&emsp;在run函数里，使用 getInstance() 方法创建一个新的 Job 实例，并设置作业名称为 "Word Count"。这个名称会在 Hadoop 的 Web UI 中显示。
```
        //创建 Job 对象
        Job job = Job.getInstance(super.getConf(), "Word Count");
```
&emsp;&emsp;设置主类，告诉MapReduce，main函数在哪里。
```
    job.setJarByClass(JobMain.class);
```
&emsp;&emsp;然后就是按顺序以此完成**读取文件**，**Map**，**Shuffle**，**Reduce**，**输出文件**的设置。

&emsp;&emsp;设置读取文件类，并添加输入路径。
&emsp;&emsp;为保证代码灵活性，这里我们不采用固定的路径，而是设置为读入的第一个参数，我们在命令行启动时，告诉它输入路径即可。
```
        job.setInputFormatClass(TextInputFormat.class);
        TextInputFormat.addInputPath(job, new Path(args[0]));
```
&emsp;&emsp;设置Mapper类及输出类型。
&emsp;&emsp;将我们自定义的类型写上，输出的key为文本，value为数字。
```
        job.setMapperClass(WordCountMapper.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(LongWritable.class);
```
&emsp;&emsp;设置shuffle类。
&emsp;&emsp;这里如果使用默认方法，则什么都不需要做。如果做了第六步，自定义了分区规则，那么我们就需要设置一下分区类Partitioner。以及Reducer的个数，与分区一致，为26。
```
        job.setPartitionerClass(AlphabeticalPartitioner.class);
        job.setNumReduceTasks(26);
```
&emsp;&emsp;设置Reducer类及输出类型。
```
        job.setReducerClass(WordCountReducer.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(LongWritable.class);
```
&emsp;&emsp;设置输出类及输出类型，还有输出的路径，这里采用命令行的第二个参数。
```
        job.setOutputFormatClass(TextOutputFormat.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(LongWritable.class);
        TextOutputFormat.setOutputPath(job, new Path("hdfs://m1:8020/wordcount/output"));
```
&emsp;&emsp;然后等待任务完成。
```
        boolean b = job.waitForCompletion(true);
        return b ? 0 : 1;
```

8. 打包
   
&emsp;&emsp;点击maven，选择Lifecycle，先点击clean，清楚上次打包的内容，然后点击test，点击屏蔽，我们在打包时跳过测试，然后点击oackage进行打包。
将打包好的，target里的jar文件复制到桌面，并上传到服务器。
```
scp 'C:\Users\Administrator\Desktop\WordCounts-1.0-SNAPSHOT.jar' root@119.3.208.111:/root/
```
9. 运行

&emsp;&emsp;通过指令 *hadoop jar 'jar包' '输入路径' '输出路径'* 来运行代码。
&emsp;&emsp;**注意：输出路径必须不存在**。如果已经存在，需要通过rm -f删除。
```
hadoop jar WordCounts-1.0-SNAPSHOT.jar /wordcount/wordcount.txt /wordcount/OutputPath
```
&emsp;&emsp;运行成功后，可以通过50070端口查看hadoop的webUI页面。
```
http://1.92.70.157:50070/
```
&emsp;&emsp;注意：由于hadoop的高可用性，hadoop的主节点不一定在m1服务器上，**也有可能在其他服务器上**。
&emsp;&emsp;访问后，看到overv后面有active，则表示hadoop主节点在当前服务器上运行。点击Utilities->Browse the file system，可以查看文件系统。进入/wordcount/OutputPath，可以看到我们的输出，有27个文件，一个success表示任务完成标记，其余26个文件来源于我们的26个分区。

&emsp;&emsp;还可以通过8088端口查看yarn装填。
```
http://121.36.18.123:8088/
```
