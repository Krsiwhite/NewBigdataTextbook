## 3.3 MapReduce案例

&emsp;&emsp;MapReduce最经典的入门案例是单词统计（WordCount），它用于统计文本中每个单词出现的次数。现在，让我们来实际编写一个MapReduce程序。MapReduce程序有严格的框架，我们必须按照其的规定自定义Map任务和Reduce任务，其一方面提供了统一的编程框架，而另一方面如此严格的框架也影响了程序的灵活性。接下来就让我们开始本次的单词计数案例的讲解。


### 3.3.1 准备数据集

&emsp;&emsp;在本次实验中，我们会使用一文本数据集，其大小为340MB，共有350万行，以`txt`形式保存，样式如图3-8所示。

<p align="center">
    <img src="/pic/3/3-8 文本数据集样式.png" width="50%">
    <br/>
    <em>图3-8 文本数据集样式</em>
</p>

&emsp;&emsp;将其上传至HDFS集群中。

```
hdfs dfs -mkdir /wordcount/
hdfs dfs -put wordcount.txt /wordcount/
```

### 3.3.2 创建项目，导入相关依赖

&emsp;&emsp;MapReduce程序一般以`Java-Maven`项目的形式编写，故我们需要新建`Maven`项目（这部分内容不在本书做重点讲解），使用JDK1.8并且导入编写MapReduce程序所必须的依赖包，在`pom.xml`中导入如下依赖：

```xml
<dependencies>
    <dependency>            <!-- Hadoop 核心库 -->
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>3.3.6</version>
    </dependency>   
    <dependency>            <!-- MapReduce 客户端核心 -->
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-mapreduce-client-core</artifactId>
        <version>3.3.6</version>
    </dependency>  
    <dependency>            <!-- HDFS 客户端 -->
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-hdfs</artifactId>
        <version>3.3.6</version>
    </dependency>
</dependencies>
```

&emsp;&emsp;注意在`pom.xml`文件中设置在编译时打包所有依赖，并设置正确的主类，否则程序无法正常运行。

### 3.3.3 编写代码

_应与3.1节中MapReduce流程相应内容结合学习_

&emsp;&emsp;在项目目录`/src/main/java`下新建Java包在其中新建3个Java类：

* `JobMain`：程序的入口，在这里组合我们编写的Map和Reduce函数。
* `WordCountMapper`：在这里编写Map任务的逻辑。
* `WordCountReducer`：在这里编写Reduce任务的逻辑。

**（1） 编写Mapper**


&emsp;&emsp;在`WordCountMapper`中引入必要的包：
```java
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import java.io.IOException;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
```

&emsp;&emsp;接着编写我们自己的Mapper，定义WordCountMapper类，继承自Mapper。

```java
public class WordCountMapper extends Mapper<LongWritable, Text, Text, LongWritable> {
    ......
}    
```

&emsp;&emsp;泛型参数中我们需要指定输入键值对的类型和输出键值对的类型。
```
<K1, V1, K2, V2>
```

回顾3.2节的内容，数据首先经过`InputFormat`，然后进入Mapper中。

>......选用`TextInputFormat`来处理数据......将每一行作为一个键值对，Key是该行在整个文件中的字节偏移量，Value则是该行行所对应的字符串......把如此格式切分后的值称为`<K1, V1>`。

&emsp;&emsp;所以此时我们的K1也即输入键应该为一个整形`int`，而V1应该指定成字符串`string`。而上文我们的`WordCountMapper`中对应位置填写的是`LongWritable`和`Text`，这是为什么呢？

&emsp;&emsp;这是因为在MapReduce中，其对Java中的基本类型做了封装，以便更快速地处理，其对应关系如图3-9所示。

<p align="center">
    <img src="/pic/3/3-9 类型对应关系.png" width="50%">
    <br/>
    <em>图3-9 类型对应关系</em>
</p>

&emsp;&emsp;所以在此处我们的K1设置为了长整形，而V1当然就是字符串对应的`Text`。经过Mapper后，我们应该分割出了一个个单词，每个单词计数1，故K2是`Text`，而V2是长整形。

&emsp;&emsp;接着我们需要编写对每个键值对的处理逻辑，按3.1所述，我们首先应该清洗每行的字符串，只保留英文字符。故定义相应的正则表达式，该正则会识别初字母数字以及空格以外的所有字符。

```
    private static final Pattern CLEAN_PATTERN = Pattern.compile("[^a-zA-Z0-9\\s]");
```

&emsp;&emsp;接着编写清洗函数，该函数应接受一个字符串，返回清洗后的字符串。首先使用正则匹配，识别出需要清洗的字符，使用`replaceAll()`将它们全部转换为空格，接着`toLowerCase()`将字符串转化为小写。然后通过`replaceAll()`将多个空格合并为一个，最后通过`trim()`去除首尾空格后返回。
```
    private String cleanText(String text) {
        Matcher matcher = CLEAN_PATTERN.matcher(text);
        text = matcher.replaceAll(" ");
        text = text.toLowerCase();
        text = text.replaceAll("\\s+", " ");
        return text.trim();
    }
```

&emsp;&emsp;之后在类中定义两个对象，避免在每次键值对的处理中创建新对象，提高性能。
```
    private final Text word = new Text();
    private final LongWritable one = new LongWritable(1);
```


&emsp;&emsp;最后一步，重写map方法，这是每个Mapper的核心方法，其规定了处理单个输入键值对的逻辑，其的三个参数，前两个分别是传进来的K1和V1，接着，处理完成后产生的`<K2,V2>`通过上下文变量`context`传递给下一步的处理。
```java
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        ......
    }
```
&emsp;&emsp;在map中首先将输入类型转化为字符串类型，并调用清洗函数清洗文本，按空格将单词分割出来。在这一步后得到一个words数组，其中每个元素就是一个单词。

```java
        String line = value.toString();
        String cleanedLine = cleanText(line);
        String[] words = cleanedLine.split("\\s+");
```

&emsp;&emsp;接着使用for循环，枚举所有单词，赋值到已经创建好的变量`word`里。通过`context.write()`将输出`<word, one>`也即`<K2, V2>`传递给后续组件处理。

```java
        for (String wordStr : words) {      // example: words = ["hello", "world"]
            if (!wordStr.isEmpty()) {
                word.set(wordStr);
                context.write(word, one);   // write("hello", 1), write("world", 1)    
            }
        }
```

**（2） 编写Reducer**

&emsp;&emsp;在`WordCountReducer`中引入必要的包：

```java
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
import java.io.IOException;
```

&emsp;&emsp;定义WordCountReducer类，继承Reducer，泛型参数中我们需要指定输入键值对的类型和输出键值对的类型。


```
public class WordCountReducer extends Reducer<Text, LongWritable, Text, LongWritable> {
    ......
}
```

&emsp;&emsp;在这里我们接受上一步Map后传进的`<K2,V2>`，输出最后的结果`<K3,V3>`。故输入键值对的类型应和Mapper中的输出键值对类型保持一致，输出键值对中Key为单词，Value为该单词的计数。


&emsp;&emsp;同样的，类中定义对象，避免在每次键值对的处理中创建新对象，提高性能。
```
    private final LongWritable result = new LongWritable();
```

&emsp;&emsp;之后重写reduce()方法，reduce()方法定义了Reduce阶段的逻辑。这里需要注意，前两个参数和map相同依旧为输入键值对，当我们观察发现，输入的Value也即V2为一个集合，这是因为在3.1中我们介绍过，Map完成后会经过Shuffle，其会将所有Key相同（在这里就是同一个单词）的Value放置在同一个集合中。输出依旧通过上下文变量传递给后续组件处理。

```
    @Override
    protected void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException {
        ......
    }
```
&emsp;&emsp;这里的逻辑很简单，对传进的键值对，遍历Values集合中元素的个数，即为Key对应单词的计数。最后将结果赋值给预先创建的常量对象里，然后输出给上下文对象。
```java
        long sum = 0;     // example: key ="hello"                                           
        for (LongWritable value : values) {     // value = [1, 1, 1, 1]
            sum += value.get();                 // final sum = 4
        }
        result.set(sum);
        context.write(key, result);
```


**（3） 自定义分区规则（可选）**

&emsp;&emsp;回顾3.1所提的Shuffle部分，包含了分区，排序，分组三步骤。默认状态下，所有键值对会分配到同一个分区，也即同一个Reducer中处理。我们可以自定义分区规则，将单词发送到不同分区中。在这里我们按照单词的首字母进行分区，划分为26个区。

&emsp;&emsp;新建Java类`AlphabeticalPartitioner`，并导入相关包：
```
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Partitioner;
```

&emsp;&emsp;定义`AlphabeticalPartitioner`类，继承分区父类`Partitioner`。在这里前两个参数为经过Mapper处理后得到的`<K2,V2>`的类型。
```java
public class AlphabeticalPartitioner extends Partitioner<Text, LongWritable> {
    ......
}    
```

&emsp;&emsp;我们只需要重写`getPartition()`方法，编写自定义的逻辑来将键值对分到对应的区中。该方法前两个参数为输入键值对，即经过Mapper处理后的`<K2,V2>`，第三个参数为分区数量，返回值为`int`，即为该键值对对应的分区编号。

```java
    @Override
    public int getPartition(Text key, LongWritable value, int numPartitions) {
        ......
    }    
```

&emsp;&emsp;在方法中，将单词首字母转化为大写，按照首字母进行分区，考虑特殊情况，如果单词为空或者null，返回0分区；如果第一个字符不是字母，那么分到最后一个分区。
```java
        String word = key.toString();
        if (word == null || word.isEmpty()) {
            return 0; 
        }
        char firstChar = Character.toUpperCase(word.charAt(0));

        if (firstChar >= 'A' && firstChar <= 'Z') {
            return firstChar - 'A';
        } else {
            return 25; 
        }
```

**（4） 编写JobMain类**

&emsp;&emsp;在编写完了各个环节所需方法后，我们来编写`JobMain`。其是MapReduce作业的入口类，负责配置和启动作业。

&emsp;&emsp;首先导入必要的依赖包。

```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
```

&emsp;&emsp;编写主类`JobMain`，继承Configured implements Tool。

```
public class JobMain extends Configured implements Tool {
    ......
}    
```
&emsp;&emsp;接着编写main函数。main函数的功能是启动一个MapReduce任务，具体包括创建配置文件，并且利用`run()`方法来启动一个任务。
```java
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        int run = ToolRunner.run(conf, new JobMain(), args);
        System.exit(run);
    }
```
&emsp;&emsp;`JobMain`的重点是重写`run()`方法，整个任务的主要流程将在这里体现。

```java
    @Override
    public int run(String[] strings) throws Exception {
        ......
    }
```
&emsp;&emsp;在`run()`方法里，使用`getInstance()`方法创建一个新的 Job 实例，并设置作业名称为 Word Count。这个名称会在 Hadoop 的 WebUI 中显示。
```
        //创建 Job 对象
        Job job = Job.getInstance(super.getConf(), "Word Count");
```
&emsp;&emsp;接着设置主类，告知MapReduce程序的入口。
```
        job.setJarByClass(JobMain.class);
```
&emsp;&emsp;然后就是按顺序完成在3.1节所述的**读取文件**，**Map**，**Shuffle**，**Reduce**，**输出文件**的各个操作。其中，Map,Shuffle,Reduce阶段的操作我们已定义好。

&emsp;&emsp;首先设置`InputFormat`为`TextInputFormat`，在该案例中，原始数据集的路径使用参数传入。
```java
        job.setInputFormatClass(TextInputFormat.class);
        TextInputFormat.addInputPath(job, new Path(args[0]));
```

&emsp;&emsp;接着设置Mapper类及输出类型。输出类型为`<K2,V2>`的类型。
```java
        job.setMapperClass(WordCountMapper.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(LongWritable.class);
```
&emsp;&emsp;之后设置Shuffle类，（如果未做选做部分，则跳过）自定义了分区规则，并设置分区的数量，其应该与ReduceTask数量保持一致(因为每个分区对应一个Reducer，也就是对应一个ReduceTask)，为26。
```java
        job.setPartitionerClass(AlphabeticalPartitioner.class);
        job.setNumReduceTasks(26);
```
&emsp;&emsp;然后设置Reducer类及输出类型。输出类型为`<K3,V3>`的类型。
```java
        job.setReducerClass(WordCountReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(LongWritable.class);
```
&emsp;&emsp;最后设置`OutputFormat`为`TextOutputFormat`，并输出路径通过参数传入。并使用`waitForCompletion()`方法等待任务完成。
```java
        job.setOutputFormatClass(TextOutputFormat.class);
        TextOutputFormat.setOutputPath(job, new Path(args[1]));
        boolean b = job.waitForCompletion(true);
        return b ? 0 : 1;
```

&emsp;&emsp;现在，我们已经编写好了所有程序代码，在maven中将项目打包并将jar包上传到集群中。


### 3.3.4 运行作业


&emsp;&emsp;通过指令 `hadoop jar 'jar包' '输入路径' '输出路径'` 来运行代码。
```shell
hadoop jar WordCounts-1.0-SNAPSHOT.jar hdfs://m1:8020/wordcount/wordcount.txt hdfs://m1:8020/your_path
```

&emsp;&emsp;其开始运行后，会在控制台中实时显示进度，也可通过8088端口在Yarn的webUI上实时查看任务的处理情况。

```
......
2025-06-04 16:55:43,525 INF0 mapreduce.Job: map 0%  reduce 0%
2025-06-04 16:56:00,719 INF0 mapreduce.Job: map 15% reduce 0%
2025-06-04 16:56:02,734 INF0 mapreduce.Job: map 37% reduce 0%
2025-06-04 16:56:06,775 INF0 mapreduce.Job: map 38% reduce 0%
......
```

### 3.3.5 查看作业结果

&emsp;&emsp;在任务运行完成后，我们可以在输出路径查看作业运行结果，如图3-10所示。`part-r-xxxxx`文件中后面的数字代表不同的分区，这里一共有26个分区对应26个输出文件，0号分区中应该存储的是所有字母a开头的单词计数结果，打开该文件，可看到结果，如图3-11所示。


<p align="center">
    <img src="/pic/3/3-10 查看结果.png" width="50%">
    <br/>
    <em>图3-10 任务结果</em>
</p>

<p align="center">
    <img src="/pic/3/a开头单词计数结果.png" width="50%">
    <br/>
    <em>图3-11 a开头单词计数结果</em>
</p>


&emsp;&emsp;在YARN的WebUI中我们可以查看该任务所申请的资源数量，如图3-12所示。从图3-12中可以看到，总共使用了30个容器。在3.1节中，我们提到容器（Container）是YARN中申请资源的单位，在MapReduce中每个Task（ReduceTask和MapTask）对应一个容器。现在让我们来计算一下，为什么这里申请了30个容器。首先每个作业的执行需要一个AM（ApplicationMaster）来控制，AM也需要放置在容器中，那么还剩下的29个容器便是为Task所占用。


&emsp;&emsp;在这其中，3个容器请求在本地节点运行（Node Local Containers），这代表其希望在数据所在的DataNode运行，这3个容器便对应着3个MapTask。我们的输入是一个300+MB的文件，划分为Block存储在HDFS集群中，每128MB划分一次，划分为了3个Block。数据传入MapReduce作业后，每个Block便成为了一个分片，而3.1中我们提到每个分片由一个MapTask单独处理。那为什么它们要请求在本地节点运行呢？这代表这三个容器分别位于这三个Block所在的节点上，这样就相当于做“本地运算”，省去了传输数据的开销。

&emsp;&emsp;剩下的2个容器请求在任意可用节点运行（Off Switch Containers），因为我们有26个分区，故这是对应着26个ReduceTask，ReduceTask一般不对数据的位置有要求，所以可以在任意节点运行。

<p align="center">
    <img src="/pic/3/3.3 yarn查看任务详情.png" width="50%">
    <br/>
    <em>图3-12 作业申请的资源数量</em>
</p>

### 3.3.6 总结

&emsp;&emsp;本节介绍了编写MapReduce作业的一般方法，并且结合单词计数的实践做了进一步说明，在其中我们做了分区的扩展，使得大家能更好的理解整个MapReduce的流程，在最后我们详细讨论了YARN上资源的分配与Task的关系。