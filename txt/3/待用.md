介绍mapreduce编程思想--以单词计数的流程举例

而后介绍yarn，安装yarn

而后java访问HDFS，介绍mapreduce编码流程，java编写mapreduce程序

接着是spark













_以下内容留存到第三章待用_
###   2.3.2 Zookeeper和Yarn
&emsp;&emsp;Yarn 是 Hadoop 2.0 引入的资源管理系统，负责集群资源管理和任务调度。Yarn作为一个通用的资源管理系统，其目标是将短作业和长服务混合部署到一个集群中，并为它们提供统一的资源管理和任务调度功能。

&emsp;&emsp;资源管理：将多台机器的资源进行整合，构建成一个整体。
    整个集群中所有资源的分配。
    运行这个程序分配多少资源给它运行。

&emsp;&emsp;任务调度：提交运行一个分布式程序，利用分布式的资源来实现运行。
    当提交了多个程序时，谁先执行，谁后执行的问题。

&emsp;&emsp;Yarn总体上采用master/slave架构，其中，ResourceManager 为master，NodeManager为slave, ResourceManager负责对各个NodeManager上的资源进行统一管理和调度。

<p align="center">
    <img src="pic/2/2-4Yarn架构.jpg" width="50%">
    <br/>
    <em>图2-4 Yarn架构</em>
</p>

&emsp;&emsp;ResourceManager (RM) ：是一个全局的资源管理器，负责整个系统的资源管理和分配。由两个组件构成：调度器(Scheduler) 和应用管理器(Applications Manager, ASM)。

1. 调度器： 主要功能是根据资源容量，队列等方面的限制条件，将系统中的资源分配给各个应用程序；
2. 应用管理器：负责管理整个系统中的所有应用程序。

&emsp;&emsp;ApplicationMaster (AM) ：用户提交的每个应用程序均包含一个独立的AM,其主要功能包括：

1. 与RM调度器协商以获取资源（用Container表示）；
2. 将得到的资源进一步分配给内部的任务；
3. 与NodeManager通信以启动/停止任务；
4. 监控所有任务的运行状态，并在任务运行失败时重新为任务申请资源以重启任务。

&emsp;&emsp;NodeManager (NM) ： NM是每个节点上的资源管理器。其主要功能包括：

1. 会定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态；
2. 接收并处理来自AM的任务启动/停止等各种请求。在一个集群中，NM通常存在多个，由于Yarn内置了容错机制，单个NM的故障不会对集群中的应用程序运行产生严重影响。

&emsp;&emsp;Container ： 是Yarn中的基本资源分配单位，是对应用程序运行环境的抽象，并为应用程序提供资源隔离环境。Container最终是由ContainerExecutor启动和运行的，Yarn提供了三种可选的ContainerExecutor:

1. DefaultContainerExecutor：默认ContainerExecutor实现，直接以进程方式启动Container,不提供任何隔离机制和安全机制。
2. LinuxContainerExecutor：提供了安全和Cgroups隔离的ContainerExecutor，它以应用程序提交者的身份运行Container，且使用Cgroups为Container提供CPU和内存隔离的运行环境。
3. DockerContainerExecutor：基于Docker实现的ContainerExecutor,可直接在YARN集群中运行Docker Container。

&emsp;&emsp;Yarn工作流程：

1. 提交应用程序：用户通过客户端与YARN ResourceManager通信，以提交应用程序，应用程序中需包含ApplicationMaster可执行代码、启动命令和资源需求、应用程序可执行代码和资源需求、优先级、提交到的队列等信息。
2. 启动ApplicationMaster： ResourceManager为该应用程序分配第一个Container,并与对应的NodeManager通信，要求它在这个Container中启动应用程序的ApplicationMaster，之后ApplicationMaster的生命周期直接被ResourceManager管理。
3. ApplicationMaster注册： ApplicationMaster启动后，首先向ResourceManager注册，这样，用户可以直接通过ResourceManager查看应用程序的运行状态，然后，它将初始化应用程序，并按照一定的策略为内部任务申请资源，监控它们的运行状态，直到运行结束，即重复步骤
4. 资源获取: ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源。
5. 请求启动Container ：一旦ApplicationMaster申请到资源后，则与对应的NodeManager通信，请求为其启动任务（NodeManager会将任务放到Container中）。
6. 启动Container：NodeManager为任务设置好运行环境(包括环境变量、jar包、二进制程序等)后，将任务启动命令写到一个脚本中，并通过ContainerExecutor运行该脚本启动任务。
7. Container监控：ApplicationMaster可通过两种方式获取各个Container的运行状态，以便在任务失败时重新启动任务。
ApplicationMaster与ResourceManager间维护了周期性心跳信息，每次通信可获取自己分管的Container的运行状态。
各个Container可通过某个RPC协议向ApplicationMaster汇报自己的状态和进度。
8. 注销ApplicationMaster：应用程序运行完成后，ApplicationMaster 向ResourceManager注销，并退出执行。

<p align="center">
    <img src="pic/2/2-5 Yarn工作流程.jpg" width="50%">
    <br/>
    <em>图2-5 Yarn工作流程</em>
</p>

&emsp;&emsp;Yarn资源调度器：资源调度器是HadoopYARN中最核心的组件之一，它是ResourceManager中的一个插拔式服务组件，负责整个集群资源的管理和分配。

&emsp;&emsp;Hadoop最初是为批处理作业而设计的，当时(MRv1) 仅提供了一种简单的FIFO (First InFirst Out)调度机制分配任务。在Hadoop 0.20.x版本或者更早的版本，Hadoop采用了平级队列组织方式：管理员将用户和资源分到若干个扁平队列中，在每个队列中，可指定一个或几个队列管理员管理这些用户和资源，比如杀死任意用户的应用程序，修改任意用户应用程序的优先级等。随着Hadoop应用越来越广泛，扁平化的队列组织方式已不能满足实际需求，从而出现了层级队列组织方式。

&emsp;&emsp;下面给出一个层级队列管理的例子：
在一个Hadoop集群中，管理员将所有计算资源划分给了两个队列，每个队列对应一个“组织”，其中有一个组织叫 “Engineering”，占用系统总资源的60%，它内部包含两个子队列“Development”和“QA”，分别占用80%和20%的资源；另一个组织叫”Marketing“，占用系统总资源的40%，它内部也包含两个子队列“Sales”和“Advertising”，分别占用30%和70%的资源。

<p align="center">
    <img src="pic/2/2-6 层次队列组织方式.jpg" width="50%">
    <br/>
    <em>图2-6 层次队列组织方式</em>
</p>

&emsp;&emsp;在实际生产环境中，对于“Engineering”队列而言，管理员可能想更有效地控制这60%资源，比如将大部分资源分配给“Development”队列的同时，能够让“QA”有最少资源保证，当“Development”中80%基础资源有剩余时，可优先共享给同父子队列“QA”，但为了防止“QA”一次性获得全部资源以至于“Development”需要资源时无法第一时间回收它们，可将“QA”最多可获得资源设为35%，为此，一种可能的配置方式如下：

<p align="center">
    <img src="pic/2/2-7 层级队列配置方式.jpg" width="50%">
    <br/>
    <em>图2-7 层级队列配置方式</em>
</p>

&emsp;&emsp;层级队列组织方式的特点:

1. 子队列：队列可以嵌套，每个队列均可以包含子队列。用户只能将应用程序提交到最底层的队列，即叶子队列。
2. 最少容量：每个子队列均有一个“最少容量比”属性，表示可以使用父队列的容量百分比。调度器总是优先选择当前资源使用率最低的队列，并为之分配资源。
3. 最大容量：为了防止一个队列超量使用资源，可以为队列设置一个最大容量，这是一个资源使用上限，任何时刻使用的资源总量不能超过该值。

&emsp;&emsp;Hadoop最初的设计目的是支持大数据批处理作业，如日志挖掘、Web索引等作业，为此，Hadoop仅提供了一个非常简单的调度机制： FIFO， 即先来先服务，在该调度机制下，所有作业被统一提交到一个队列中，Hadoop按照提交顺序依次运行这些作业。但随着Hadoop的普及，单个Hadoop集群的用户量越来越大，不同用户提交的应用程序往往具有不同的服务质量要求(Quality Of Service,简称QoS)，简单的FIFO调度策略不仅不能满足多样化需求，也不能充分利用硬件资源。为了克服单队列FIFO调度器的不足，多用户多队列调度器诞生了。当前主要有两种多用户资源调度器设计思路：

1. 第一种是在一个物理集群上虚拟多个Hadoop集群，这些集群各自拥有全套独立的Hadoop服务，典型的代表是HOD (Hadoop On Demand)调度器。
2. 另一种是扩展Hadoop调度器，使之支持多个队列多用户，这种调度器允许管理员按照应用需求对用户或者应用程序分组，并为不同的分组分配不同的资源量，同时通过添加各种约束防止单个用户或者应用程序独占资源，进而能够满足各种QoS需求，典型代表是Yahoo!的Capacity Scheduler和Facebook的Fair Scheduler。

&emsp;&emsp;Capacity Scheduler：以队列为单位划分资源，每个队列可设定一定比例的资源最低保证和使用上限，同时，每个用户也可设定一定的资源使用上限以防止资源滥用，而当一个队列的资源有剩余时，可暂时将剩余资源共享给其他队列。其特点如下：
1. 容量保证：管理员可为每个队列设置资源最低保证和资源使用上限，而所有提交到该队列的应用程序共享这些资源。
2. 灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列释放资源后会归还给该队列。
3. 多重租赁：支持多用户共享集群和多应用程序同时运行，为防止单个应用程序、用户或者队列独占集群中的资源，管理员可为之增加多重约束（比如单个用户最多使用的资源量）。
4. 安全保证：管理员可通过ACL限制每个队列的访问控制列表，普通用户可为自己的应用程序指定其他哪些用户可管理它。
5. 动态更新配置文件：管理员可根据需要动态修改各种资源调度器相关配置参数而无需重启集群。

&emsp;&emsp;Capacity Scheduler允许用户在配置文件 capacity-scheduler.xml 中设置队列层级关系、队列资源占用比等信息，以层次队列组织方式为例，Capacity Scheduler的配置方式下图所示。

<p align="center">
    <img src="pic/2/2-8 Capacity Scheduler配置方式.jpg" width="50%">
    <br/>
    <em>图2-8 Capacity Scheduler配置方式</em>
</p>

&emsp;&emsp;Fair Scheduler：同Capacity Scheduler类似，以队列为单位划分资源，每个队列可设定一定比例的资源最低保证和使用上限，同时，每个用户也可设定一定的资源使用上限以防止资源滥用；当一个队列的资源有剩余时，可暂时将剩余资源共享给其他队列。Fair Scheduler与Capacity Scheduler不同之处：

1. 资源公平共享：在每个队列中，Fair Scheduler可选择按照FIFO、Fair等为应用程序分配资源，其中Fair策略是一种基于最大最小公平算法实现的资源多路复用方式；默认情况下，每个队列内部采用该方式分配资源。这意味着，如果一个队列中有两个应用程序同时运行，则每个应用程序可得到1/2的资源；如果三个应用程序同时运行，则每个应用程序可得到1/3的资源。
2. 调度策略配置灵活：Fair Scheduler允许管理员为每个队列单独设置调度策略（当前支持FIFO、Fair和DRF三种）。
3. 提高小应用程序响应时间：由于采用了最大最小公平算法，小作业可以快速获取资源并运行完成。
4. 应用程序在队列间转移：用户可动态将一个正在运行的应用从一个队列转移到另外一个队列中。

<p align="center">
    <img src="pic/2/2-9 Capacity Scheduler和Fair Scheduler对比.jpg" width="50%">
    <br/>
    <em>图2-9 Capacity Scheduler和Fair Scheduler对比</em>
</p>

&emsp;&emsp;基于节点标签的调度：是从2.6.0版本开始，Yarn引入的一种新的调度策略。该机制的主要引入动机是更好地让Yarn运行在异构集群中，进而更好地管理和调度混合类型的应用程序。基于标签的调度是一种调度策略，就像基于优先级的调度一样，是调度器中众多调度策略中的一种，可以跟其他调度策略混合使用。基本思想是：用户可为每个NodeManager打上标签，比如highmem，highdisk等， 以作为NodeManager的基本属性；同时，用户可以为调度器中的队列设置若干标签，以限制该队列只能占用包含对应标签的节点资源，这样，提交到某个队列中的作业，只能运行在特定的一些节点上。

&emsp;&emsp;一个简单的应用案例：公司A最初Hadoop集群共有20个节点，硬件资源是32GB内存，4TB磁盘；后来，随着Spark计算框架的流行，公司希望引入Spark技术，而为了更好地运行Spark程序，公司特地买了10个大内存节点，内存是64GB。为了让Spark与MapReduce等不同类型的程序“和谐”地运行在一个集群中，公司A规定:：Spark程序只运行在后来的10个大内存节点上，而之前的MapReduce程序既可以运行在之前的20个节点上，也可以运行在后来的10个大内存节点上，如下图所示:

<p align="center">
    <img src="pic/2/2-10 公司A计算资源情况.jpg" width="50%">
    <br/>
    <em>图2-10 公司A计算资源情况</em>
</p>

&emsp;&emsp;基于标签的调度机制：
    1） 为20个旧节点打上normal标签，为10个新节点打上highmem标签。设置系统级别label：yarn rmadmin -addToClusterNodeLabels “normal, highmem”为节点打label，比如节点node1的label为normal：
    yarn rmadmin -replacelabelsOnNode “node1-address, normal"
    2）在Capacity Scheduler中，创建两个队列，分别是hadoop和spark，其中hadoop队列可使用的标签是normal和highmem，其中normal默认为label，而spark则是highmem，并配置两个队列的capacity和max-capacity等属性。
    资源容量配置如下：
        capacity(hadoop) = 50 #hadoop队列可使用的无label资源比例为50%
        capacity(hadoop, label=normal) = 100 #hadoop队列可使用的normal标签资源比例为100%
        capacity(hadoop, label= highmem)=10 #hadoop队列可使用的highmem标签资源比例为10%
        capacity(spark) = 50 #spark队列可使用的无标签资源比例为50%
        capacity(spark, label= highmem) =90 #spark队列可使用的highmem标签资源比例为90%
    3）将Spark作业提交到spark队列中，MapReduce作业提交到hadoop队列中(需指定使用的哪种label资源，否则spark资源永远无法得到使用，默认是normal)。
        #提交Spark作业到spark队列中
        spark- submit --queue spark –class XXX...
        #提交MapReduce作业到hadoop队列中
        hadoop jar x.jar -Dmapreduce.job.queuename=hadoop…

&emsp;&emsp;Yarn的高可用性：Yarn高可用性(High Availability)是指通过部署多个ResourceManager(RM)实例，确保当主RM故障时，系统能自动切换到备用RM，从而避免单点故障，保证集群持续可用。

&emsp;&emsp;高可用性（HA）的核心机制：
1. 主备架构：一个Active RM，一个或多个Standby RM
2. 状态同步：通过Zookeeper共享RM状态
3. 自动故障转移：Active RM故障时自动选举新Active RM
4. 客户端透明切换：客户端自动重定向到新Active RM

&emsp;&emsp;Yarn高可用性的配置：编辑`yarn-site.xml`（所有节点），将其修改为：

```
<?xml version="1.0"?>
<configuration>
  <!-- 属性配置 -->
    <!-- 启用HA -->
    <property>
    <name>yarn.resourcemanager.ha.enabled</name>
    <value>true</value>
    </property>

    <!-- 定义集群ID -->
    <property>
    <name>yarn.resourcemanager.cluster-id</name>
    <value>yarn-cluster</value>
    </property>

    <!-- 定义RM节点ID -->
    <property>
    <name>yarn.resourcemanager.ha.rm-ids</name>
    <value>rm1,rm2</value>
    </property>

    <!-- 指定各RM主机 -->
    <property>
    <name>yarn.resourcemanager.hostname.rm1</name>
    <value>m1</value>
    </property>
    <property>
    <name>yarn.resourcemanager.hostname.rm2</name>
    <value>m2</value>
    </property>

    <!-- Zookeeper配置 -->
    <property>
    <name>yarn.resourcemanager.zk-address</name>
    <value>m1:2181,m2:2181,m3:2181</value>
    </property>
</configuration>
```

&emsp;&emsp;Yarn高可用性的验证：
1. 按顺序停止服务
```
# 在m1上停止单节点RM
yarn --daemon stop resourcemanager

# 在所有节点停止NodeManager
yarn --daemon stop nodemanager  # m1/m2/m3
```
2. 启动Zookeeper集群
```
# 在m1/m2/m3上启动ZK
zkServer.sh start
```
3. 启动HA架构服务
```
# 在m1上启动rm1
yarn --daemon start resourcemanager

# 在m2上启动rm2
yarn --daemon start resourcemanager

# 在所有节点启动NodeManager
yarn --daemon start nodemanager  # m1/m2/m3

# 在m2上启动JobHistory
mapred --daemon start historyserver
```
4. 验证结果
```
# 检查rm状态
yarn rmadmin -getServiceState rm1  # 应为active
yarn rmadmin -getServiceState rm2  # 应为standby


# 测试故障转移
# 模拟m1故障
kill -9 $(pgrep -f ResourceManager)  # 在m1上执行

# 检查自动切换
yarn rmadmin -getServiceState rm2  # 应自动变为active
```
