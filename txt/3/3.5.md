## 3.5 Spark案例
&emsp;&emsp;现在我们来使用PySpark来实现单词计数小程序。

1. 安装PySpark

&emsp;&emsp;如果尚未安装PySpark，通过pip安装：
```
pip install pyspark
```

2. 创建并上传数据到HDFS
```
hdfs dfs -mkdir /wordcount/
hdfs dfs -put wordcount_input.txt /wordcount/
```

3. 编写PySpark程序——wordcount.py
   
&emsp;&emsp;使用PySpark实现简单的词频统计程序。

3.1 引入必要的模块

&emsp;&emsp;SparkSession：用于构建 Spark 应用；

&emsp;&emsp;re：正则表达式模块，用于文本清洗；

&emsp;&emsp;sys：用于获取命令行参数和退出程序。

```
from pyspark.sql import SparkSession
import re
import sys
```

3.2对每行文本进行清洗处理，确保只保留字母、数字和单个空格。

&emsp;&emsp;使用re的正则表达式，将不是字母、数字的字符替换为空格。转化为小写后将多个空格替换为单个空格。最后使用strip()去除首尾空格。
```
def clean_text(text):
    text = re.sub(r'[^a-zA-Z0-9\s]', ' ', text)
    text = text.lower()
    text = re.sub(r'\s+', ' ', text)
    return text.strip()
```

3.3开始写主程序，同样使用命令行参数来给定输入输出路径

```
def main():
    input_path = sys.argv[1]
    output_path = sys.argv[2]

```
&emsp;&emsp;这里的路径，如果包含"hdfs://"，则表示在hdfs上的文件，包含"file://"则表示是本地文件。

3.4创建SparkSession
```
    spark = SparkSession.builder.appName("WordCount").getOrCreate()
```

3.5执行MapReduce流程

&emsp;&emsp;这里我们使用spark的以下方法来自定义mapreduce流程：

&emsp;&emsp;textFile()：读取文本文件，返回 RDD[String]；

&emsp;&emsp;flatMap()：对每一行调用 clean_text() 并分割成单词列表；

&emsp;&emsp;map()：将每个单词映射为 (word, 1) 键值对；

&emsp;&emsp;reduceByKey()：按照 key（即单词）聚合计数；

&emsp;&emsp;sortBy()：按 value（计数）降序排序。
```
    try:
        word_counts = spark.sparkContext.textFile(input_path) \
            .flatMap(lambda line: clean_text(line).split()) \
            .map(lambda word: (word, 1)) \
            .reduceByKey(lambda a, b: a + b) \
            .sortBy(lambda x: x[1], ascending=False)
```
3.6输出答案

```
      word_counts.saveAsTextFile(output_path)
```

3.7异常处理与终止。

&emsp;&emsp;捕获运行时异常，最终确保spark.stop()被执行，释放资源。
```
    except Exception as e:
        print(f"Error: {str(e)}", file=sys.stderr)
        sys.exit(1)
    finally:
        spark.stop()
```

4. 运行代码

```
spark-submit --master yarn python.py hdfs:///输入 hdfs:///输出
```
5. 在yarn上查看任务详情

<p align="center">
    <img src="/pic/3/3.3yarn查看spark任务详情.png" width="50%">
    <br/>
    <em>3-17 yarn查看spark任务详情</em>
</p>
