## 3.5 Spark案例

&emsp;&emsp;现在我们使用Spark来实现单词计数程序。在这里为了突显出Spark的优势，我们使用`Python`的SparkAPI来实现，总的来说，编写Spark的单词计数程序非常简单。这里依旧使用在3.3节中提到的文本数据集。

&emsp;&emsp;我们只需要做两件事：

1. 编写`Python`代码。
2. 提交Spark程序。

### 3.5.1 编写代码


&emsp;&emsp;首先导入依赖：

```python
from pyspark.sql import SparkSession    # 用于构建 Spark 应用
import re                               # 正则，用于文本清洗   
import sys                              # 用于获取命令行参数和退出程序
```

&emsp;&emsp;和3.3节中步骤相同，我们需要实现一个做字符串清洗的函数，确保文本只保留字母、数字和单个空格。该函数接受一行字符串，返回的清洗后的字符串。

```python
def clean_text(text):
    text = re.sub(r'[^a-zA-Z0-9\s]', ' ', text)
    text = text.lower()
    text = re.sub(r'\s+', ' ', text)
    return text.strip()
```

&emsp;&emsp;编写`main()`函数，同样使用命令行参数来给定输入输出路径。路径中如果包含"hdfs://"，则表示在hdfs上的文件，包含"file://"则表示是本地文件。

```python
def main():
    input_path = sys.argv[1]
    output_path = sys.argv[2]

```


&emsp;&emsp;创建SparkSession，其是Spark程序的起点，指定程序名为WordCount。
```python
    spark = SparkSession.builder.appName("WordCount").getOrCreate()
```

&emsp;&emsp;接着我们需要编写核心的Spark流程，回顾3.4节中Spark的流程，Spark中的操作可分为转换（Transformation）操作和行动（Action）操作两类。

&emsp;&emsp;首先使用`textFile()`，其类似于MapReduce中的`TextInputFormat`，经过此步后，会将数据转换为RDD，并且以行的形式组织。（`[string1, string2, ..., stringn]`）接着调用`flatMap()`，对每一行调用`clean_text().split()`（首先清洗，然后按照空格切分清洗后的字符串，这样就将字符串转换为一个单词列表），(`[word1, word2, ..., wordn]`)之后将单词列表中的每个元素变为一个键值对`<word, 1>`，`reduceByKey()`会按照Key（即单词）聚合计数，到这里就完成了每个单词的计数，最后使用`sortBy()` 按Value（计数）降序排序。这里的原理在3.4节中已介绍过。

&emsp;&emsp;最后使用`saveAsTextFile()`方法输出结果，这是一个行动（Action）操作，程序执行到此才会真正触发前面的所有操作。

```python
    try:
        word_counts = spark.sparkContext.textFile(input_path) \
            .flatMap(lambda line: clean_text(line).split()) \
            .map(lambda word: (word, 1)) \
            .reduceByKey(lambda a, b: a + b) \
            .sortBy(lambda x: x[1], ascending=False)
        word_counts.saveAsTextFile(output_path)
```

&emsp;&emsp;考虑异常处理与终止，捕获运行时异常，最终确保`spark.stop()`被执行，释放资源。

```python
    except Exception as e:
        print(f"Error: {str(e)}", file=sys.stderr)
        sys.exit(1)
    finally:
        spark.stop()
```

### 3.5.2 提交Spark程序

&emsp;&emsp;提交代码，指定在YARN上运行，并且指定部署模式为集群模式，在这里指定输入为3.3中数据集。在运行过程中，控制台会实时输出该作业运行状态。

```shell
spark-submit --master yarn --deploy-mode cluster python.py hdfs:///wordcount/wordcount.txt hdfs:///your_path
```

```
25/06/05 13:47:12 INFO Client: Application report for application_1749097706453_0001 (state: RUNNING)
25/06/05 13:47:34 INFO Client: Application report for application_1749097706453_0001 (state: FINISHED)
```

&emsp;&emsp;等待任务执行完后，我们可以在Yarn的WebUI查看任务完成情况，如图3-21所示。我们可以看到处理同样的文件，Spark执行只花费了不到一分钟的时间，而在MapReduce上花费了1分钟48秒，从中可以看出Spark带来的性能改进。我们同时也可以在输出路径中查看作业结果，如图3-22，3-23所示。

<p align="center">
    <img src="/pic/3/3-21 Spark程序完成情况.png" width="50%">
    <br/>
    <em>3-21 Spark程序完成情况</em>
</p>

<p align="center">
    <img src="/pic/3/3-22 Spark单词计数输出-1.png" width="50%">
    <br/>
    <em>3-22 Spark单词计数输出-1</em>
</p>

<p align="center">
    <img src="/pic/3/3-23 Spark单词计数输出.png" width="50%">
    <br/>
    <em>3-23 Spark单词计数输出-2</em>
</p>