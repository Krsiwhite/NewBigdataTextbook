## 3.2 Yarn：资源协调者

&emsp;&emsp;我们在2.1节中提到，Hadoop生态有三大核心组件，分别是 HDFS、MapReduce 和 Yarn；在第二章中，已详细介绍了HDFS的相关内容，在上一节中，我们也探讨了MapReduce的编程模型和处理思想，那么为什么在这本章的第二节要去介绍一个几乎从未提到过的Yarn呢？这就是因为Yarn身份的特殊性；我们知道，无论是批处理还是流处理，它们的计算过程是需要花费资源的，例如在MapReduce中，在Map阶段，数据被切割成多个分片后分别交给多个MapTask并行处理，在这其中它们必然需要占用CPU来完成计算，Reduce阶段同样也是如此，每个分区会由一个ReduceTask处理。那么如何决定这些任务交给哪些主机处理呢？如果有一台主机上的MapTask迟迟得不到完成，那么整个MapReduce任务就会无限推迟；如果一台主机上有着多个批处理任务，怎么去安排它们合理地使用资源。这些都是MapReduce任务实际落地后会遇见的关于资源管理的典型问题，而Yarn在解决这些问题中扮演着极其重要的角色。

### 3.2.1 为什么我们需要Yarn？

&emsp;&emsp;如果想要说清楚为什么需要Yarn，那么我们最好回顾一下在没有Yarn的时候，整个MapReduce是怎样的。

&emsp;&emsp;我们目前使用的Hadoop版本是3.3.6，Yarn在2.X版本时才作为Hadoop的核心组件，在1.X版本时还不存在Yarn，在那时，MapReduce既要负责完成实际编写的任务，又要负责资源管理、任务调度等工作。

&emsp;&emsp;经典的Hadoop1.X的MapReduce采用Master/Slave结构。Master是整个集群的唯一全局管理者，功能包括作业管理、状态监控和任务调度等，叫做JobTracker。Slave负责任务的执行和任务状态的回报，叫做TaskTracker。任务（Task）分为ReduceTask和MapTask两种。

&emsp;&emsp;TaskTracker 一方面从JobTracker 接收并执行各种命令(运行任务、提交任务、杀死任务等）；另一方面，将本地节点上各个任务的状态通过心跳周期性汇报给JobTracker，这些信息主要包括两部分：机器级别信息（节点健康情况等）和任务级别信息（任务执行进度、任务执行情况等）。而JobTracker会给 TaskTracker 下达各种命令，主要包括：启动任务（LaunchTaskAction）、提交任务（CommitTaskAction）、终止任务（KillTaskAction）等。

&emsp;&emsp;Hadoop1.X的任务执行过程大致如下：

1. Client向JobTracker提交一个作业（一个MapReduce程序，包括多个ReduceTask和MapTask）
2. JobTracker对该作业进行初始化，创建一些内部的数据结构。
3. 各个TaskTracker通过心跳机制向JobTracker汇报可用资源情况。
4. JobTracker将待处理的任务（一个作业分为若干MapTask和ReduceTask）分配给可用的TaskTracker。
5. TaskTracker执行任务。


<p align="center">
    <img src="/pic/3/3-16 Hadoop1.X架构图.png" width="50%">
    <br/>
    <em>图3-16 Hadoop1.X架构图</em>
</p>



&emsp;&emsp;传统的MapReduce被人诟病之处主要在于：

* 可靠性差：和 HDFS 、HBase的单点故障类似，一旦主节点JobTracker出现故障就会导致整个集群不可用。

* 扩展性差：JobTracker同时负责作业调度（将作业调度给可用的TaskTracker）和任务进度管理（监控任务，重启失败任务等），这种负荷使得JobTracker成为整个平台的瓶颈。
* 无法支持异构的计算框架：在一个组织中，不可能只有批处理计算的需求，也会产生流处理的需求、大规模并行处理的需求等，这些需求催生了后续章节中会提到到的Spark等架构，而传统的JobTracker和TaskTracker无法支持多种计算框架并存。

&emsp;&emsp;为了克服以上不足，Hadoop开始向下一代发展，新的集群调度框架Yarn诞生了。Yarn接管了所有资源管理的功能，并通过可插拔的方式很好地兼容了异构的计算框架。

### 3.2.2 Yarn的思想

&emsp;&emsp;Yarn的思想是将JobTracker的责任划分给两个独立的进程：资源管理器（ResourceManager）负责管理集群的所有资源；应用管理器（ApplicationMaster）负责管理集群上各作业的执行。

&emsp;&emsp;具体来说，Yarn中资源分配以容器（Container）为单位，容器中包含了运行一个任务（Task）所需的资源，如CPU，网络资源等。对于用户提交的每个作业，都对应着一个在自己的应用管理器，也就是每个应用管理器负责管理其对应作业中各任务的执行。

&emsp;&emsp;综上所述，Yarn中包括以下几个角色：

* 客户端（Client）：向集群提交作业，可以是MapReduce作业或是来自其他计算框架的应用程序，例如Spark等
* 资源管理器（ResourceManager）：Yarn集群的主节点，负责调度整个集群的计算资源。
* 节点管理器（NodeManager）：Yarn集群的从节点，负责各任务的具体执行，运行和监控Container实例中各任务的执行。
* 应用管理器（ApplicationMaster）：调度一个作业上的所有任务，任务（Task）和ApplicationMaster都运行在Container中，Container由主节点ResourceManager分配，运行在从节点NodeManager上。
* 底层的分布式文件系统：通常是HDFS。


  
<p align="center">
    <img src="/pic/3/3-14 MapReduce1.X和Yarn对比.png" width="50%">
    <br/>
    <em>图3-14 MapReduce1.X和Yarn对比</em>
</p>

&emsp;&emsp;结合图3-4，大致描述一个客户作业的执行过程：

1. 客户端向集群提交任务，任务可以是MapReduce作业、Spark应用程序或其他分布式计算任务。
2. ResourceManager收到用户的作业提交请求，并为该作业分配一个独立的ApplicationMaster（AM），该AM最终注册运行在某个NodeManager的Container上。
3. ApplicationMaster向ResourceManager申请该作业所需资源以执行作业中的各个任务（通常包括Map任务和Reduce任务）。资源最终以位于从节点上的Container分配。
4. Map阶段：在Map阶段，任务会在各个容器中执行。NodeManager负责启动这些容器，并根据作业的逻辑从HDFS的DataNode中拉取所需的数据块。
5. Reduce阶段：在Reduce阶段，每个容器内的Reduce任务会接收Map阶段产生的中间数据，执行聚合操作，并将最终结果写回HDFS中。
6. 任务执行完毕后释放资源：一旦作业的所有任务都完成了，ApplicationMaster向ResourceManager请求释放已分配的资源。这些资源可以被后续作业使用。

<p align="center">
    <img src="/pic/3/3-4 Yarn运行流程.png" width="50%">
    <br/>
    <em>图3-4 Yarn运行流程</em>
</p>

&emsp;&emsp;在整个任务的执行过程中，HDFS用于存储数据，计算框架（如MapReduce或Spark）用于处理数据，而YARN用于协调和管理计算资源。这三个组件的协同工作使得Hadoop集群能够高效地执行大规模分布式计算任务。


### 3.2.3 配置HA-Yarn

&emsp;&emsp;现在，我们已经完全了解了Yarn带来的好处，接下来的工作便是配置Yarn，在原有集群（在之前我们已经配置好了HDFS+ZooKeeper+HBase）的主机上，分配好ResourceManager和NodeManager。在这里我们会直接配置HA-Yarn，也就是配合ZooKeeper集群实现高可用。大致过程如下：

_在主机m1上执行_

**（1） 配置`mapred-site.xml`**

&emsp;&emsp;Yarn组件已集成在安装好的Hadoop中，我们只需修改配置文件将其启动即可。首先修改`/root/hadoop-3.3.6/etc/hadoop/`下的`mapred-site.xml`，将其修改如下：

```xml
<configuration>
        <!--指定mapreduce上运行在yarn上-->
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
        <!--指定保存作业记录的端口-->
        <property>
                <name>mapreduce.jobhistory.address</name>
                <value>m2:10020</value>
        </property>
        <!--指定保存作业记录的web访问端口-->
        <property>
                <name>mapreduce.jobhistory.webapp.address</name>
                <value>m2:19888</value>
        </property>
        <!--手动指定MapReduce环境变量，否则执行任务可能报错，路径为实际Hadoop安装路径-->
        <property>
                <name>yarn.app.mapreduce.am.env</name>
                <value>HADOOP_MAPRED_HOME=/root/hadoop-3.3.6</value>
        </property>
        <property>
                <name>mapreduce.map.env</name>
                <value>HADOOP_MAPRED_HOME=/root/hadoop-3.3.6</value>
        </property>
        <property>
                <name>mapreduce.reduce.env</name>
                <value>HADOOP_MAPRED_HOME=/root/hadoop-3.3.6</value>
        </property>
</configuration>
```

**（2） 配置`yarn-site.xml`**

&emsp;&emsp;接着修改`yarn-site.xml`，启动yarn集群，配置高可用，在这里我们配置ResourceManager数量为2，分别运行在`m3`和`m2`上。

```xml
<configuration>
<!--指定yarn上允许运行的分布式代码为mapreduce-->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
<!-- 开启日志聚合：将各个节点上的日志文件集中到HDFS中，便于管理 -->
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
<!-- 设置日志保存时间 -->
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>106800</value>
    </property>
    <property>
        <name>yarn.log.server.url</name>
        <value>http://m2:19888/jobhistory/logs</value>
    </property>
<!--配置resourcemanager的HA-->
    <property>
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
    </property>
<!-- RM 集群标识 -->
    <property>
        <name>yarn.resourcemanager.cluster-id</name>
        <value>yarn-cluster</value>
    </property> 
    <!-- RM 的逻辑 ID 列表 -->
    <property>
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm2,rm3</value>
    </property> 
    <!-- RM1 的主机地址 -->
    <property>
        <name>yarn.resourcemanager.hostname.rm2</name>
        <value>m2</value>
    </property>
    <!-- RM1 的主机web管理界面地址 --> 
    <property>
        <name>yarn.resourcemanager.webapp.address.rm2</name>
        <value>m2:8088</value>
    </property>
    <!-- RM2 的主机地址 -->
    <property>
        <name>yarn.resourcemanager.hostname.rm3</name>
        <value>m3</value>
    </property> 
    <!-- RM2 的主机web管理界面地址 -->  
    <property>
        <name>yarn.resourcemanager.webapp.address.rm3</name>
        <value>m3:8088</value>
    </property>
    <!-- ZooKeeper 集群的地址 -->  
    <property>
        <name>yarn.resourcemanager.zk-address</name>
        <value>m1:2181,m2:2181,m3:2181</value>
    </property> 
    <!-- 启用自动恢复 --> 
    <property>
        <name>yarn.resourcemanager.recovery.enabled</name>
        <value>true</value>
    </property> 
    <!-- 用于yarn故障转移持久化zk的类 -->
    <property>
        <name>yarn.resourcemanager.store.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>  
    </property>  
    <!-- 配置Yarn分配资源相关，包括nodemanager节点可用内存大小，可用cpu核心数等信息 -->
    <!-- 不配置可能导致MapReduce任务进度卡在0 -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>3072</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>2</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>256</value>
    </property>
</configuration>
```

**（3） 配置`yarn-env.sh`**

&emsp;&emsp;最后修改`yarn-env.sh`，设定运行用户为`root`。

```sh
export YARN_RESOURCEMANAGER_USER="root"  # ResourceManager 运行用户
export YARN_NODEMANAGER_USER="root"     # NodeManager 运行用户
```

**（4） 同步配置文件**

&emsp;&emsp;将配置文件同步至另外两台主机：

```shell
scp -r /root/hadoop-3.3.6/etc/hadoop m2:/root/hadoop-3.3.6/etc
scp -r /root/hadoop-3.3.6/etc/hadoop m3:/root/hadoop-3.3.6/etc
```

**（5） 启动**

&emsp;&emsp;首先在各个主机上`zkServer.sh start`启动ZooKeeper集群，接着在`m1`上执行`start-all.sh`启动HDFS+Yarn集群。接着使用`mapred --daemon start historyserver`来启动JobHistory服务器，其可以存储每个作业的详细历史信息，包括作业的摘要、任务级别的统计数据等。

```shell
root@m1:~# start-all.sh
Starting namenodes on [m1 m2]
Starting datanodes
Starting journal nodes [m1 m2 m3]
Starting ZK Failover Controllers on NN hosts [m1 m2]
Starting resourcemanagers on [ m2 m3]
Starting nodemanagers
root@m1:~# mapred --daemon start historyserver
```

&emsp;&emsp;之后使用`jps`查看各台主机，结果应如下：

```shell
root@m1:~# jps
2993 NodeManager
2642 DFSZKFailoverController
2164 DataNode
5080 Jps
2012 NameNode
1661 QuorumPeerMain
2399 JournalNode

root@m2:~# jps
2274 DFSZKFailoverController
1654 QuorumPeerMain
1848 NameNode
1961 DataNode
2425 ResourceManager
2538 NodeManager
2095 JournalNode
4495 Jps
5445 JobHistoryServer

root@m3:~# jps
2240 NodeManager
1778 DataNode
1654 QuorumPeerMain
1979 JournalNode
3788 Jps
2127 ResourceManager
```

**（6） 访问**

&emsp;&emsp;我们可以使用8088端口访问ResourceManager的Web页面，我们访问`m3`的8088端口，如图3-5，点击左边的`Cluster-About`，可看见该集群信息，从中我们可以看到我们已成功配置了HA-Yarn。

<p align="center">
    <img src="/pic/3/3-5 Yarn集群Web访问.png" width="50%">
    <br/>
    <em>图3-5 Yarn集群Web访问</em>
</p>

&emsp;&emsp;也可使用命令查看两个ResourceManager的状态并验证高可用，当将Active ResourceManager终止后，另一RM将转变为Active状态：

```shell
root@m2:~# yarn rmadmin -getServiceState rm3
standby
root@m2:~# yarn rmadmin -getServiceState rm2
active
root@m2:~# kill 2425
root@m2:~# yarn rmadmin -getServiceState rm2
Operation failed: ... failed on connection exception ...
root@m2:~# yarn rmadmin -getServiceState rm3
active
```

&emsp;&emsp;如图3-6，点击左边的`Cluster-Nodes`可看见各NodeManager的监控信息。

<p align="center">
    <img src="/pic/3/3-6 各NodeManager状态信息.png" width="50%">
    <br/>
    <em>图3-6 各NodeManager状态信息</em>
</p>

&emsp;&emsp;可在`m2:19888`访问JobHistory服务器的Web界面，如图3-7。

<p align="center">
    <img src="/pic/3/3-7 jobhistory界面.png" width="50%">
    <br/>
    <em>图3-7 jobhistory界面</em>
</p>

### 3.2.4 总结

&emsp;&emsp;在了解了Yarn的思想以及作用之后，相信大家对MapReduce背后的底层原理有了更深层次的了解，在配置完HA-Yarn之后，我们已经完成了实操开始编写MapReduce程序的所有准备工作。在3.3中，我们将亲自完成在3.1节中所描述的单词计数程序，并在此基础上更进一步。
