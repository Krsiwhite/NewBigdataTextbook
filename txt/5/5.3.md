## 5.3 Hive进阶实践

&emsp;&emsp;在本小节，我们会学到Hive的一些进阶使用知识。

### 5.3.1 Hive中的表类型

&emsp;&emsp;Hive 中的表根据数据的物理存储方式和生命周期，可分为以下几类：

**（1） 内部表（Managed Table）**

&emsp;&emsp;内部表也称为托管表（Managed Table），其数据的生命周期由 Hive 管理。创建内部表时，Hive 会在仓库目录（默认是`/user/hive/warehouse`）下为表创建一个对应的文件夹。当用户删除表时，Hive 会自动删除表的元数据和数据文件。内部表适用于 Hive 完全掌控数据生命周期的场景。在上小节中我们创建的所有表都属于内部表。


**（2） 外部表（External Table）**

&emsp;&emsp;外部表的数据文件不归 Hive 管理，而是存放在 Hive 仓库目录外部的位置（如 HDFS 上的其他路径、对象存储等）。当删除外部表时，Hive 仅删除表的元数据，不会删除实际数据文件。适合需要与其他系统共享数据或需要保护数据的场景。我们可以使用`external`关键字来创建外部表。在下面的例子中，我们创建了一个外部表，并规定每行中每个属性以逗号`,`划分，数据存储在HDFS集群下的目录`/publicData`中。


```sql
create external table t_user7(
id int,
name string
)
row format delimited
fields terminated by ','
location '/publicData';
```

```shell
root@m1:~# hdfs dfs -mkdir /publicData
root@m1:~# hdfs dfs -put test.txt /publicData
root@m1:~# beeline -u jdbc:hive2://192.168.0.2:10000 -n root
Connecting to jdbc:hive2://192.168.0.2:10000
0: jdbc:hive2://192.168.0.2:10000> create external table t_user7(
. . . . . . . . . . . . . . . . .> id int,
. . . . . . . . . . . . . . . . .> name string
. . . . . . . . . . . . . . . . .> )
. . . . . . . . . . . . . . . . .> row format delimited
. . . . . . . . . . . . . . . . .> fields terminated by ','
. . . . . . . . . . . . . . . . .> location '/publicData';
0: jdbc:hive2://192.168.0.2:10000> select * from t_user7;
+-------------+---------------+
| t_user7.id  | t_user7.name  |
+-------------+---------------+
| 1           | 小明            |
| 2           | 小红            |
| 3           | 小王            |
+-------------+---------------+
```

&emsp;&emsp;之后，我们可以使用drop语句删除该表，可发现，原始数据依旧存在：

```shell
0: jdbc:hive2://192.168.0.2:10000> drop table t_user7;
0: jdbc:hive2://192.168.0.2:10000> Closing: 0: jdbc:hive2://192.168.0.2:10000
root@m1:~# hdfs dfs -ls /publicData
Found 1 items
-rw-r--r--   3 root supergroup         27 2025-05-30 15:56 /publicData/test.txt
```

**（3） 临时表（Temporary Table）**

&emsp;&emsp;临时表是会话级别的表，仅在当前会话中存在，关闭会话后自动删除。临时表适合用于临时计算和中间结果存储。可以使用关键字`temporary`来创建临时表。

```
create temporary table temp_result (
  id int,
  value double
);
```

**（4） 分区表（Partitioned Table）**

&emsp;&emsp;为了提高SQL语句的查询效率，例如当执行`select * from orders where create_date='20230826';`时，假如数据量比较大，这个sql语句就是相当于执行了一次全表扫描，效率不佳。于是可以将数据按照天进行分区，一个分区就是一个文件夹，当你查询`20230826`的时候只需要去`20230826`这个文件夹中取数据即可，不需要全表扫描，提高了查询效率。这就是Hive中的分区表。

&emsp;&emsp;分区表是按照某些字段（如日期、地区等）对数据进行划分的表。每个分区对应 HDFS 上的一个目录，便于查询优化和数据管理。在下面的例子中，我们创建了一个分区表，规定以`dt`属性分区，很显然`dt`属性不在普通属性里，这是一个伪列，但其可以当普通字段使用，后面我们会讲解这是为什么。


```
create table if not exists part1(
  id int,
  name string,
  age int
)
partitioned by (dt string)
row format delimited 
fields terminated by ','
lines terminated by '\n';

```

&emsp;&emsp;在创建好分区表之后，我们准备两份数据，分别为`user1.txt`和`user2.txt`。内容为：

```
user1.txt
1,zhangsan,21
2,lisi,25
3,wangwu,33

user2.txt
4,zhaoliu,38
5,laoyan,36
6,xiaoqian,12
```

&emsp;&emsp;将两个文件上传，我们将其按日期分区。如图5-7所示，我们可以看见其就是将不同分区的数据依据分区的属性存储在不同的文件夹中。

```
load data local inpath '/root/user1.txt' into table part1 partition(dt='2025-05-29');
load data local inpath '/root/user2.txt' into table part1 partition(dt='2025-05-30');
```

<p align="center">
    <img src="/pic/5/5-7 Hive中的分区表.png" width="50%">
    <br/>
    <em>图5-7 Hive中的分区表</em>
</p>

&emsp;&emsp;使用SQL语句查看该表，可以发现`dt`实际上也成为了表中的一种属性，这也就解答了为什么其可以当作普通属性使用。

```
0: jdbc:hive2://192.168.0.2:10000> select * from part1;
+-----------+-------------+------------+-------------+
| part1.id  | part1.name  | part1.age  |  part1.dt   |
+-----------+-------------+------------+-------------+
| 1         | zhangsan    | 21         | 2025-05-29  |
| 2         | lisi        | 25         | 2025-05-29  |
| 3         | wangwu      | 33         | 2025-05-29  |
| 4         | zhaoliu     | 38         | 2025-05-30  |
| 5         | laoyan      | 36         | 2025-05-30  |
| 6         | xiaoqian    | 12         | 2025-05-30  |
+-----------+-------------+------------+-------------+
```

&emsp;&emsp;总结一下，分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集，在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。



**（5） 桶表（Bucket Table）**

&emsp;&emsp;桶表通过对某个字段（如 user_id）进行哈希分桶，将数据分散存储到固定数量的文件中。这有助于提升查询性能和支持抽样查询。 数据分区可能导致有些分区，数据过多；有些分区，数据极少。分桶是将数据集分解为若干部分（数据文件）的另一种技术。分区和分桶其实都是对数据更细粒度的管理。当单个分区或者表中的数据越来越大，分区不能细粒度的划分数据时，我们就采用分桶技术将数据更细粒度的划分和管理。在下面的例子中创建了一个桶表，其指明使用字段`id`进行分桶，`into 4 buckets`代表数据会分散进四个不同桶（也就是说整个表被拆分成四个文件）。

```
create table stu_bucket(id int, name string)
clustered by(id) 
into 4 buckets
row format delimited fields terminated by ' ';

```

&emsp;&emsp;桶表的原理是，其首先根据指定的字段做哈希函数，之后对结果取模决定该行被存储进哪个桶中，如此做可以使得每个桶中存储的数据量尽可能一致。

```
example: hash("hello") = 41 % 4 = 1,
should into bucket 1;
```

&emsp;&emsp;接着我们创建一个`student.txt`文件用于导入。

```
student.txt
1001 ss1
1002 ss2
1003 ss3
1004 ss4
1005 ss5
1006 ss6
1007 ss7
1008 ss8
1009 ss9
1010 ss10
1011 ss11
1012 ss12
1013 ss13
1014 ss14
1015 ss15
1016 ss16
```

&emsp;&emsp;接着使用指令导入。

&emsp;&emsp;`load data local inpath '/root/student.txt' into table stu_bucket;`

&emsp;&emsp;如图5-8所示，我们可以看见这个表中存在四个数据文件，也就是四个桶。我们可以查看文件`000000_0`，也就是第一个桶，可以看见其中的数据如图5-9所示。

<p align="center">
    <img src="/pic/5/5-8 Hive中的桶表.png" width="50%">
    <br/>
    <em>图5-8 Hive中的桶表</em>
</p>

<p align="center">
    <img src="/pic/5/5-9 桶中的数据.png" width="50%">
    <br/>
    <em>图5-9 桶中的数据</em>
</p>

### 5.3.2 使用JavaAPI连接Hive

&emsp;&emsp;首先我们需要新建一个maven项目，并且导入依赖：

```xml
<dependencies>
    <dependency>
        <groupId>org.apache.hive</groupId>
        <artifactId>hive-jdbc</artifactId>
        <version>4.0.1</version>
        <classifier>standalone</classifier> <!-- Hive4 专用，包含完整依赖 -->
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>3.3.6</version> <!-- 根据你的Hadoop版本调整 -->
    </dependency>
    <dependency>
        <groupId>org.apache.thrift</groupId>
        <artifactId>libthrift</artifactId>
        <version>0.21.0</version> <!-- 根据 Hive 版本调整 -->
    </dependency>
</dependencies>
```

&emsp;&emsp;编写Java程序查询并输出表中的所有行，由于代码比较简单，整体如下：
```java
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.Statement;

public class HiveReadExample {
    // JDBC URL, username, and password
    private static String driverName = "org.apache.hive.jdbc.HiveDriver";
    private static String jdbcUrl = "jdbc:hive2://m1:10000/default"; // 根据实际Host和Port调整
    private static String user = "root";  // 根据Hive配置修改
    private static String password = "1111121";  // 根据Hive配置修改
    public static void main(String[] args) {
        try {
            // 1. 加载 Hive JDBC 驱动
            Class.forName(driverName);
            // 2. 创建连接
            Connection conn = DriverManager.getConnection(jdbcUrl, user, password);
            Statement stmt = conn.createStatement();
            // 3. 执行查询
            String sql = "SELECT id, name, age FROM part1";
            ResultSet res = stmt.executeQuery(sql);
            // 4. 打印结果
            System.out.println("ID\tName\tAge");
            while (res.next()) {
                int id = res.getInt("id");
                String name = res.getString("name");
                int age = res.getInt("age");
                System.out.println(id + "\t" + name + "\t" + age);
            }
            // 5. 关闭连接
            res.close();
            stmt.close();
            conn.close();
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
```

&emsp;&emsp;将项目打包后执行即可，可看见结果如下：

```shell
D:\NewBigdataTextbook\code\demo\target>java -jar demo-1.0-SNAPSHOT-shaded.jar
ID      Name    Age
1       zhangsan        21
2       lisi    25
3       wangwu  33
4       zhaoliu 38
5       laoyan  36
6       xiaoqian        12
```

### 5.3.3 Spark SQL

&emsp;&emsp;由于Hive不再支持Hive on Spark，无法从Hive侧演示将计算引擎从MapReduce切换到Spark带来的巨大提升。但在前文介绍Spark时，我们就提到了Spark SQL，其基于底层的Spark Core提供了对SQL语言的支持。其同样支持使用Hive的元数据来操作。故本小节使用Spark SQL以演示Spark带来的巨大性能提升。首先，我们需要配置Spark。在`/root/spark-3.5.5/conf`目录下，新建文件`spark-defaults.conf`，在其中写入如下，注意其中`hive.metastore.uris`中需要填入Hive中metastore所在主机IP。

```
spark.sql.catalogImplementation=hive
spark.hadoop.hive.metastore.uris=thrift://192.168.0.2:9083
spark.sql.warehouse.dir=/user/hive/warehouse
```

&emsp;&emsp;之后照例启动metastore和hiveserver2，使用命令`spark-sql`连接Hive。

```
root@m1:~# spark-sql
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/05/31 11:33:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/05/31 11:33:47 WARN HiveConf: HiveConf of name hive.metastore.event.db.notification.api.auth does not exist
25/05/31 11:33:47 WARN HiveConf: HiveConf of name hive.server2.active.passive.ha.enable does not exist
Spark Web UI available at http://m1:4040
Spark master: local[*], Application Id: local-1748662429104
```

&emsp;&emsp;之后，我们便可使用SQL语句操控数据库，在这里我们看到，一条插入语句的执行只花费了2秒！花费时间不到MapReduce的十分之一，这是巨大的性能提升。

```
spark-sql (default)> show tables;
stu
student
students
part1
stu_bucket
Time taken: 2.052 seconds, Fetched 5 row(s)
spark-sql (default)> select * from students;
25/05/31 11:34:08 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.
1234    daming  男
Time taken: 2.462 seconds, Fetched 1 row(s)
spark-sql (default)> insert into students values(12, "danan", "男");
Time taken: 2.098 seconds
```

&emsp;&emsp;我们可以在端口4040看到Spark的WebUI，在其中，可以看见我们刚刚执行的两条SQL语句的任务完成情况，如图5-10所示。同样的，我们可以在Yarn集群的WebUI中看到提交的Spark任务，如图5-11。

<p align="center">
    <img src="/pic/5/5-10 Spark SQL.png" width="50%">
    <br/>
    <em>图5-10 Spark SQL</em>
</p>

<p align="center">
    <img src="/pic/5/5-11 Spark提交到Yarn.png" width="50%">
    <br/>
    <em>图5-11 Spark提交到Yarn</em>
</p>


### 5.3.3 总结

&emsp;&emsp;在本节中，我们学习了Hive的进阶知识，到此完成了本书对Hive的所有介绍，在一般情况下，建议大家使用Spark SQL来操控Hive，这比使用MapReduce要快得多。在下一节中，我们会简单地介绍其他的一些常用数据仓库。