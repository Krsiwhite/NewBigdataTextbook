## 4.4 数据流处理

#### 4.4.1 Storm及其安装配置

&emsp;&emsp;Storm是一个分布式、可靠、容错的实时流式数据处理的系统。在Storm中，先要设计一个用于实时计算的图状结构，我们称之为拓扑（topology）。这个拓扑将会被提交给集群，由集群中的主控节点（master node）分发代码，将任务分配给工作节点（worker node）执行。一个拓扑中包括spout和bolt两种角色，其中spout发送消息，负责将数据流以tuple元组的形式发送出去；而bolt则负责转换这些数据流，在bolt中可以完成计算、过滤等操作，bolt自身也可以随机将数据发送给其他bolt。由spout发射出的tuple是不可变数组，对应着固定的键值对。

&emsp;&emsp;接下来我们开始安装配置Storm。

&emsp;&emsp;下载Storm：

```bash
# 下载Storm 2.5.0版本（由于清华镜像中没有2.5.0版本的Storm，所以使用中央仓库的。国外镜像访问速度可能较慢）
wget https://archive.apache.org/dist/storm/apache-storm-2.5.0/apache-storm-2.5.0.tar.gz

# 解压
tar -zxvf apache-storm-2.5.0.tar.gz
```

&emsp;&emsp;环境变量配置：

```bash
# 在m1上，编辑/etc/profile
export STORM_HOME=/root/apache-storm-2.5.0
export PATH=$PATH:$STORM_HOME/bin

# 拷贝环境变量到m2，m3中：
scp -r /etc/profile m2:/etc
scp -r /etc/profile m3:/etc


# 在三台机器上分别执行如下代码，使环境变量生效
source /etc/profile
```

&emsp;&emsp;配置Storm：

```bash
# 进入storm配置目录，配置storm.yaml
cd ${STORM_HOME}/conf
编辑storm.yaml，添加如下内容：
storm.zookeeper.servers: # 这里输入三台服务器的名称
     - "m1"
     - "m2"
     - "m3"
# nimbus nodes list
nimbus.seeds: ["m1"]
# nimbus host node
nimbus.host: "m1"
# Web页面端口号
ui.port: 8081
# workers进程的端口，每一个worker进程都会使用一个端口来接收消息
supervisor.slots.ports:
  - 6700
  - 6701
  - 6702
  - 6703
storm.local.dir: "/root/storm/data"

# 创建storm数据文件目录
mkdir /root/storm/data
# 注：supervisor.slots.ports 参数用来配置 workers 进程接收消息的端口，默认每个 supervisor 节点上会启动 4 个 worker，当然你也可以按照自己的需要和服务器性能进行设置，假设只想启动 2 个 worker 的话，此处配置 2 个端口即可。

# 拷贝配置文件到m2，m3中：
scp -r /root/apache-storm-2.5.0 m2:/etc
scp -r /root/apache-storm-2.5.0 m3:/etc
```

&emsp;&emsp;启动storm：

```bash
# 启动Storm之前，首先启动zookeeper集群。
# 在三台服务器上，分别执行：
zkServer.sh start

# 在m1上开启nimbus进程：
storm nimbus &

#正常没问题的情况应该会显示“[1] 进程号”然后没有其他消息。
#但首次启动有可能出现如下报错：
#Need python version > 2.6
#这是因为storm启动依赖特定的组件，而这些组件需要python 2.6以上的版本。
#如果你是一直按照本书进行实验的，那你的服务器上应该已经安装了python3的，只是storm默认识别到了其他版本的python，我们可以通过软链接的方式避开这个问题，在三台机器上执行下面这条命令实现软链接：
update-alternatives --install /usr/bin/python python /usr/bin/python3 1

#保存文件后，拷贝storm文件到另外两台机器上
scp -r /root/apache-storm-2.5.0 m2:/root
scp -r /root/apache-storm-2.5.0 m3:/root

#然后再在m1上输入 
storm nimbus &

#新开一个m1窗口检查jps，nimbus显示已成功启动。
#在m2，m3上开启supervisor进程
storm supervisor &

#此时m2和m3的jps出现supervisor，即表示启动成功。

#如果需要在web上查看storm部署情况，可以新开m1控制台或Ctrl+C之后，继续执行如下指令
storm ui & （这个时候jps会多一个core）
storm logviewer & （这个时候jps会多一个logviewer）
#之后访问https://1.92.64.216:8081/。（注意：1.92.64.216指的是m1主机，即nimbus主机的公网地址。输入私网地址打不开！读者配置时根据自己实际的公网地址替换）。
#到此，storm的安装配置过程完成。
```

#### 4.4.2 Flume+Kafka+Storm+HBase整合

&emsp;&emsp;在4.3和本节，我们已经安装了Flume、Kafka和Storm，并且在4.3中，我们整合了Flume和Kafka，接下来我们将把刚刚安装的Storm也整合进去，同时用上在2.4节中学过的Hbase，完成Flume、Kafka、Storm及Hbase的整合。

&emsp;&emsp;我们本节中将完成一个小案例：向Flume输入数据，再将数据传输到Kafka中，将Kafka消费者收到的数据通过Storm处理转换为全小写并放入HBase储存，完成该案例将帮助我们体会到流处理更完整的流程。

&emsp;&emsp;：

`Flume输入数据 -> Kafka消费者接收数据 -> Storm消费Kafka消费者接收的数据并处理 -> HBase存储Storm处理后的数据。`

&emsp;&emsp;整合步骤如下：

1. 启动各组件
2. 编写Java程序
3. 运行Java程序
4. 向Flume输入数据
5. 验证数据存储
6. 观察拓扑运行情况

**（1）启动各组件**

```bash
# 启动Zookeeper
# 由于Kafka依赖于Zookeeper管理集群信息。因此在启动Kafka之前，需要先启动zookeeper。
# 在m1，m2，m3三台机器分别执行
zkServer.sh start

# 启动Hadoop&HBase
# 在m1上启动Hadoop
start-dfs.sh

# 在m3上启动HBase：
start-hbase.sh

# 启动Kafka
# 在m1，m2，m3三台机器分别执行（-daemon是守护进程，后台执行）：
cd ${KAFKA_HOME}
bin/kafka-server-start.sh -daemon config/server.properties

# 启动Storm
#在m1上输入
storm nimbus &

#在m2，m3上开启supervisor进程
storm supervisor &

# 创建HBase表格
#在m3上，输入
hbase shell

#创建表格
create ‘storm_test’, ‘cf’

# 启动Flume Agent
flume-ng agent --conf conf --conf-file ${FLUME_HOME}/conf/flume.conf --name agent1 -Dflume.root.logger=INFO,console
```

&emsp;&emsp;此时三台服务器上输入jps，分别为：
```bash
# m1
DFSZKFailoverController
Kafka
HRegionServer
NameNode
Application
Nimbus
Jps
JournalNode
QuorumPeerMain

# m2
DFSZKFailoverController
Kafka
HRegionServer
NameNode
Supervisor
Jps
JournalNode
QuorumPeerMain
LogWriter

# m3
Kafka
HMaster
HRegionServer
DataNode
Supervisor
Jps
JournalNode
QuorumPeerMain
Worker
LogWriter
```

**（2）编写Java程序**

&emsp;&emsp;我们要完成一个程序，完成Storm处理Kafka数据，将Storm从Kafka中获取到的数据转化为小写，并将数据存储到HBase。

&emsp;&emsp;相关java代码如下：

```java
// Kafka的Bootstrap服务器地址
String bootstrapServers = "m1:9092,m2:9092,m3:9092";

// Kafka主题
String topic = "mylog";

// 构建Storm拓扑
StormTopology topology = buildTopology(bootstrapServers, topic);

// 创建Storm配置
Config config = new Config();
config.setDebug(true);// 开启调试模式

// 提交拓扑到Storm集群
StormSubmitter.submitTopology(
        "storm_topology",// 拓扑名称
        config,// 配置
        topology// 拓扑
);
System.out.println("拓扑提交成功");
```

```java
    /**
     * HBase存储Bolt类，用于将数据存储到HBase表中
     */
    public static class HBaseStorageBolt extends BaseRichBolt {
        private Connection connection;// HBase连接
        private Table table;// HBase表

        /**
         * 在Bolt初始化时，建立HBase连接并获取表
         */
        @Override
        public void prepare(Map<String, Object> map, TopologyContext topologyContext, OutputCollector outputCollector) {
            try {
                // 创建HBase连接
                connection = ConnectionFactory.createConnection(HBaseConfiguration.create());

                // 获取HBase表，表名为"storm_test"
                table = connection.getTable(org.apache.hadoop.hbase.TableName.valueOf("storm_test"));
            } catch (Exception e) {
                throw new RuntimeException("无法连接到HBase", e);
            }
        }

        /**
         * 处理每个从Kafka接收到的消息，并将其存储到HBase表中
         */
        @Override
        public void execute(Tuple tuple) {
            try {
                // 从元组中获取消息内容并转为小写
                String value = tuple.getStringByField("value").toLowerCase();

                // 使用"row_时间戳"作为row key，避免重复
                String rowKey = "row_" + System.currentTimeMillis();

                // 创建Put操作，将数据写入HBase表
                Put put = new Put(Bytes.toBytes(rowKey));
                put.addColumn(Bytes.toBytes("cf"), Bytes.toBytes("qualifier"), Bytes.toBytes(value));
                table.put(put);
            } catch (Exception e) {
                throw new RuntimeException("无法将数据存入Hbase", e);
            }
        }


        /**
         * 声明输出字段，由于该Bolt不输出数据，因此无需声明
         */
        @Override
        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            // No output fields to declare for HBase storage
        }

        /**
         * 在Bolt关闭时，释放HBase资源
         */
        @Override
        public void cleanup() {
            try {
                if (table != null) table.close();
                if (connection != null) connection.close();
            } catch (Exception e) {
                throw new RuntimeException("无法关闭Hbase资源", e);
            }
        }
    }
```

```java
/**
 * 构建Storm拓扑
 * @param bootstrapServers Kafka的Bootstrap服务器地址
 * @param topic Kafka主题
 * @return 构建好的Storm拓扑
 */
public static StormTopology buildTopology(String bootstrapServers, String topic) {
    // 构建KafkaSpout配置
    KafkaSpoutConfig<String, String> kafkaSpoutConfig = KafkaSpoutConfig.builder(bootstrapServers, topic)
            .setProp("group.id", "storm-kafka-group")  // 设置Kafka消费者组ID
            .build();

    // 创建拓扑构建器
    TopologyBuilder builder = new TopologyBuilder();

    // 添加KafkaSpout到拓扑，名称为"kafka-spout"
    builder.setSpout("kafka-spout", new KafkaSpout<>(kafkaSpoutConfig));

    // 添加HBase存储Bolt到拓扑，名称为"hbase-storage-bolt"，并将其与KafkaSpout连接
    builder.setBolt("hbase-storage-bolt", new HBaseStorageBolt()).shuffleGrouping("kafka-spout");

    // 返回构建好的拓扑
    return builder.createTopology();
}
```

&emsp;&emsp;由于我们需要用到Storm、Kafka和Hbase的API，所以我们需要添加如下依赖，同时需要用打包插件将其打包到jar包中。

```bash
    <dependencies>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-core</artifactId>
            <version>2.5.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>3.9.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-kafka-client</artifactId>
            <version>2.5.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-client</artifactId>
            <version>2.5.11</version>
        </dependency>
    </dependencies>
```

&emsp;&emsp;完成编程后，加载maven项目，在maven项目处首先使用clean然后package打包为jar，对项目目录/target下的jar名为FlumeKafkaStormHBase-1.0.jar,将该jar包上传到m1机器的/root目录。

**（3）运行Java程序**

```bash
# 在m1上运行jar包：
storm jar FlumeKafkaStormHBase-1.0.jar bigdatatextbook.KafkaStormHBaseTopology
```

**（4）向Flume输入数据**

&emsp;&emsp;运行成功之后，开始进行数据传输：

```bash
# 新开一个m1终端，使用telnet连接
telnet m1 4141
# 然后可以输入任意信息，观察Flume控制台的输出，以及Kafka消费者的输出
HELLO WORLD!
Hello World!
```

**（5）验证数据存储**

```bash
#在m3机器上输入：
Hbase shell

#查看hbase表格数据
scan ‘storm_test’

#即可看到刚才消费者消费的消息数据，rowkey为row_时间戳，column cf:qualifier，然后出现刚才消费的内容（完成小写转换后的）。
```

**（6）通过Storm UI观察拓扑运行情况**

&emsp;&emsp;在m1机器上输入
`storm ui &`

&emsp;&emsp;打开http://1.92.64.216:8081/（需要替换成m1机器的公网地址）

&emsp;&emsp;打开Topology Summary下面的拓扑名称“storm_topology”，可以看到拓扑的运行情况。