## 4.4 数据流处理

### 4.4.1 Storm

**（1） Storm的工作原理**

&emsp;&emsp;Storm是一个分布式、可靠、容错的实时流式数据处理的系统。在Storm中，先要设计一个用于实时计算的图状结构，我们称之为拓扑（topology）。这个拓扑将会被提交给集群，由集群中的主控节点（master node）分发代码，将任务分配给工作节点（worker node）执行。一个拓扑中包括spout和bolt两种角色，其中spout发送消息，负责将数据流以tuple元组的形式发送出去；而bolt则负责转换这些数据流，在bolt中可以完成计算、过滤等操作，bolt自身也可以随机将数据发送给其他bolt。由spout发射出的tuple是不可变数组，对应着固定的键值对。


**（2） Storm的安装配置**


&emsp;&emsp;下载Storm：

```bash
# 下载Storm 2.5.0版本（由于清华镜像中没有2.5.0版本的Storm，所以使用中央仓库的。国外镜像访问速度可能较慢）
wget https://archive.apache.org/dist/storm/apache-storm-2.5.0/apache-storm-2.5.0.tar.gz

# 解压
tar -zxvf apache-storm-2.5.0.tar.gz
```

&emsp;&emsp;环境变量配置：

```bash
# 在m1上，编辑/etc/profile
export STORM_HOME=/root/apache-storm-2.5.0
export PATH=$PATH:$STORM_HOME/bin

# 拷贝环境变量到m2，m3中：
scp -r /etc/profile m2:/etc
scp -r /etc/profile m3:/etc


# 在三台机器上分别执行如下代码，使环境变量生效
source /etc/profile
```

&emsp;&emsp;配置Storm：

```bash
# 进入storm配置目录，配置storm.yaml
cd ${STORM_HOME}/conf
编辑storm.yaml，添加如下内容：
storm.zookeeper.servers: # 这里输入三台服务器的名称
     - "m1"
     - "m2"
     - "m3"
# nimbus nodes list
nimbus.seeds: ["m1"]
# nimbus host node
nimbus.host: "m1"
# Web页面端口号
ui.port: 8081
# workers进程的端口，每一个worker进程都会使用一个端口来接收消息
supervisor.slots.ports:
  - 6700
  - 6701
  - 6702
  - 6703
storm.local.dir: "/root/storm/data"

# 创建storm数据文件目录
mkdir /root/storm/data
# 注：supervisor.slots.ports 参数用来配置 workers 进程接收消息的端口，默认每个 supervisor 节点上会启动 4 个 worker，当然你也可以按照自己的需要和服务器性能进行设置，假设只想启动 2 个 worker 的话，此处配置 2 个端口即可。

# 拷贝配置文件到m2，m3中：
scp -r /root/apache-storm-2.5.0 m2:/etc
scp -r /root/apache-storm-2.5.0 m3:/etc
```

&emsp;&emsp;启动storm：

```bash
# 启动Storm之前，首先启动zookeeper集群。
# 在三台服务器上，分别执行：
zkServer.sh start

# 在m1上开启nimbus进程：
storm nimbus &

#正常没问题的情况应该会显示“[1] 进程号”然后没有其他消息。
#但首次启动有可能出现如下报错：
#Need python version > 2.6
#这是因为storm启动依赖特定的组件，而这些组件需要python 2.6以上的版本。
#如果你是一直按照本书进行实验的，那你的服务器上应该已经安装了python3的，只是storm默认识别到了其他版本的python，我们可以通过软链接的方式避开这个问题，在三台机器上执行下面这条命令实现软链接：
update-alternatives --install /usr/bin/python python /usr/bin/python3 1

#保存文件后，拷贝storm文件到另外两台机器上
scp -r /root/apache-storm-2.5.0 m2:/root
scp -r /root/apache-storm-2.5.0 m3:/root

#然后再在m1上输入 
storm nimbus &

#新开一个m1窗口检查jps，nimbus显示已成功启动。
#在m2，m3上开启supervisor进程
storm supervisor &

#此时m2和m3的jps出现supervisor，即表示启动成功。

#如果需要在web上查看storm部署情况，可以新开m1控制台或Ctrl+C之后，继续执行如下指令
storm ui & （这个时候jps会多一个core）
storm logviewer & （这个时候jps会多一个logviewer）
#之后访问https://1.92.64.216:8081/。（注意：1.92.64.216指的是m1主机，即nimbus主机的公网地址。输入私网地址打不开！读者配置时根据自己实际的公网地址替换）。
#到此，storm的安装配置过程完成。
```


**（3） Flume+Kafka+Storm+HBase结合实战**

&emsp;&emsp;在4.3和本节，我们已经安装了Flume、Kafka和Storm，并且在4.3中，我们整合了Flume和Kafka，接下来我们将把刚刚安装的Storm也整合进去，同时用上在2.4节中学过的Hbase，完成Flume、Kafka、Storm及Hbase的整合。

&emsp;&emsp;我们本节中将完成一个小案例：向Flume输入数据，再将数据传输到Kafka中，将Kafka消费者收到的数据通过Storm处理转换为全小写并放入HBase储存，完成该案例将帮助我们体会到流处理更完整的流程。

&emsp;&emsp;：

`Flume输入数据 -> Kafka消费者接收数据 -> Storm消费Kafka消费者接收的数据并处理 -> HBase存储Storm处理后的数据。`

&emsp;&emsp;整合步骤如下：

1. 启动各组件
2. 编写Java程序
3. 运行Java程序
4. 向Flume输入数据
5. 验证数据存储
6. 观察拓扑运行情况

**（1）启动各组件**

```bash
# 启动Zookeeper
# 由于Kafka依赖于Zookeeper管理集群信息。因此在启动Kafka之前，需要先启动zookeeper。
# 在m1，m2，m3三台机器分别执行
zkServer.sh start

# 启动Hadoop&HBase
# 在m1上启动Hadoop
start-dfs.sh

# 在m3上启动HBase：
start-hbase.sh

# 启动Kafka
# 在m1，m2，m3三台机器分别执行（-daemon是守护进程，后台执行）：
cd ${KAFKA_HOME}
bin/kafka-server-start.sh -daemon config/server.properties

# 启动Storm
#在m1上输入
storm nimbus &

#在m2，m3上开启supervisor进程
storm supervisor &

# 创建HBase表格
#在m3上，输入
hbase shell

#创建表格
create ‘storm_test’, ‘cf’

# 启动Flume Agent
flume-ng agent --conf conf --conf-file ${FLUME_HOME}/conf/flume.conf --name agent1 -Dflume.root.logger=INFO,console
```

&emsp;&emsp;此时三台服务器上输入jps，分别为：
```bash
# m1
DFSZKFailoverController
Kafka
HRegionServer
NameNode
Application
Nimbus
Jps
JournalNode
QuorumPeerMain

# m2
DFSZKFailoverController
Kafka
HRegionServer
NameNode
ConsoleConsumer
Supervisor
Jps
JournalNode
QuorumPeerMain

# m3
DFSZKFailoverController
Kafka
HMaster
HRegionServer
DataNode
ConsoleConsumer
Supervisor
Jps
JournalNode
QuorumPeerMain
```

**（2）编写Java程序**

&emsp;&emsp;我们要完成一个程序，完成Storm处理Kafka数据，将Storm从Kafka中获取到的数据转化为小写，并将数据存储到HBase。

&emsp;&emsp;相关java代码如下：

```java
// Kafka的Bootstrap服务器地址
String bootstrapServers = "m1:9092,m2:9092,m3:9092";

// Kafka主题
String topic = "mylog";

// 构建Storm拓扑
StormTopology topology = buildTopology(bootstrapServers, topic);

// 创建Storm配置
Config config = new Config();
config.setDebug(true);// 开启调试模式

// 提交拓扑到Storm集群
StormSubmitter.submitTopology(
        "storm_topology",// 拓扑名称
        config,// 配置
        topology// 拓扑
);
System.out.println("拓扑提交成功");
```

```java
    /**
     * HBase存储Bolt类，用于将数据存储到HBase表中
     */
    public static class HBaseStorageBolt extends BaseRichBolt {
        private Connection connection;// HBase连接
        private Table table;// HBase表

        /**
         * 在Bolt初始化时，建立HBase连接并获取表
         */
        @Override
        public void prepare(Map<String, Object> map, TopologyContext topologyContext, OutputCollector outputCollector) {
            try {
                // 创建HBase连接
                connection = ConnectionFactory.createConnection(HBaseConfiguration.create());

                // 获取HBase表，表名为"storm_test"
                table = connection.getTable(org.apache.hadoop.hbase.TableName.valueOf("storm_test"));
            } catch (Exception e) {
                throw new RuntimeException("无法连接到HBase", e);
            }
        }

        /**
         * 处理每个从Kafka接收到的消息，并将其存储到HBase表中
         */
        @Override
        public void execute(Tuple tuple) {
            try {
                // 从元组中获取消息内容并转为小写
                String value = tuple.getStringByField("value").toLowerCase();

                // 使用"row_时间戳"作为row key，避免重复
                String rowKey = "row_" + System.currentTimeMillis();

                // 创建Put操作，将数据写入HBase表
                Put put = new Put(Bytes.toBytes(rowKey));
                put.addColumn(Bytes.toBytes("cf"), Bytes.toBytes("qualifier"), Bytes.toBytes(value));
                table.put(put);
            } catch (Exception e) {
                throw new RuntimeException("无法将数据存入Hbase", e);
            }
        }


        /**
         * 声明输出字段，由于该Bolt不输出数据，因此无需声明
         */
        @Override
        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            // No output fields to declare for HBase storage
        }

        /**
         * 在Bolt关闭时，释放HBase资源
         */
        @Override
        public void cleanup() {
            try {
                if (table != null) table.close();
                if (connection != null) connection.close();
            } catch (Exception e) {
                throw new RuntimeException("无法关闭Hbase资源", e);
            }
        }
    }
```

```java
/**
 * 构建Storm拓扑
 * @param bootstrapServers Kafka的Bootstrap服务器地址
 * @param topic Kafka主题
 * @return 构建好的Storm拓扑
 */
public static StormTopology buildTopology(String bootstrapServers, String topic) {
    // 构建KafkaSpout配置
    KafkaSpoutConfig<String, String> kafkaSpoutConfig = KafkaSpoutConfig.builder(bootstrapServers, topic)
            .setProp("group.id", "storm-kafka-group")  // 设置Kafka消费者组ID
            .build();

    // 创建拓扑构建器
    TopologyBuilder builder = new TopologyBuilder();

    // 添加KafkaSpout到拓扑，名称为"kafka-spout"
    builder.setSpout("kafka-spout", new KafkaSpout<>(kafkaSpoutConfig));

    // 添加HBase存储Bolt到拓扑，名称为"hbase-storage-bolt"，并将其与KafkaSpout连接
    builder.setBolt("hbase-storage-bolt", new HBaseStorageBolt()).shuffleGrouping("kafka-spout");

    // 返回构建好的拓扑
    return builder.createTopology();
}
```

&emsp;&emsp;此外，需要编辑pom.xml，导入必要依赖，使用`maven-shade-plugin`将依赖一起打包，并指定主类。所需依赖如下：

```bash

    <dependencies>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-core</artifactId>
            <version>2.5.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>3.9.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-kafka-client</artifactId>
            <version>2.5.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-client</artifactId>
            <version>2.5.11</version>
        </dependency>
    </dependencies>
```

&emsp;&emsp;完成编程后，加载maven项目，在maven项目处首先使用clean然后package打包为jar，对项目目录/target下的jar改名为FlumeKafkaStormHBase.jar,并将该jar包上传到m1机器的/root目录。

**（3）运行Java程序**

```bash
# 在m1上运行jar包：
storm jar FlumeKafkaStormHBase.jar bigdatatextbook.KafkaStormHBaseTopology
```

**（4）向Flume输入数据**

&emsp;&emsp;运行成功之后，开始进行数据传输：

```bash
# 新开一个m1终端，使用telnet连接
telnet m1 4141
# 然后可以输入任意信息，观察Flume控制台的输出，以及Kafka消费者的输出
HELLO WORLD!
Hello World!
```

**（5）验证数据存储**

```bash
#在m3机器上输入：
Hbase shell

#查看hbase表格数据
scan ‘storm_test’

#即可看到刚才消费者消费的消息数据，rowkey为row_时间戳，column cf:qualifier，然后出现刚才消费的内容（完成小写转换后的）。
```

6. **通过Storm UI观察拓扑运行情况**：

&emsp;&emsp;在m1机器上输入
`storm ui &`

&emsp;&emsp;打开http://1.92.64.216:8081/（需要替换成m1机器的公网地址）

&emsp;&emsp;打开Topology Summary下面的拓扑名称“storm_topology”，可以看到拓扑的运行情况。

&emsp;&emsp;到此，本实验结束。

### 4.4.2 Spark Streaming 工作机制

&emsp;&emsp;在大数据时代，许多业务场景都对实时数据处理提出了要求，例如金融风控、实时推荐、舆情监测等。Spark Streaming 正是应对这种需求而生的一个基于 Spark 批处理框架的流处理引擎。与传统的“纯流式处理”不同，Spark Streaming 并非一条条逐条处理数据，而是采用了一种称为“微批处理（Micro-Batch）”的模型。可以想象有一条源源不断的实时数据流，我们不会逐条处理，而是每隔几秒（如1秒或2秒）把这段时间内的数据“打包”成一个小批次（mini-batch），然后像处理批数据那样进行统一调度和并行计算。

**（1） Spark Streaming整体架构与数据流动路径**

&emsp;&emsp;Spark Streaming 可以接入来自多种来源的数据，包括 Kafka、Flume、Socket、Twitter、HDFS、Amazon Kinesis 等。在图4-5中我们数据源输入到Spark Streaming中。
<p align="center">
    <img src="/pic/4/4-5 Spark Streaming处理流程.png" width="50%">
    <br/>
    <em>图4-5 Spark Streaming处理流程</em>
</p>
&emsp;&emsp;Spark Streaming 是整个系统的核心，负责：
   - 持续地接收来自输入源的数据流；
   - 将数据划分成时间窗口（micro-batches）；
   - 使用 Spark 的强大计算能力进行实时分析处理，比如过滤、聚合、转化等；
   - 将处理后的数据发送给下游系统。

&emsp;&emsp;处理完的数据可以被写入以下目标：
   - HDFS：用于持久化存储。
   - Databases：如 MySQL、PostgreSQL、Cassandra 等，便于后续查询分析。
   - Dashboards：实时可视化系统，例如 Apache Superset、Grafana 等，用于业务监控和报警。

**（2） Spark Streaming中离散化流（DStream）**

 &emsp;&emsp;通过将连续的数据流分解为一系列时间间隔内的RDD（弹性分布式数据集），每个RDD代表一个特定时间段内的数据集合，从而实现了对实时数据的批处理。具体而言，随着时间推移，数据被分割成固定大小的时间窗口，每个窗口对应一个RDD，如图4-6所示，time 1至time 4分别生成各自的RDD。

 &emsp;&emsp;在处理阶段，Spark Streaming 提供了丰富的 API 来对这些小批次（其实就是 RDD）执行转换操作，比如 map、filter、reduce、join 等，还支持窗口操作（window）、状态维护（updateStateByKey）等高级功能。
   - 窗口操作（window）允许我们对多个连续的微批（micro-batches）进行合并计算，从而实现基于时间窗口的聚合分析（如滑动平均、滚动求和、统计计数等）。
   - updateStateByKey 允许我们在保证容错性的前提下，将每个 key 对应的上一次状态与本次到来的新数据结合，更新并保存新的状态。

 &emsp;&emsp;最终，处理结果可以被输出到 HDFS、数据库、控制台，或者发送到外部消息队列中（如 Kafka 或其他系统），实现端到端的实时数据处理。

<p align="center">
    <img src="/pic/4/4-6 Dstream与RDD.png" width="50%">
    <br/>
    <em>图4-6 Dstream与RDD</em>
</p>
&emsp;&emsp;以上就是 Spark Streaming 的标准处理流程，其整体结构可类比为数据流的四道工序：采集、分批、处理、输出。

&emsp;&emsp;在 Spark Streaming 中，最核心的概念是 DStream（Discretized Stream，离散流）。DStream 代表的是一个逻辑上的连续数据流，但它的底层结构其实是由一系列 时间序列上的 RDD 构成。每个时间窗口内产生一个 RDD，多个时间点上连续生成的 RDD 组成了这个 DStream。

&emsp;&emsp;你可以将 DStream 类比为一条“实时传送带”，每隔一段时间生成一个小批次数据单元（一个 RDD）。每个小批次就像一个“数据箱子”，通过连续处理这些 RDD，即可实现整个数据流的处理任务。


### 4.4.3 Flink详细介绍

&emsp;&emsp;Apache Flink 是一个框架和分布式处理引擎，用于在无边界和有边界数据流上进行有状态的计算。Flink 能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算。

&emsp;&emsp;Apache Flink 功能强大，支持开发和运行多种不同种类的应用程序。它的主要特性包括：批流一体化、精密的状态管理、事件时间支持以及精确一次的状态一致性保障等。Flink 不仅可以运行在包括 YARN、 Mesos、Kubernetes 在内的多种资源管理框架上，还支持在裸机集群上独立部署。在启用高可用选项的情况下，它不存在单点失效问题。事实证明，Flink 已经可以扩展到数千核心，其状态可以达到 TB 级别，且仍能保持高吞吐、低延迟的特性。世界各地有很多要求严苛的流处理应用都运行在 Flink 之上。

&emsp;&emsp;接下来，我们来介绍一下Flink中的几个重要概念

**（1） 批与流**

   - 批处理的特点是有界、持久、大量，非常适合需要访问全套记录才能完成的计算工作，一般用于离线统计。 
   - 流处理的特点是无界、实时, 无需针对整个数据集执行操作，而是对通过系统传输的每个数据项执行操作，一般用于实时统计。
   
&emsp;&emsp;如图4-7所示，在Spark的世界观中，一切都是由批次组成的，离线数据是一个大批次，而实时数据是由一个一个无限的小批次组成的。而在Flink的世界观中，一切都是由流组成的，离线数据是有界限的流，实时数据是一个没有界限的流，这就是所谓的有界流和无界流。
<p align="center">
    <img src="/pic/4/4-7 Flink批次.png" width="50%">
    <br/>
    <em>图4-7 Flink批次.png</em>
</p>
   - 无界流：有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果的完整性。
   - 有界流：有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理。

**（2） Flink的部署应用**

&emsp;&emsp;Apache Flink 是一个分布式系统，它需要计算资源来执行应用程序。Flink 集成了所有常见的集群资源管理器，例如Hadoop YARN、 Apache Mesos和 Kubernetes，但同时也可以作为独立集群运行。

&emsp;&emsp;Flink 被设计为能够很好地工作在上述每个资源管理器中，这是通过资源管理器特定(resource-manager-specific)的部署模式实现的。Flink 可以采用与当前资源管理器相适应的方式进行交互。
   
&emsp;&emsp;部署 Flink 应用程序时，Flink 会根据应用程序配置的并行性自动标识所需的资源，并从资源管理器请求这些资源。在发生故障的情况下，Flink 通过请求新资源来替换发生故障的容器。提交或控制应用程序的所有通信都是通过 REST 调用进行的，这可以简化 Flink 与各种环境中的集成。

&emsp;&emsp;如图4-8所示，有状态的 Flink 程序针对本地状态访问进行了优化。任务的状态始终保留在内存中，如果状态大小超过可用内存，则会保存在能高效访问的磁盘数据结构中。任务通过访问本地（通常在内存中）状态来进行所有的计算，从而产生非常低的处理延迟。Flink 通过定期和异步地对本地状态进行持久化存储来保证故障场景下精确一次的状态一致性。
<p align="center">
    <img src="/pic/4/4-8 Flink内存利用.png" width="50%">
    <br/>
    <em>图4-8 Flink内存利用</em>
</p>
&emsp;&emsp;Spark Streaming 是整个系统的核心，负责：
   - 持续地接收来自输入源的数据流；
   - 将数据划分成时间窗口（micro-batches）；
   - 使用 Spark 的强大计算能力进行实时分析处理，比如过滤、聚合、转化等；
   - 将处理后的数据发送给下游系统。

&emsp;&emsp;处理完的数据可以被写入以下目标：
   - HDFS：用于持久化存储。
   - Databases：如 MySQL、PostgreSQL、Cassandra 等，便于后续查询分析。
   - Dashboards：实时可视化系统，例如 Apache Superset、Grafana 等，用于业务监控和报警。

**（3） Flink分层API**

 &emsp;&emsp;Flink 根据抽象程度分层，提供了三种不同的 API。每一种 API 在简洁性和表达力上有着不同的侧重，并且针对不同的应用场景。
<p align="center">
    <img src="/pic/4/4-9 Flink分层API.png" width="50%">
    <br/>
    <em>图4-9 Flink分层API</em>
</p>


* ProcessFunction：可以处理一或两条输入数据流中的单个事件或者归入一个特定窗口内的多个事件。它提供了对于时间和状态的细粒度控制。开发者可以在其中任意地修改状态，也能够注册定时器用以在未来的某一时刻触发回调函数。因此，你可以利用ProcessFunction实现许多有状态的事件驱动应用所需要的基于单个事件的复杂业务逻辑。
* DataStream API：为许多通用的流处理操作提供了处理原语。这些操作包括窗口、逐条记录的转换操作，在处理事件时进行外部数据库查询等。DataStream API 支持 Java 和 Scala 语言，预先定义了例如map()、reduce()、aggregate() 等函数。你可以通过扩展实现预定义接口或使用 Java、Scala 的 lambda 表达式实现自定义的函数。
* SQL & Table API：Flink 支持两种关系型的 API，Table API 和 SQL。这两个 API 都是批处理和流处理统一的 API，这意味着在无边界的实时数据流和有边界的历史记录数据流上，关系型 API 会以相同的语义执行查询，并产生相同的结果。Table API和SQL借助了 Apache Calcite来进行查询的解析，校验以及优化。它们可以与DataStream和DataSet API无缝集成，并支持用户自定义的标量函数，聚合函数以及表值函数。Flink 的关系型 API 旨在简化数据分析、数据流水线和 ETL 应用的定义。
